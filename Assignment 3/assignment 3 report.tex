\documentclass[12pt]{article}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{geometry}
\geometry{a4paper, margin=1in}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{pgfplots}
\pgfplotsset{compat=1.17}
\usepackage{float}

\begin{document}



\title{Predicting Cricket Match Outcomes Using Decision Trees and Random Forests}
\author{Parth Thakur\\
        2021CS50615}

\date{\today}

\section{Decision Trees and Random Forests}
\subsection{Decision Tree Construction}
    \begin{itemize}
        \item Constructed decision trees using mutual information for attribute selection.
        \item Handled continuous attributes through median-based splitting and categorical attributes through k-way splitting.
        \item Experimented with maximum depths of \{5, 10, 15, 20, 25\}.
    \end{itemize}


\begin{table}[H]
\centering
\caption{Decision Tree Accuracies at Various Depths}
\label{table:decision_tree_accuracies}
\begin{tabular}{ccc}
\toprule
Depth & Training Accuracy & Test Accuracy \\
\midrule
5  & 0.8855 & 0.5440 \\
10 & 0.9963 & 0.5657 \\
15 & 0.9969 & 0.5657 \\
20 & 0.9969 & 0.5657 \\
25 & 0.9969 & 0.5657 \\
\bottomrule
\end{tabular}
\end{table}

\begin{center}
    \includegraphics[width=0.8\textwidth]{Assignment 3/q1/(a)depth_vs_accuracy.png}
\end{center}

Training Accuracy for only loss prediction: 0.4966

Training Accuracy for only win prediction: 0.5033

Test Accuracy for only loss prediction: 0.5036

Test Accuracy for only win prediction: 0.4963

\begin{itemize}
    \item \textbf{Overfitting with Increasing Depth:} Training accuracy increases with depth, reaching nearly 99.7\% for depths 15, 20, and 25, indicating overfitting. Test accuracy remains constant around 56.57\% for depths above 10, suggesting the model doesn't generalize well to unseen data.
    
    \item \textbf{Consistent Test Accuracy:} Test accuracy is relatively consistent across different depths, suggesting that increased complexity at higher depths doesn't improve performance on unseen data.
    
    \item \textbf{Balanced Win/Loss Prediction:} Accuracies for only loss and only win prediction are roughly balanced around 50\%, indicating no significant bias towards predicting one class over the other.
    
    \item \textbf{Gap Between Training and Test Accuracies:} The significant gap between training and test accuracies, especially at higher depths, further indicates overfitting.
    
    \item \textbf{Optimal Maximum Depth:} A maximum depth of 10 appears to be a reasonable choice, offering a balance between training and test accuracies without excessive overfitting.
\end{itemize}


\subsection{Decision Tree One Hot Encoding}
    \begin{itemize}
        \item Implemented one hot encoding for categorical attributes.
        \item Repeated decision tree construction for depths \{15, 25, 35, 45\}.
        \item Compared results with the previous method.
    \end{itemize}

    \begin{table}[h]
\centering
\begin{tabular}{@{}ccc@{}}
\toprule
Max Depth & Training Accuracy & Test Accuracy \\ \midrule
15        & 0.7022            & 0.5574        \\
25        & 0.8398            & 0.6039        \\
35        & 0.9233            & 0.5926        \\
45        & 0.9868            & 0.6060        \\ \bottomrule
\end{tabular}
\caption{Accuracies of the decision tree with varying maximum depths.}
\end{table}

\begin{center}
    \includegraphics[width=0.8\textwidth]{Assignment 3/q1/(b)depth_vs_accuracy.png}
\end{center}

\paragraph{Observations}
\begin{itemize}
    \item The training accuracy increases with the depth of the tree, indicating a more complex model that better fits the training data.
    \item The test accuracy does not consistently improve with increased depth, suggesting that deeper trees may be overfitting the training data.
    \item The highest test accuracy is achieved at a depth of 45, but this comes with a significantly high training accuracy, which may imply overfitting.
    \item The depth of 25 offers a balance between training and test accuracies, potentially providing a model that generalizes better than deeper trees.
\end{itemize}

\subsection{Decision Tree Post Pruning}
    \begin{itemize}
        \item Applied post-pruning based on a validation set.
        \item Plotted training, validation, and test set accuracies against the number of nodes.
    \end{itemize}

    \begin{table}[ht]
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
 & Validation Accuracy & Train Accuracy & Test Accuracy \\ 
\midrule
Attempt 1 & 0.6437 & 0.6313 & 0.5657 \\
Attempt 2 & 0.6966 & 0.6313 & 0.5657 \\
Attempt 3 & 0.7241 & 0.6313 & 0.5657 \\
Attempt 4 & 0.7494 & 0.6313 & 0.5657 \\
\bottomrule
\end{tabular}
\caption{Model accuracies over different attempts}
\label{tab:accuracies}
\end{table}



\subsection{Decision Tree Sci-kit Learn}
    \begin{itemize}
        \item Built decision trees using the sci-kit learn library.
        \item Varying max depth in range \{15, 25, 35, 45\} and pruning parameters ccp alpha in range \{0.001, 0.01, 0.1, 0.2\}.
        \item Reported train and test accuracies and plotted against max depth and ccp alpha.
        \item Used validation set to determine optimal parameters.
        \item Compared results with parts (b) and (c).
    \end{itemize}



\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Max Depth} & \textbf{Training Accuracy} & \textbf{Test Accuracy} & \textbf{Validation Accuracy} \\ \hline
15                 & 0.824454                   & 0.605998               & 0.643678                     \\ \hline
25                 & 0.992334                   & 0.619442               & 0.625287                     \\ \hline
35                 & 0.999744                   & 0.608066               & 0.619540                     \\ \hline
45                 & 1.000000                   & 0.619442               & 0.620690                     \\ \hline
\end{tabular}
\caption{Decision Tree Accuracies with Varying Max Depth}
\label{table:decision_tree_accuracies}
\end{table}

Some observations from the table:
\begin{itemize}
  \item The training accuracy increases with the depth of the tree, reaching perfect accuracy at a depth of 45. This is indicative of overfitting, where the model perfectly learns the training data including its noise.
  \item The test and validation accuracies do not improve significantly with increased depth, which supports the indication of overfitting. The model does not generalize well to unseen data when the tree is too deep.
  \item The validation accuracy is highest for a depth of 15, suggesting that shallower trees may be better at generalizing in this case.
  \item The lack of substantial difference between test and validation accuracies across different depths suggests that the model's ability to generalize has reached a plateau and is not significantly affected by increasing the tree depth beyond a certain point.
\end{itemize}


\subsection{Random Forests}
    \begin{itemize}
        \item Used sci-kit learn library to grow Random Forests.
        \item Experimented with parameters: n\_estimators (50 to 350), max\_features (0.1 to 1.0), min\_samples\_split (2 to 10).
        \item Performed grid search for parameter tuning based on out-of-bag accuracy.
        \item Reported training, out-of-bag, validation, and test set accuracies.
        \item Compared results with parts (c) and (d).
    \end{itemize}

\begin{table}[h!]
\centering
\begin{tabular}{|l|c|}
\hline
\textbf{Parameter/Metric} & \textbf{Value} \\
\hline
Best max\_features & 0.3 \\
Best min\_samples\_split & 10 \\
Best n\_estimators & 350 \\
OOB Accuracy & 72.81\% \\
Training Accuracy & 94.57\% \\
Validation Accuracy & 72.30\% \\
Test Accuracy & 72.80\% \\
\hline
\end{tabular}
\caption{Random Forest optimal parameters and accuracies}
\label{tab:rf_results}
\end{table}

\paragraph{Observations}
\begin{itemize}
  \item The optimal Random Forest model with moderate features per split (\(0.3\)) and a large number of trees (\(350\)) indicates a balance between model complexity and generalization.
  \item The OOB accuracy closely mirrors the validation and test accuracies, suggesting the model's robustness in generalizing to unseen data.
  \item The substantial gap between training accuracy (\(94.57\%\)) and OOB/validation/test accuracies (\(~72\%\)) may indicate overfitting, although the model's performance is consistent across different data samples.
\end{itemize}


\section{Neural Networks}

\subsection{Single Hidden Layer Experiments (Part b)}

\subsubsection{Results}


\begin{center}
    \includegraphics[width=0.8\textwidth]{Assignment 3/q2/(b) f1 vs hidden_size.png}
\end{center}



\subsection{Depth of Neural Network Experiments (Part c and d)}
\subsubsection{Methodology}
A different stopping criteria was used for the last 2 layer configs as the change in loss for them in the first few iterations was low.

\subsubsection{Results and Discussion}
Tables and plots for precision, recall, and F1 score for each class, and the average F1 score vs. the network depth. Discussion of the results and the impact of adaptive learning rate.

\subsection{Experiments with ReLU Activation (Part e)}
\subsubsection{Methodology}
Explanation of the changes made to the neural network to incorporate ReLU activation.

\subsubsection{Results and Discussion}
Tables and plots for precision, recall, and F1 score for each class, and the average F1 score vs. network depth with ReLU activation. Discussion of the results and comparison with previous parts.

\subsection{Comparison with scikit-learn Implementation (Part f)}
\subsubsection{Methodology}
Description of the neural network implementation using scikit-learn's MLPClassifier.

\subsubsection{Results and Discussion}
Comparison of precision, recall, and F1 score for each class with the custom implementation. Discussion of the observations and any differences noted.

\end{document}