{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import sys\n",
    "import pdb\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def get_data(x_path, y_path):\n",
    "    '''\n",
    "    Args:\n",
    "        x_path: path to x file\n",
    "        y_path: path to y file\n",
    "    Returns:\n",
    "        x: np array of [NUM_OF_SAMPLES x n]\n",
    "        y: np array of [NUM_OF_SAMPLES]\n",
    "    '''\n",
    "    x = np.load(x_path)\n",
    "    y = np.load(y_path)\n",
    "\n",
    "    x = x.astype('float')\n",
    "\n",
    "    #normalize each example in x to have 0 mean and 1 std\n",
    "    \n",
    "    # Calculate the mean and standard deviation for each feature\n",
    "    feature_means = np.mean(x, axis=0)\n",
    "    feature_stds = np.std(x, axis=0)\n",
    "    feature_stds = feature_stds + (feature_stds == 0)\n",
    "\n",
    "    # Normalize each feature to have 0 mean and 1 std\n",
    "    x = (x - feature_means) / (feature_stds)\n",
    "    \n",
    "    # Adjust labels to start from 0 if they start from 1\n",
    "    y = y - 1\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "def get_metric(y_true, y_pred):\n",
    "    '''\n",
    "    Args:\n",
    "        y_true: np array of [NUM_SAMPLES x r] (one hot) \n",
    "                or np array of [NUM_SAMPLES]\n",
    "        y_pred: np array of [NUM_SAMPLES x r] (one hot) \n",
    "                or np array of [NUM_SAMPLES]\n",
    "                \n",
    "    '''\n",
    "    results = classification_report(y_pred, y_true)\n",
    "    print(results)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T13:34:15.069700Z",
     "start_time": "2023-11-04T13:34:13.566503Z"
    }
   },
   "id": "caeb9da8ee899ae"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "((10000, 1024), (10000,), (1000, 1024), (1000,))"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test, y_test = get_data('x_test.npy', 'y_test.npy')\n",
    "x_train, y_train = get_data('x_train.npy', 'y_train.npy')\n",
    "\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T13:34:15.389647Z",
     "start_time": "2023-11-04T13:34:15.073309Z"
    }
   },
   "id": "d92553baff72547d"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "label_encoder = OneHotEncoder(sparse_output = False)\n",
    "label_encoder.fit(np.expand_dims(y_train, axis = -1))\n",
    "\n",
    "y_train_onehot = label_encoder.transform(np.expand_dims(y_train, axis = -1))\n",
    "y_test_onehot = label_encoder.transform(np.expand_dims(y_test, axis = -1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T13:34:22.739639Z",
     "start_time": "2023-11-04T13:34:22.728551Z"
    }
   },
   "id": "2bdaeadda38be13e"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.20171276],\n       [0.19835099],\n       [0.19595456],\n       [0.20106116],\n       [0.20292053]])"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layer_sizes):\n",
    "        \"\"\"\n",
    "        Initializes the neural network with the given layer sizes.\n",
    "        layer_sizes is a list of integers, where the i-th integer represents\n",
    "        the number of neurons in the i-th layer.\n",
    "        \"\"\"\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.activations = []\n",
    "\n",
    "        # Initialize weights and biases for each layer\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            # Weights are initialized with small random values\n",
    "            self.weights.append(np.random.randn(layer_sizes[i+1], layer_sizes[i]) * 0.01)\n",
    "            self.biases.append(np.zeros((layer_sizes[i+1], 1)))\n",
    "            \n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    def relu_derivative(self, z):\n",
    "        return (z > 0).astype(float)\n",
    "\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        The sigmoid activation function.\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def softmax(self, z):\n",
    "        \"\"\"\n",
    "        The softmax function.\n",
    "        \"\"\"\n",
    "        e_z = np.exp(z)  # Subtracting np.max(z) for numerical stability\n",
    "        return e_z / e_z.sum(axis=0, keepdims=True)\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        \"\"\"\n",
    "        Performs a feedforward computation.\n",
    "        \"\"\"\n",
    "        activation = x\n",
    "        self.activations = [x]  # List to store all the activations, layer by layer\n",
    "\n",
    "        # Compute activations for each layer\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(w, activation) + b\n",
    "            activation = self.relu(z) if w is not self.weights[-1] else self.softmax(z)\n",
    "            self.activations.append(activation)\n",
    "\n",
    "        return self.activations[-1]  # The final activation is the output of the network\n",
    "\n",
    "# Let's test the initialization and feedforward computation with a small network\n",
    "nn = NeuralNetwork([1024, 100, 5])  # A network with 1024 input features, one hidden layer with 100 neurons, and 5 output classes\n",
    "sample_input = np.random.randn(1024, 1)  # A random sample input\n",
    "output = nn.feedforward(sample_input)  # Perform a feedforward computation\n",
    "\n",
    "output  # Display the output probabilities"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T15:22:29.380857Z",
     "start_time": "2023-11-04T15:22:29.367477Z"
    }
   },
   "id": "839aebbd1796dc2e"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "class NeuralNetwork(NeuralNetwork):  # Extending the previously defined NeuralNetwork class\n",
    "    def cross_entropy_loss(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Computes the cross-entropy loss.\n",
    "        \"\"\"\n",
    "        m = y_true.shape[1]  # Number of examples\n",
    "        # To avoid division by zero, we clip the predictions to a minimum value\n",
    "        # y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        # Compute the cross-entropy loss\n",
    "        loss = -np.sum(y_true * np.log(y_pred)) / m\n",
    "        return loss\n",
    "\n",
    "    def backpropagation(self, y_true):\n",
    "        \"\"\"\n",
    "        Performs backpropagation to compute the gradients of the loss function\n",
    "        with respect to the weights and biases.\n",
    "        \"\"\"\n",
    "        m = y_true.shape[1]  # Number of examples\n",
    "        y_pred = self.activations[-1]  # The output of the last layer\n",
    "        y_true = y_true.reshape(y_pred.shape)  # Ensure same shape\n",
    "\n",
    "        # Initialize gradients for each layer\n",
    "        d_weights = [np.zeros(w.shape) for w in self.weights]\n",
    "        d_biases = [np.zeros(b.shape) for b in self.biases]\n",
    "\n",
    "        # Calculate derivative of loss w.r.t. the last layer output\n",
    "        d_loss = y_pred - y_true\n",
    "\n",
    "        for i in reversed(range(len(d_weights))):\n",
    "            d_activations = d_loss * self.relu_derivative(self.activations[i+1]) if i != len(d_weights) - 1 else d_loss\n",
    "            d_weights[i] = np.dot(d_activations, self.activations[i].T) / m\n",
    "            d_biases[i] = np.sum(d_activations, axis=1, keepdims=True) / m\n",
    "            if i != 0:\n",
    "                d_loss = np.dot(self.weights[i].T, d_activations)\n",
    "\n",
    "        return d_weights, d_biases\n",
    "\n",
    "    def sigmoid_derivative(self, s):\n",
    "        \"\"\"\n",
    "        Derivative of the sigmoid function.\n",
    "        \"\"\"\n",
    "        return s * (1 - s)\n",
    "\n",
    "    def update_parameters(self, d_weights, d_biases, learning_rate):\n",
    "        \"\"\"\n",
    "        Updates the parameters using the computed gradients.\n",
    "        \"\"\"\n",
    "        # Update each parameter with a simple gradient descent step\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= learning_rate * d_weights[i]\n",
    "            self.biases[i] -= learning_rate * d_biases[i]\n",
    "            \n",
    "    def train(self, x_train, y_train, epochs, mini_batch_size, learning_rate, conv_threshold = 0.001, conv_epochs = 5):\n",
    "        n = x_train.shape[1]  # Total number of training examples\n",
    "\n",
    "        # Training loop\n",
    "        loss_history = []\n",
    "        permutation = np.random.permutation(n)\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle the training data for each epoch\n",
    "            x_train_shuffled = x_train[:, permutation]\n",
    "            y_train_shuffled = y_train[:, permutation]\n",
    "\n",
    "            # Mini-batch loop\n",
    "            for k in range(0, n, mini_batch_size):\n",
    "                mini_batch_x = x_train_shuffled[:, k:k + mini_batch_size]\n",
    "                mini_batch_y = y_train_shuffled[:, k:k + mini_batch_size]\n",
    "                # Forward pass\n",
    "                self.feedforward(mini_batch_x)\n",
    "                # Backward pass\n",
    "                d_weights, d_biases = self.backpropagation(mini_batch_y)\n",
    "                # Update parameters\n",
    "                self.update_parameters(d_weights, d_biases, learning_rate)\n",
    "            \n",
    "            loss = self.cross_entropy_loss(self.feedforward(x_train), y_train)\n",
    "            # Optional: Print the loss after each epoch (can be commented out for speed)\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}\")\n",
    "            \n",
    "            loss_history.append(loss)\n",
    "            \n",
    "            if len(loss_history) > conv_epochs:\n",
    "                temp = loss_history[-conv_epochs:]\n",
    "                if np.std(temp) < conv_threshold:\n",
    "                    print('Converged')\n",
    "                    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T15:22:32.158434Z",
     "start_time": "2023-11-04T15:22:32.156680Z"
    }
   },
   "id": "15ad70a12f16b147"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.9515\n",
      "Epoch 2/10, Loss: 0.7405\n",
      "Epoch 3/10, Loss: 0.5967\n",
      "Epoch 4/10, Loss: 0.5117\n",
      "Epoch 5/10, Loss: 0.4558\n",
      "Epoch 6/10, Loss: 0.4163\n",
      "Epoch 7/10, Loss: 0.3864\n",
      "Epoch 8/10, Loss: 0.3627\n",
      "Epoch 9/10, Loss: 0.3430\n",
      "Epoch 10/10, Loss: 0.3260\n"
     ]
    }
   ],
   "source": [
    "# Create a fresh instance of the neural network with the corrected train method\n",
    "nn = NeuralNetwork([1024, 100, 5])\n",
    "# Convert labels to one-hot encoding again\n",
    "y_train_one_hot = np.eye(5)[y_train].T\n",
    "\n",
    "# Train the neural network again with the corrected training method\n",
    "nn.train(x_train.T, y_train_one_hot, epochs=10, mini_batch_size=32, learning_rate=0.01)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T13:37:28.284448Z",
     "start_time": "2023-11-04T13:37:25.295799Z"
    }
   },
   "id": "f3df8f2c73fc76ec"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "class NeuralNetwork(NeuralNetwork):  # Extending the previously defined NeuralNetwork class\n",
    "    def train_c(self, x_train, y_train, epochs, mini_batch_size, learning_rate, conv_threshold = 0.001, conv_epochs = 5):\n",
    "        n = x_train.shape[1]  # Total number of training examples\n",
    "\n",
    "        # Training loop\n",
    "        loss_history = []\n",
    "        permutation = np.random.permutation(n)\n",
    "        learning_rate_c = learning_rate\n",
    "        for epoch in range(epochs):\n",
    "            learning_rate = learning_rate_c / pow(epoch+1, 0.5)\n",
    "            # Shuffle the training data for each epoch\n",
    "            x_train_shuffled = x_train[:, permutation]\n",
    "            y_train_shuffled = y_train[:, permutation]\n",
    "\n",
    "            # Mini-batch loop\n",
    "            for k in range(0, n, mini_batch_size):\n",
    "                mini_batch_x = x_train_shuffled[:, k:k + mini_batch_size]\n",
    "                mini_batch_y = y_train_shuffled[:, k:k + mini_batch_size]\n",
    "                # Forward pass\n",
    "                self.feedforward(mini_batch_x)\n",
    "                # Backward pass\n",
    "                d_weights, d_biases = self.backpropagation(mini_batch_y)\n",
    "                # Update parameters\n",
    "                self.update_parameters(d_weights, d_biases, learning_rate)\n",
    "            \n",
    "            loss = self.cross_entropy_loss(self.feedforward(x_train), y_train)\n",
    "            # Optional: Print the loss after each epoch (can be commented out for speed)\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}\")\n",
    "            \n",
    "            loss_history.append(loss)\n",
    "            \n",
    "            if len(loss_history) > conv_epochs:\n",
    "                temp = loss_history[-conv_epochs:]\n",
    "                if np.std(temp) < conv_threshold:\n",
    "                    print('Converged')\n",
    "                    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T15:22:36.150882Z",
     "start_time": "2023-11-04T15:22:36.143453Z"
    }
   },
   "id": "4afa85796ad6feab"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: [1024, 512, 5]\n",
      "Epoch 1/200, Loss: 0.8414\n",
      "Epoch 2/200, Loss: 0.6865\n",
      "Epoch 3/200, Loss: 0.6037\n",
      "Epoch 4/200, Loss: 0.5507\n",
      "Epoch 5/200, Loss: 0.5130\n",
      "Epoch 6/200, Loss: 0.4843\n",
      "Epoch 7/200, Loss: 0.4614\n",
      "Epoch 8/200, Loss: 0.4426\n",
      "Epoch 9/200, Loss: 0.4268\n",
      "Epoch 10/200, Loss: 0.4132\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[49], line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlayer: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlayer\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      6\u001B[0m nn \u001B[38;5;241m=\u001B[39m NeuralNetwork(layer)\n\u001B[0;32m----> 7\u001B[0m \u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_c\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_train\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mT\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train_one_hot\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m200\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmini_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m32\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m models_e[\u001B[38;5;28mstr\u001B[39m(layer)] \u001B[38;5;241m=\u001B[39m nn\n",
      "Cell \u001B[0;32mIn[35], line 24\u001B[0m, in \u001B[0;36mNeuralNetwork.train_c\u001B[0;34m(self, x_train, y_train, epochs, mini_batch_size, learning_rate, conv_threshold, conv_epochs)\u001B[0m\n\u001B[1;32m     22\u001B[0m     d_weights, d_biases \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbackpropagation(mini_batch_y)\n\u001B[1;32m     23\u001B[0m     \u001B[38;5;66;03m# Update parameters\u001B[39;00m\n\u001B[0;32m---> 24\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate_parameters\u001B[49m\u001B[43m(\u001B[49m\u001B[43md_weights\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43md_biases\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     26\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcross_entropy_loss(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeedforward(x_train), y_train)\n\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m# Optional: Print the loss after each epoch (can be commented out for speed)\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[34], line 51\u001B[0m, in \u001B[0;36mNeuralNetwork.update_parameters\u001B[0;34m(self, d_weights, d_biases, learning_rate)\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweights)):\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweights[i] \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m learning_rate \u001B[38;5;241m*\u001B[39m d_weights[i]\n\u001B[0;32m---> 51\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbiases[i] \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m learning_rate \u001B[38;5;241m*\u001B[39m d_biases[i]\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "hidden_layers = [[512], [512, 256], [512, 256, 128], [512, 256, 128, 64]]\n",
    "layers = [[1024] + hidden_layer + [5] for hidden_layer in hidden_layers]\n",
    "models_e = {}\n",
    "for layer in layers:\n",
    "    print(f'layer: {layer}')\n",
    "    nn = NeuralNetwork(layer)\n",
    "    nn.train_c(x_train.T, y_train_one_hot, epochs=200, mini_batch_size=32, learning_rate=0.01)\n",
    "    models_e[str(layer)] = nn"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T16:17:19.893847Z",
     "start_time": "2023-11-04T16:16:53.761453Z"
    }
   },
   "id": "e405b4822b98cdc2"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: [1024, 100, 100, 100, 5]\n",
      "Epoch 1/200, Loss: 1.6092\n",
      "Epoch 2/200, Loss: 1.6091\n",
      "Epoch 3/200, Loss: 1.6091\n",
      "Epoch 4/200, Loss: 1.6091\n",
      "Epoch 5/200, Loss: 1.6091\n",
      "Epoch 6/200, Loss: 1.6090\n",
      "Epoch 7/200, Loss: 1.6090\n",
      "Epoch 8/200, Loss: 1.6090\n",
      "Epoch 9/200, Loss: 1.6090\n",
      "Epoch 10/200, Loss: 1.6090\n",
      "Epoch 11/200, Loss: 1.6090\n",
      "Epoch 12/200, Loss: 1.6090\n",
      "Epoch 13/200, Loss: 1.6090\n",
      "Epoch 14/200, Loss: 1.6089\n",
      "Epoch 15/200, Loss: 1.6089\n",
      "Epoch 16/200, Loss: 1.6089\n",
      "Epoch 17/200, Loss: 1.6089\n",
      "Epoch 18/200, Loss: 1.6089\n",
      "Epoch 19/200, Loss: 1.6089\n",
      "Epoch 20/200, Loss: 1.6088\n",
      "Epoch 21/200, Loss: 1.6088\n",
      "Epoch 22/200, Loss: 1.6088\n",
      "Epoch 23/200, Loss: 1.6088\n",
      "Epoch 24/200, Loss: 1.6087\n",
      "Epoch 25/200, Loss: 1.6087\n",
      "Epoch 26/200, Loss: 1.6087\n",
      "Epoch 27/200, Loss: 1.6086\n",
      "Epoch 28/200, Loss: 1.6086\n",
      "Epoch 29/200, Loss: 1.6086\n",
      "Epoch 30/200, Loss: 1.6085\n",
      "Epoch 31/200, Loss: 1.6085\n",
      "Epoch 32/200, Loss: 1.6084\n",
      "Epoch 33/200, Loss: 1.6084\n",
      "Epoch 34/200, Loss: 1.6083\n",
      "Epoch 35/200, Loss: 1.6082\n",
      "Epoch 36/200, Loss: 1.6082\n",
      "Epoch 37/200, Loss: 1.6081\n",
      "Epoch 38/200, Loss: 1.6080\n",
      "Epoch 39/200, Loss: 1.6079\n",
      "Epoch 40/200, Loss: 1.6078\n",
      "Epoch 41/200, Loss: 1.6077\n",
      "Epoch 42/200, Loss: 1.6076\n",
      "Epoch 43/200, Loss: 1.6074\n",
      "Epoch 44/200, Loss: 1.6073\n",
      "Epoch 45/200, Loss: 1.6071\n",
      "Epoch 46/200, Loss: 1.6069\n",
      "Epoch 47/200, Loss: 1.6066\n",
      "Epoch 48/200, Loss: 1.6064\n",
      "Epoch 49/200, Loss: 1.6061\n",
      "Epoch 50/200, Loss: 1.6058\n",
      "Epoch 51/200, Loss: 1.6054\n",
      "Epoch 52/200, Loss: 1.6049\n",
      "Epoch 53/200, Loss: 1.6044\n",
      "Epoch 54/200, Loss: 1.6038\n",
      "Epoch 55/200, Loss: 1.6031\n",
      "Epoch 56/200, Loss: 1.6023\n",
      "Epoch 57/200, Loss: 1.6013\n",
      "Epoch 58/200, Loss: 1.6002\n",
      "Epoch 59/200, Loss: 1.5987\n",
      "Epoch 60/200, Loss: 1.5970\n",
      "Epoch 61/200, Loss: 1.5949\n",
      "Epoch 62/200, Loss: 1.5923\n",
      "Epoch 63/200, Loss: 1.5889\n",
      "Epoch 64/200, Loss: 1.5846\n",
      "Epoch 65/200, Loss: 1.5790\n",
      "Epoch 66/200, Loss: 1.5715\n",
      "Epoch 67/200, Loss: 1.5613\n",
      "Epoch 68/200, Loss: 1.5468\n",
      "Epoch 69/200, Loss: 1.5256\n",
      "Epoch 70/200, Loss: 1.4938\n",
      "Epoch 71/200, Loss: 1.4450\n",
      "Epoch 72/200, Loss: 1.3721\n",
      "Epoch 73/200, Loss: 1.2772\n",
      "Epoch 74/200, Loss: 1.1845\n",
      "Epoch 75/200, Loss: 1.1177\n",
      "Epoch 76/200, Loss: 1.0718\n",
      "Epoch 77/200, Loss: 1.0373\n",
      "Epoch 78/200, Loss: 1.0100\n",
      "Epoch 79/200, Loss: 0.9876\n",
      "Epoch 80/200, Loss: 0.9685\n",
      "Epoch 81/200, Loss: 0.9518\n",
      "Epoch 82/200, Loss: 0.9368\n",
      "Epoch 83/200, Loss: 0.9231\n",
      "Epoch 84/200, Loss: 0.9106\n",
      "Epoch 85/200, Loss: 0.8989\n",
      "Epoch 86/200, Loss: 0.8880\n",
      "Epoch 87/200, Loss: 0.8776\n",
      "Epoch 88/200, Loss: 0.8678\n",
      "Epoch 89/200, Loss: 0.8583\n",
      "Epoch 90/200, Loss: 0.8492\n",
      "Epoch 91/200, Loss: 0.8402\n",
      "Epoch 92/200, Loss: 0.8314\n",
      "Epoch 93/200, Loss: 0.8228\n",
      "Epoch 94/200, Loss: 0.8145\n",
      "Epoch 95/200, Loss: 0.8063\n",
      "Epoch 96/200, Loss: 0.7982\n",
      "Epoch 97/200, Loss: 0.7902\n",
      "Epoch 98/200, Loss: 0.7823\n",
      "Epoch 99/200, Loss: 0.7744\n",
      "Epoch 100/200, Loss: 0.7665\n",
      "Epoch 101/200, Loss: 0.7588\n",
      "Epoch 102/200, Loss: 0.7512\n",
      "Epoch 103/200, Loss: 0.7437\n",
      "Epoch 104/200, Loss: 0.7362\n",
      "Epoch 105/200, Loss: 0.7289\n",
      "Epoch 106/200, Loss: 0.7217\n",
      "Epoch 107/200, Loss: 0.7145\n",
      "Epoch 108/200, Loss: 0.7074\n",
      "Epoch 109/200, Loss: 0.7002\n",
      "Epoch 110/200, Loss: 0.6931\n",
      "Epoch 111/200, Loss: 0.6860\n",
      "Epoch 112/200, Loss: 0.6788\n",
      "Epoch 113/200, Loss: 0.6717\n",
      "Epoch 114/200, Loss: 0.6645\n",
      "Epoch 115/200, Loss: 0.6572\n",
      "Epoch 116/200, Loss: 0.6499\n",
      "Epoch 117/200, Loss: 0.6426\n",
      "Epoch 118/200, Loss: 0.6352\n",
      "Epoch 119/200, Loss: 0.6278\n",
      "Epoch 120/200, Loss: 0.6203\n",
      "Epoch 121/200, Loss: 0.6128\n",
      "Epoch 122/200, Loss: 0.6052\n",
      "Epoch 123/200, Loss: 0.5976\n",
      "Epoch 124/200, Loss: 0.5898\n",
      "Epoch 125/200, Loss: 0.5819\n",
      "Epoch 126/200, Loss: 0.5740\n",
      "Epoch 127/200, Loss: 0.5659\n",
      "Epoch 128/200, Loss: 0.5577\n",
      "Epoch 129/200, Loss: 0.5493\n",
      "Epoch 130/200, Loss: 0.5409\n",
      "Epoch 131/200, Loss: 0.5323\n",
      "Epoch 132/200, Loss: 0.5238\n",
      "Epoch 133/200, Loss: 0.5153\n",
      "Epoch 134/200, Loss: 0.5068\n",
      "Epoch 135/200, Loss: 0.4983\n",
      "Epoch 136/200, Loss: 0.4899\n",
      "Epoch 137/200, Loss: 0.4815\n",
      "Epoch 138/200, Loss: 0.4732\n",
      "Epoch 139/200, Loss: 0.4650\n",
      "Epoch 140/200, Loss: 0.4569\n",
      "Epoch 141/200, Loss: 0.4490\n",
      "Epoch 142/200, Loss: 0.4412\n",
      "Epoch 143/200, Loss: 0.4336\n",
      "Epoch 144/200, Loss: 0.4261\n",
      "Epoch 145/200, Loss: 0.4187\n",
      "Epoch 146/200, Loss: 0.4116\n",
      "Epoch 147/200, Loss: 0.4046\n",
      "Epoch 148/200, Loss: 0.3977\n",
      "Epoch 149/200, Loss: 0.3910\n",
      "Epoch 150/200, Loss: 0.3845\n",
      "Epoch 151/200, Loss: 0.3782\n",
      "Epoch 152/200, Loss: 0.3721\n",
      "Epoch 153/200, Loss: 0.3661\n",
      "Epoch 154/200, Loss: 0.3603\n",
      "Epoch 155/200, Loss: 0.3546\n",
      "Epoch 156/200, Loss: 0.3490\n",
      "Epoch 157/200, Loss: 0.3436\n",
      "Epoch 158/200, Loss: 0.3384\n",
      "Epoch 159/200, Loss: 0.3333\n",
      "Epoch 160/200, Loss: 0.3283\n",
      "Epoch 161/200, Loss: 0.3235\n",
      "Epoch 162/200, Loss: 0.3188\n",
      "Epoch 163/200, Loss: 0.3142\n",
      "Epoch 164/200, Loss: 0.3097\n",
      "Epoch 165/200, Loss: 0.3054\n",
      "Epoch 166/200, Loss: 0.3012\n",
      "Epoch 167/200, Loss: 0.2971\n",
      "Epoch 168/200, Loss: 0.2931\n",
      "Epoch 169/200, Loss: 0.2892\n",
      "Epoch 170/200, Loss: 0.2854\n",
      "Epoch 171/200, Loss: 0.2816\n",
      "Epoch 172/200, Loss: 0.2780\n",
      "Epoch 173/200, Loss: 0.2744\n",
      "Epoch 174/200, Loss: 0.2709\n",
      "Epoch 175/200, Loss: 0.2674\n",
      "Epoch 176/200, Loss: 0.2641\n",
      "Epoch 177/200, Loss: 0.2608\n",
      "Epoch 178/200, Loss: 0.2576\n",
      "Epoch 179/200, Loss: 0.2544\n",
      "Epoch 180/200, Loss: 0.2513\n",
      "Epoch 181/200, Loss: 0.2483\n",
      "Epoch 182/200, Loss: 0.2453\n",
      "Epoch 183/200, Loss: 0.2424\n",
      "Epoch 184/200, Loss: 0.2395\n",
      "Epoch 185/200, Loss: 0.2366\n",
      "Epoch 186/200, Loss: 0.2339\n",
      "Epoch 187/200, Loss: 0.2311\n",
      "Epoch 188/200, Loss: 0.2284\n",
      "Epoch 189/200, Loss: 0.2258\n",
      "Epoch 190/200, Loss: 0.2232\n",
      "Epoch 191/200, Loss: 0.2207\n",
      "Epoch 192/200, Loss: 0.2182\n",
      "Epoch 193/200, Loss: 0.2158\n",
      "Epoch 194/200, Loss: 0.2133\n",
      "Epoch 195/200, Loss: 0.2109\n",
      "Epoch 196/200, Loss: 0.2086\n",
      "Epoch 197/200, Loss: 0.2063\n",
      "Epoch 198/200, Loss: 0.2040\n",
      "Epoch 199/200, Loss: 0.2018\n",
      "Epoch 200/200, Loss: 0.1995\n",
      "layer: [1024, 100, 100, 100, 100, 5]\n",
      "Epoch 1/200, Loss: 1.6092\n",
      "Epoch 2/200, Loss: 1.6092\n",
      "Epoch 3/200, Loss: 1.6092\n",
      "Epoch 4/200, Loss: 1.6092\n",
      "Epoch 5/200, Loss: 1.6092\n",
      "Epoch 6/200, Loss: 1.6091\n",
      "Epoch 7/200, Loss: 1.6091\n",
      "Epoch 8/200, Loss: 1.6091\n",
      "Epoch 9/200, Loss: 1.6091\n",
      "Epoch 10/200, Loss: 1.6091\n",
      "Epoch 11/200, Loss: 1.6091\n",
      "Epoch 12/200, Loss: 1.6091\n",
      "Epoch 13/200, Loss: 1.6091\n",
      "Epoch 14/200, Loss: 1.6091\n",
      "Epoch 15/200, Loss: 1.6091\n",
      "Epoch 16/200, Loss: 1.6091\n",
      "Epoch 17/200, Loss: 1.6091\n",
      "Epoch 18/200, Loss: 1.6091\n",
      "Epoch 19/200, Loss: 1.6091\n",
      "Epoch 20/200, Loss: 1.6091\n",
      "Epoch 21/200, Loss: 1.6091\n",
      "Epoch 22/200, Loss: 1.6091\n",
      "Epoch 23/200, Loss: 1.6091\n",
      "Epoch 24/200, Loss: 1.6091\n",
      "Epoch 25/200, Loss: 1.6091\n",
      "Epoch 26/200, Loss: 1.6091\n",
      "Epoch 27/200, Loss: 1.6091\n",
      "Epoch 28/200, Loss: 1.6091\n",
      "Epoch 29/200, Loss: 1.6091\n",
      "Epoch 30/200, Loss: 1.6091\n",
      "Epoch 31/200, Loss: 1.6091\n",
      "Epoch 32/200, Loss: 1.6091\n",
      "Epoch 33/200, Loss: 1.6091\n",
      "Epoch 34/200, Loss: 1.6091\n",
      "Epoch 35/200, Loss: 1.6091\n",
      "Epoch 36/200, Loss: 1.6091\n",
      "Epoch 37/200, Loss: 1.6091\n",
      "Epoch 38/200, Loss: 1.6091\n",
      "Epoch 39/200, Loss: 1.6091\n",
      "Epoch 40/200, Loss: 1.6091\n",
      "Epoch 41/200, Loss: 1.6091\n",
      "Epoch 42/200, Loss: 1.6091\n",
      "Epoch 43/200, Loss: 1.6091\n",
      "Epoch 44/200, Loss: 1.6091\n",
      "Epoch 45/200, Loss: 1.6091\n",
      "Epoch 46/200, Loss: 1.6091\n",
      "Epoch 47/200, Loss: 1.6091\n",
      "Epoch 48/200, Loss: 1.6091\n",
      "Epoch 49/200, Loss: 1.6091\n",
      "Epoch 50/200, Loss: 1.6091\n",
      "Epoch 51/200, Loss: 1.6091\n",
      "Epoch 52/200, Loss: 1.6091\n",
      "Epoch 53/200, Loss: 1.6091\n",
      "Epoch 54/200, Loss: 1.6091\n",
      "Epoch 55/200, Loss: 1.6091\n",
      "Epoch 56/200, Loss: 1.6091\n",
      "Epoch 57/200, Loss: 1.6091\n",
      "Epoch 58/200, Loss: 1.6091\n",
      "Epoch 59/200, Loss: 1.6091\n",
      "Epoch 60/200, Loss: 1.6091\n",
      "Epoch 61/200, Loss: 1.6091\n",
      "Epoch 62/200, Loss: 1.6091\n",
      "Epoch 63/200, Loss: 1.6091\n",
      "Epoch 64/200, Loss: 1.6091\n",
      "Epoch 65/200, Loss: 1.6091\n",
      "Epoch 66/200, Loss: 1.6091\n",
      "Epoch 67/200, Loss: 1.6091\n",
      "Epoch 68/200, Loss: 1.6091\n",
      "Epoch 69/200, Loss: 1.6091\n",
      "Epoch 70/200, Loss: 1.6091\n",
      "Epoch 71/200, Loss: 1.6091\n",
      "Epoch 72/200, Loss: 1.6091\n",
      "Epoch 73/200, Loss: 1.6091\n",
      "Epoch 74/200, Loss: 1.6091\n",
      "Epoch 75/200, Loss: 1.6091\n",
      "Epoch 76/200, Loss: 1.6091\n",
      "Epoch 77/200, Loss: 1.6091\n",
      "Epoch 78/200, Loss: 1.6091\n",
      "Epoch 79/200, Loss: 1.6091\n",
      "Epoch 80/200, Loss: 1.6091\n",
      "Epoch 81/200, Loss: 1.6091\n",
      "Epoch 82/200, Loss: 1.6091\n",
      "Epoch 83/200, Loss: 1.6091\n",
      "Epoch 84/200, Loss: 1.6091\n",
      "Epoch 85/200, Loss: 1.6091\n",
      "Epoch 86/200, Loss: 1.6091\n",
      "Epoch 87/200, Loss: 1.6091\n",
      "Epoch 88/200, Loss: 1.6091\n",
      "Epoch 89/200, Loss: 1.6091\n",
      "Epoch 90/200, Loss: 1.6091\n",
      "Epoch 91/200, Loss: 1.6091\n",
      "Epoch 92/200, Loss: 1.6091\n",
      "Epoch 93/200, Loss: 1.6091\n",
      "Epoch 94/200, Loss: 1.6091\n",
      "Epoch 95/200, Loss: 1.6091\n",
      "Epoch 96/200, Loss: 1.6091\n",
      "Epoch 97/200, Loss: 1.6091\n",
      "Epoch 98/200, Loss: 1.6091\n",
      "Epoch 99/200, Loss: 1.6091\n",
      "Epoch 100/200, Loss: 1.6091\n",
      "Epoch 101/200, Loss: 1.6091\n",
      "Epoch 102/200, Loss: 1.6091\n",
      "Epoch 103/200, Loss: 1.6091\n",
      "Epoch 104/200, Loss: 1.6091\n",
      "Epoch 105/200, Loss: 1.6091\n",
      "Epoch 106/200, Loss: 1.6091\n",
      "Epoch 107/200, Loss: 1.6091\n",
      "Epoch 108/200, Loss: 1.6091\n",
      "Epoch 109/200, Loss: 1.6091\n",
      "Epoch 110/200, Loss: 1.6091\n",
      "Epoch 111/200, Loss: 1.6091\n",
      "Epoch 112/200, Loss: 1.6091\n",
      "Epoch 113/200, Loss: 1.6091\n",
      "Epoch 114/200, Loss: 1.6091\n",
      "Epoch 115/200, Loss: 1.6091\n",
      "Epoch 116/200, Loss: 1.6091\n",
      "Epoch 117/200, Loss: 1.6091\n",
      "Epoch 118/200, Loss: 1.6091\n",
      "Epoch 119/200, Loss: 1.6091\n",
      "Epoch 120/200, Loss: 1.6091\n",
      "Epoch 121/200, Loss: 1.6091\n",
      "Epoch 122/200, Loss: 1.6091\n",
      "Epoch 123/200, Loss: 1.6091\n",
      "Epoch 124/200, Loss: 1.6091\n",
      "Epoch 125/200, Loss: 1.6091\n",
      "Epoch 126/200, Loss: 1.6091\n",
      "Epoch 127/200, Loss: 1.6091\n",
      "Epoch 128/200, Loss: 1.6091\n",
      "Epoch 129/200, Loss: 1.6091\n",
      "Epoch 130/200, Loss: 1.6091\n",
      "Epoch 131/200, Loss: 1.6091\n",
      "Epoch 132/200, Loss: 1.6091\n",
      "Epoch 133/200, Loss: 1.6091\n",
      "Epoch 134/200, Loss: 1.6091\n",
      "Epoch 135/200, Loss: 1.6091\n",
      "Epoch 136/200, Loss: 1.6091\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlayer: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlayer\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      3\u001B[0m nn \u001B[38;5;241m=\u001B[39m NeuralNetwork(layer)\n\u001B[0;32m----> 4\u001B[0m \u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_c\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_train\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mT\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train_one_hot\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m200\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmini_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m32\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconv_threshold\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1e-9\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconv_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m models_e[\u001B[38;5;28mstr\u001B[39m(layer)] \u001B[38;5;241m=\u001B[39m nn\n",
      "Cell \u001B[0;32mIn[9], line 20\u001B[0m, in \u001B[0;36mNeuralNetwork.train_c\u001B[0;34m(self, x_train, y_train, epochs, mini_batch_size, learning_rate, conv_threshold, conv_epochs)\u001B[0m\n\u001B[1;32m     18\u001B[0m mini_batch_y \u001B[38;5;241m=\u001B[39m y_train_shuffled[:, k:k \u001B[38;5;241m+\u001B[39m mini_batch_size]\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[0;32m---> 20\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeedforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmini_batch_x\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# Backward pass\u001B[39;00m\n\u001B[1;32m     22\u001B[0m d_weights, d_biases \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbackpropagation(mini_batch_y)\n",
      "Cell \u001B[0;32mIn[4], line 48\u001B[0m, in \u001B[0;36mNeuralNetwork.feedforward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;66;03m# Compute activations for each layer\u001B[39;00m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m w, b \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweights, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbiases):\n\u001B[0;32m---> 48\u001B[0m     z \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mw\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mactivation\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m+\u001B[39m b\n\u001B[1;32m     49\u001B[0m     activation \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrelu(z) \u001B[38;5;28;01mif\u001B[39;00m w \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweights[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msoftmax(z)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactivations\u001B[38;5;241m.\u001B[39mappend(activation)\n",
      "File \u001B[0;32m<__array_function__ internals>:180\u001B[0m, in \u001B[0;36mdot\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for layer in layers[-2:]:\n",
    "    print(f'layer: {layer}')\n",
    "    nn = NeuralNetwork(layer)\n",
    "    nn.train_c(x_train.T, y_train_one_hot, epochs=200, mini_batch_size=32, learning_rate=0.01, conv_threshold=1e-9, conv_epochs=10)\n",
    "    models_e[str(layer)] = nn"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T15:12:06.669944Z",
     "start_time": "2023-11-04T15:09:43.741898Z"
    }
   },
   "id": "ef9a9593a8c89cd3"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: [1024, 100, 100, 100, 100, 5]\n",
      "Epoch 1/200, Loss: 1.6157\n",
      "Epoch 2/200, Loss: 1.6157\n",
      "Epoch 3/200, Loss: 1.6157\n",
      "Epoch 4/200, Loss: 1.6157\n",
      "Epoch 5/200, Loss: 1.6157\n",
      "Epoch 6/200, Loss: 1.6157\n",
      "Epoch 7/200, Loss: 1.6157\n",
      "Epoch 8/200, Loss: 1.6157\n",
      "Epoch 9/200, Loss: 1.6157\n",
      "Epoch 10/200, Loss: 1.6157\n",
      "Epoch 11/200, Loss: 1.6157\n",
      "Converged\n"
     ]
    }
   ],
   "source": [
    "layer = layers[-1]\n",
    "print(f'layer: {layer}')\n",
    "nn = NeuralNetwork(layer)\n",
    "for\n",
    "nn.train(x_train.T, y_train_one_hot, epochs=200, mini_batch_size=32, learning_rate=1, conv_threshold=1e-6, conv_epochs=10)\n",
    "models_e[str(layer)] = nn"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T16:14:27.601260Z",
     "start_time": "2023-11-04T16:14:19.691200Z"
    }
   },
   "id": "d8d02f07498399fc"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# save models\n",
    "import pickle\n",
    "\n",
    "with open('pickles/models_e.pickle', 'wb') as handle:\n",
    "    pickle.dump(models_e, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T13:46:26.919139Z",
     "start_time": "2023-11-04T13:46:26.598689Z"
    }
   },
   "id": "1ed8f4bd1d247cdf"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1024, 100, 5] hidden layer size\n",
      "Training\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      1988\n",
      "           1       0.95      0.95      0.95      1983\n",
      "           2       0.90      0.91      0.90      1930\n",
      "           3       0.88      0.88      0.88      2015\n",
      "           4       0.94      0.94      0.94      2084\n",
      "\n",
      "    accuracy                           0.93     10000\n",
      "   macro avg       0.93      0.93      0.93     10000\n",
      "weighted avg       0.93      0.93      0.93     10000\n",
      "\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       223\n",
      "           1       0.87      0.92      0.89       187\n",
      "           2       0.79      0.81      0.80       194\n",
      "           3       0.74      0.72      0.73       193\n",
      "           4       0.89      0.82      0.85       203\n",
      "\n",
      "    accuracy                           0.86      1000\n",
      "   macro avg       0.85      0.85      0.85      1000\n",
      "weighted avg       0.86      0.86      0.86      1000\n",
      "\n",
      "[1024, 100, 100, 5] hidden layer size\n",
      "Training\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00      1980\n",
      "           1       0.99      0.99      0.99      1979\n",
      "           2       0.96      0.97      0.97      1932\n",
      "           3       0.94      0.95      0.94      2001\n",
      "           4       0.97      0.96      0.97      2108\n",
      "\n",
      "    accuracy                           0.97     10000\n",
      "   macro avg       0.97      0.97      0.97     10000\n",
      "weighted avg       0.97      0.97      0.97     10000\n",
      "\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       225\n",
      "           1       0.90      0.94      0.92       189\n",
      "           2       0.82      0.86      0.84       191\n",
      "           3       0.76      0.75      0.75       190\n",
      "           4       0.89      0.81      0.85       205\n",
      "\n",
      "    accuracy                           0.88      1000\n",
      "   macro avg       0.87      0.87      0.87      1000\n",
      "weighted avg       0.87      0.88      0.87      1000\n",
      "[1024, 100, 100, 100, 5] hidden layer size\n",
      "Training\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       1.00      0.21      0.35     10000\n",
      "\n",
      "    accuracy                           0.21     10000\n",
      "   macro avg       0.20      0.04      0.07     10000\n",
      "weighted avg       1.00      0.21      0.35     10000\n",
      "\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       1.00      0.19      0.32      1000\n",
      "\n",
      "    accuracy                           0.19      1000\n",
      "   macro avg       0.20      0.04      0.06      1000\n",
      "weighted avg       1.00      0.19      0.32      1000\n",
      "\n",
      "[1024, 100, 100, 100, 100, 5] hidden layer size\n",
      "Training\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       1.00      0.21      0.35     10000\n",
      "\n",
      "    accuracy                           0.21     10000\n",
      "   macro avg       0.20      0.04      0.07     10000\n",
      "weighted avg       1.00      0.21      0.35     10000\n",
      "\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       1.00      0.19      0.32      1000\n",
      "\n",
      "    accuracy                           0.19      1000\n",
      "   macro avg       0.20      0.04      0.06      1000\n",
      "weighted avg       1.00      0.19      0.32      1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# report results\n",
    "for layer in layers:\n",
    "    nn = models_e[str(layer)]\n",
    "    y_train_pred = np.argmax(nn.feedforward(x_train.T), axis=0)\n",
    "    y_test_pred = np.argmax(nn.feedforward(x_test.T), axis=0)\n",
    "    results = classification_report(y_train_pred, y_train)\n",
    "    print(f\"{layer} hidden layer size\")\n",
    "    print('Training')\n",
    "    print(results)\n",
    "    \n",
    "    results = classification_report(y_test_pred, y_test)\n",
    "    print('Test')\n",
    "    print(results)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T13:46:27.409862Z",
     "start_time": "2023-11-04T13:46:26.897258Z"
    }
   },
   "id": "2388e4f84d9812fa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "avg_f1_scores_training = []\n",
    "avg_f1_scores_test = []\n",
    "for layer in layers:\n",
    "    nn = models_e[str(layer)]\n",
    "    y_train_pred = np.argmax(nn.feedforward(x_train.T), axis=0)\n",
    "    y_test_pred = np.argmax(nn.feedforward(x_test.T), axis=0)\n",
    "    results = classification_report(y_train_pred, y_train, output_dict=True)\n",
    "    avg_f1_scores_training.append(results['weighted avg']['f1-score'])\n",
    "    \n",
    "    results = classification_report(y_test_pred, y_test, output_dict=True)\n",
    "    avg_f1_scores_test.append(results['weighted avg']['f1-score'])\n",
    "    \n",
    "plt.plot([len(layer) for layer in layers], avg_f1_scores_training, label = 'Training')\n",
    "plt.plot([len(layer) for layer in layers], avg_f1_scores_test, label = 'Test')\n",
    "plt.xlabel('Hidden Layers Depth')\n",
    "plt.ylabel('Average F1 Score')\n",
    "plt.legend()\n",
    "plt.savefig('(e) relu adaptive training f1 vs hidden_depth.png')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-04T13:46:27.913749Z"
    }
   },
   "id": "3baa6aa5e40f560e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d3d01210aa9b4c89"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
