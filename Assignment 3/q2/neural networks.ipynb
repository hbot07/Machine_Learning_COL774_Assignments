{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-04T15:49:31.007313Z",
     "start_time": "2023-11-04T15:49:30.282136Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import sys\n",
    "import pdb\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def get_data(x_path, y_path):\n",
    "    '''\n",
    "    Args:\n",
    "        x_path: path to x file\n",
    "        y_path: path to y file\n",
    "    Returns:\n",
    "        x: np array of [NUM_OF_SAMPLES x n]\n",
    "        y: np array of [NUM_OF_SAMPLES]\n",
    "    '''\n",
    "    x = np.load(x_path)\n",
    "    y = np.load(y_path)\n",
    "\n",
    "    x = x.astype('float')\n",
    "\n",
    "    #normalize each example in x to have 0 mean and 1 std\n",
    "    \n",
    "    # Calculate the mean and standard deviation for each feature\n",
    "    feature_means = np.mean(x, axis=0)\n",
    "    feature_stds = np.std(x, axis=0)\n",
    "    feature_stds = feature_stds + (feature_stds == 0)\n",
    "\n",
    "    # Normalize each feature to have 0 mean and 1 std\n",
    "    x = (x - feature_means) / (feature_stds)\n",
    "    \n",
    "    # Adjust labels to start from 0 if they start from 1\n",
    "    y = y - 1\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "def get_metric(y_true, y_pred):\n",
    "    '''\n",
    "    Args:\n",
    "        y_true: np array of [NUM_SAMPLES x r] (one hot) \n",
    "                or np array of [NUM_SAMPLES]\n",
    "        y_pred: np array of [NUM_SAMPLES x r] (one hot) \n",
    "                or np array of [NUM_SAMPLES]\n",
    "                \n",
    "    '''\n",
    "    results = classification_report(y_pred, y_true)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "outputs": [
    {
     "data": {
      "text/plain": "((10000, 1024), (10000,), (1000, 1024), (1000,))"
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test, y_test = get_data('x_test.npy', 'y_test.npy')\n",
    "x_train, y_train = get_data('x_train.npy', 'y_train.npy')\n",
    "\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T20:48:14.907282Z",
     "start_time": "2023-11-03T20:48:14.709194Z"
    }
   },
   "id": "55dc64fea5b7f37c"
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [
    {
     "data": {
      "text/plain": "array([-0.39813868,  0.1879447 ,  0.91463024, ..., -0.15117837,\n        0.26519742,  0.        ])"
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T20:48:14.915988Z",
     "start_time": "2023-11-03T20:48:14.907635Z"
    }
   },
   "id": "730b00b27185555f"
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [],
   "source": [
    "label_encoder = OneHotEncoder(sparse_output = False)\n",
    "label_encoder.fit(np.expand_dims(y_train, axis = -1))\n",
    "\n",
    "y_train_onehot = label_encoder.transform(np.expand_dims(y_train, axis = -1))\n",
    "y_test_onehot = label_encoder.transform(np.expand_dims(y_test, axis = -1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T20:48:14.961634Z",
     "start_time": "2023-11-03T20:48:14.917795Z"
    }
   },
   "id": "9c612b2b9691a17d"
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.21001698],\n       [0.18440967],\n       [0.20223693],\n       [0.20182329],\n       [0.20151313]])"
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layer_sizes):\n",
    "        \"\"\"\n",
    "        Initializes the neural network with the given layer sizes.\n",
    "        layer_sizes is a list of integers, where the i-th integer represents\n",
    "        the number of neurons in the i-th layer.\n",
    "        \"\"\"\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.activations = []\n",
    "\n",
    "        # Initialize weights and biases for each layer\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            # Weights are initialized with small random values\n",
    "            self.weights.append(np.random.randn(layer_sizes[i+1], layer_sizes[i]) * 0.01)\n",
    "            self.biases.append(np.zeros((layer_sizes[i+1], 1)))\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        The sigmoid activation function.\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def softmax(self, z):\n",
    "        \"\"\"\n",
    "        The softmax function.\n",
    "        \"\"\"\n",
    "        e_z = np.exp(z)  # Subtracting np.max(z) for numerical stability\n",
    "        return e_z / e_z.sum(axis=0, keepdims=True)\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        \"\"\"\n",
    "        Performs a feedforward computation.\n",
    "        \"\"\"\n",
    "        activation = x\n",
    "        self.activations = [x]  # List to store all the activations, layer by layer\n",
    "\n",
    "        # Compute activations for each layer\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(w, activation) + b\n",
    "            activation = self.sigmoid(z) if w is not self.weights[-1] else self.softmax(z)\n",
    "            self.activations.append(activation)\n",
    "\n",
    "        return self.activations[-1]  # The final activation is the output of the network\n",
    "\n",
    "# Let's test the initialization and feedforward computation with a small network\n",
    "nn = NeuralNetwork([1024, 100, 5])  # A network with 1024 input features, one hidden layer with 100 neurons, and 5 output classes\n",
    "sample_input = np.random.randn(1024, 1)  # A random sample input\n",
    "output = nn.feedforward(sample_input)  # Perform a feedforward computation\n",
    "\n",
    "output  # Display the output probabilities"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T20:48:14.971503Z",
     "start_time": "2023-11-03T20:48:14.952378Z"
    }
   },
   "id": "c7e0def69df983a4"
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [],
   "source": [
    "class NeuralNetwork(NeuralNetwork):  # Extending the previously defined NeuralNetwork class\n",
    "    def cross_entropy_loss(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Computes the cross-entropy loss.\n",
    "        \"\"\"\n",
    "        m = y_true.shape[1]  # Number of examples\n",
    "        # To avoid division by zero, we clip the predictions to a minimum value\n",
    "        # y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        # Compute the cross-entropy loss\n",
    "        loss = -np.sum(y_true * np.log(y_pred)) / m\n",
    "        return loss\n",
    "\n",
    "    def backpropagation(self, y_true):\n",
    "        \"\"\"\n",
    "        Performs backpropagation to compute the gradients of the loss function\n",
    "        with respect to the weights and biases.\n",
    "        \"\"\"\n",
    "        m = y_true.shape[1]  # Number of examples\n",
    "        y_pred = self.activations[-1]  # The output of the last layer\n",
    "        y_true = y_true.reshape(y_pred.shape)  # Ensure same shape\n",
    "\n",
    "        # Initialize gradients for each layer\n",
    "        d_weights = [np.zeros(w.shape) for w in self.weights]\n",
    "        d_biases = [np.zeros(b.shape) for b in self.biases]\n",
    "\n",
    "        # Calculate derivative of loss w.r.t. the last layer output\n",
    "        d_loss = y_pred - y_true\n",
    "\n",
    "        for i in reversed(range(len(d_weights))):\n",
    "            d_activations = d_loss * self.sigmoid_derivative(self.activations[i+1]) if i != len(d_weights) - 1 else d_loss\n",
    "            d_weights[i] = np.dot(d_activations, self.activations[i].T) / m\n",
    "            d_biases[i] = np.sum(d_activations, axis=1, keepdims=True) / m\n",
    "            if i != 0:\n",
    "                d_loss = np.dot(self.weights[i].T, d_activations)\n",
    "\n",
    "        return d_weights, d_biases\n",
    "\n",
    "    def sigmoid_derivative(self, s):\n",
    "        \"\"\"\n",
    "        Derivative of the sigmoid function.\n",
    "        \"\"\"\n",
    "        return s * (1 - s)\n",
    "\n",
    "    def update_parameters(self, d_weights, d_biases, learning_rate):\n",
    "        \"\"\"\n",
    "        Updates the parameters using the computed gradients.\n",
    "        \"\"\"\n",
    "        # Update each parameter with a simple gradient descent step\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= learning_rate * d_weights[i]\n",
    "            self.biases[i] -= learning_rate * d_biases[i]\n",
    "            \n",
    "    def train(self, x_train, y_train, epochs, mini_batch_size, learning_rate, conv_threshold = 0.001, conv_epochs = 5):\n",
    "        n = x_train.shape[1]  # Total number of training examples\n",
    "\n",
    "        # Training loop\n",
    "        loss_history = []\n",
    "        permutation = np.random.permutation(n)\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle the training data for each epoch\n",
    "            x_train_shuffled = x_train[:, permutation]\n",
    "            y_train_shuffled = y_train[:, permutation]\n",
    "\n",
    "            # Mini-batch loop\n",
    "            for k in range(0, n, mini_batch_size):\n",
    "                mini_batch_x = x_train_shuffled[:, k:k + mini_batch_size]\n",
    "                mini_batch_y = y_train_shuffled[:, k:k + mini_batch_size]\n",
    "                # Forward pass\n",
    "                self.feedforward(mini_batch_x)\n",
    "                # Backward pass\n",
    "                d_weights, d_biases = self.backpropagation(mini_batch_y)\n",
    "                # Update parameters\n",
    "                self.update_parameters(d_weights, d_biases, learning_rate)\n",
    "            \n",
    "            loss = self.cross_entropy_loss(self.feedforward(x_train), y_train)\n",
    "            # Optional: Print the loss after each epoch (can be commented out for speed)\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}\")\n",
    "            \n",
    "            loss_history.append(loss)\n",
    "            \n",
    "            if len(loss_history) > conv_epochs:\n",
    "                temp = loss_history[-conv_epochs:]\n",
    "                if np.std(temp) < conv_threshold:\n",
    "                    print('Converged')\n",
    "                    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T20:56:11.482667Z",
     "start_time": "2023-11-03T20:56:11.479660Z"
    }
   },
   "id": "9798dcda491c7393"
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 1.3209\n",
      "Epoch 2/1000, Loss: 0.9888\n",
      "Epoch 3/1000, Loss: 0.8633\n",
      "Epoch 4/1000, Loss: 0.7913\n",
      "Epoch 5/1000, Loss: 0.7416\n",
      "Epoch 6/1000, Loss: 0.7035\n",
      "Epoch 7/1000, Loss: 0.6723\n",
      "Epoch 8/1000, Loss: 0.6458\n",
      "Epoch 9/1000, Loss: 0.6226\n",
      "Epoch 10/1000, Loss: 0.6021\n",
      "Epoch 11/1000, Loss: 0.5837\n",
      "Epoch 12/1000, Loss: 0.5670\n",
      "Epoch 13/1000, Loss: 0.5519\n",
      "Epoch 14/1000, Loss: 0.5381\n",
      "Epoch 15/1000, Loss: 0.5255\n",
      "Epoch 16/1000, Loss: 0.5138\n",
      "Epoch 17/1000, Loss: 0.5030\n",
      "Epoch 18/1000, Loss: 0.4930\n",
      "Epoch 19/1000, Loss: 0.4837\n",
      "Epoch 20/1000, Loss: 0.4749\n",
      "Epoch 21/1000, Loss: 0.4667\n",
      "Epoch 22/1000, Loss: 0.4590\n",
      "Epoch 23/1000, Loss: 0.4517\n",
      "Epoch 24/1000, Loss: 0.4448\n",
      "Epoch 25/1000, Loss: 0.4383\n",
      "Epoch 26/1000, Loss: 0.4321\n",
      "Epoch 27/1000, Loss: 0.4262\n",
      "Epoch 28/1000, Loss: 0.4205\n",
      "Epoch 29/1000, Loss: 0.4151\n",
      "Epoch 30/1000, Loss: 0.4100\n",
      "Epoch 31/1000, Loss: 0.4050\n",
      "Epoch 32/1000, Loss: 0.4003\n",
      "Epoch 33/1000, Loss: 0.3957\n",
      "Epoch 34/1000, Loss: 0.3913\n",
      "Epoch 35/1000, Loss: 0.3871\n",
      "Epoch 36/1000, Loss: 0.3830\n",
      "Epoch 37/1000, Loss: 0.3790\n",
      "Epoch 38/1000, Loss: 0.3752\n",
      "Epoch 39/1000, Loss: 0.3715\n",
      "Epoch 40/1000, Loss: 0.3679\n",
      "Epoch 41/1000, Loss: 0.3644\n",
      "Epoch 42/1000, Loss: 0.3610\n",
      "Epoch 43/1000, Loss: 0.3577\n",
      "Epoch 44/1000, Loss: 0.3545\n",
      "Epoch 45/1000, Loss: 0.3513\n",
      "Epoch 46/1000, Loss: 0.3483\n",
      "Epoch 47/1000, Loss: 0.3453\n",
      "Epoch 48/1000, Loss: 0.3424\n",
      "Epoch 49/1000, Loss: 0.3395\n",
      "Epoch 50/1000, Loss: 0.3367\n",
      "Epoch 51/1000, Loss: 0.3339\n",
      "Epoch 52/1000, Loss: 0.3312\n",
      "Epoch 53/1000, Loss: 0.3286\n",
      "Epoch 54/1000, Loss: 0.3260\n",
      "Epoch 55/1000, Loss: 0.3234\n",
      "Epoch 56/1000, Loss: 0.3209\n",
      "Epoch 57/1000, Loss: 0.3185\n",
      "Epoch 58/1000, Loss: 0.3160\n",
      "Epoch 59/1000, Loss: 0.3136\n",
      "Epoch 60/1000, Loss: 0.3113\n",
      "Epoch 61/1000, Loss: 0.3090\n",
      "Epoch 62/1000, Loss: 0.3067\n",
      "Epoch 63/1000, Loss: 0.3044\n",
      "Epoch 64/1000, Loss: 0.3022\n",
      "Epoch 65/1000, Loss: 0.3000\n",
      "Epoch 66/1000, Loss: 0.2978\n",
      "Epoch 67/1000, Loss: 0.2956\n",
      "Epoch 68/1000, Loss: 0.2935\n",
      "Epoch 69/1000, Loss: 0.2914\n",
      "Epoch 70/1000, Loss: 0.2893\n",
      "Epoch 71/1000, Loss: 0.2872\n",
      "Epoch 72/1000, Loss: 0.2852\n",
      "Epoch 73/1000, Loss: 0.2832\n",
      "Epoch 74/1000, Loss: 0.2812\n",
      "Epoch 75/1000, Loss: 0.2792\n",
      "Epoch 76/1000, Loss: 0.2772\n",
      "Epoch 77/1000, Loss: 0.2753\n",
      "Epoch 78/1000, Loss: 0.2733\n",
      "Epoch 79/1000, Loss: 0.2714\n",
      "Epoch 80/1000, Loss: 0.2695\n",
      "Epoch 81/1000, Loss: 0.2676\n",
      "Epoch 82/1000, Loss: 0.2658\n",
      "Epoch 83/1000, Loss: 0.2639\n",
      "Epoch 84/1000, Loss: 0.2621\n",
      "Epoch 85/1000, Loss: 0.2602\n",
      "Epoch 86/1000, Loss: 0.2584\n",
      "Epoch 87/1000, Loss: 0.2566\n",
      "Epoch 88/1000, Loss: 0.2548\n",
      "Epoch 89/1000, Loss: 0.2530\n",
      "Epoch 90/1000, Loss: 0.2513\n",
      "Epoch 91/1000, Loss: 0.2495\n",
      "Epoch 92/1000, Loss: 0.2477\n",
      "Epoch 93/1000, Loss: 0.2460\n",
      "Epoch 94/1000, Loss: 0.2443\n",
      "Epoch 95/1000, Loss: 0.2425\n",
      "Epoch 96/1000, Loss: 0.2408\n",
      "Epoch 97/1000, Loss: 0.2391\n",
      "Epoch 98/1000, Loss: 0.2374\n",
      "Epoch 99/1000, Loss: 0.2357\n",
      "Epoch 100/1000, Loss: 0.2340\n",
      "Epoch 101/1000, Loss: 0.2324\n",
      "Epoch 102/1000, Loss: 0.2307\n",
      "Epoch 103/1000, Loss: 0.2291\n",
      "Epoch 104/1000, Loss: 0.2274\n",
      "Epoch 105/1000, Loss: 0.2258\n",
      "Epoch 106/1000, Loss: 0.2242\n",
      "Epoch 107/1000, Loss: 0.2225\n",
      "Epoch 108/1000, Loss: 0.2209\n",
      "Epoch 109/1000, Loss: 0.2193\n",
      "Epoch 110/1000, Loss: 0.2177\n",
      "Epoch 111/1000, Loss: 0.2161\n",
      "Epoch 112/1000, Loss: 0.2146\n",
      "Epoch 113/1000, Loss: 0.2130\n",
      "Epoch 114/1000, Loss: 0.2114\n",
      "Epoch 115/1000, Loss: 0.2099\n",
      "Epoch 116/1000, Loss: 0.2083\n",
      "Epoch 117/1000, Loss: 0.2068\n",
      "Epoch 118/1000, Loss: 0.2052\n",
      "Epoch 119/1000, Loss: 0.2037\n",
      "Epoch 120/1000, Loss: 0.2022\n",
      "Epoch 121/1000, Loss: 0.2007\n",
      "Epoch 122/1000, Loss: 0.1992\n",
      "Epoch 123/1000, Loss: 0.1977\n",
      "Epoch 124/1000, Loss: 0.1962\n",
      "Epoch 125/1000, Loss: 0.1947\n",
      "Epoch 126/1000, Loss: 0.1933\n",
      "Epoch 127/1000, Loss: 0.1918\n",
      "Epoch 128/1000, Loss: 0.1903\n",
      "Epoch 129/1000, Loss: 0.1889\n",
      "Epoch 130/1000, Loss: 0.1874\n",
      "Epoch 131/1000, Loss: 0.1860\n",
      "Epoch 132/1000, Loss: 0.1846\n",
      "Epoch 133/1000, Loss: 0.1832\n",
      "Epoch 134/1000, Loss: 0.1818\n",
      "Epoch 135/1000, Loss: 0.1804\n",
      "Epoch 136/1000, Loss: 0.1790\n",
      "Epoch 137/1000, Loss: 0.1776\n",
      "Epoch 138/1000, Loss: 0.1762\n",
      "Epoch 139/1000, Loss: 0.1748\n",
      "Epoch 140/1000, Loss: 0.1735\n",
      "Epoch 141/1000, Loss: 0.1721\n",
      "Epoch 142/1000, Loss: 0.1708\n",
      "Epoch 143/1000, Loss: 0.1694\n",
      "Epoch 144/1000, Loss: 0.1681\n",
      "Epoch 145/1000, Loss: 0.1668\n",
      "Epoch 146/1000, Loss: 0.1655\n",
      "Epoch 147/1000, Loss: 0.1642\n",
      "Epoch 148/1000, Loss: 0.1629\n",
      "Epoch 149/1000, Loss: 0.1616\n",
      "Epoch 150/1000, Loss: 0.1603\n",
      "Epoch 151/1000, Loss: 0.1590\n",
      "Epoch 152/1000, Loss: 0.1578\n",
      "Epoch 153/1000, Loss: 0.1565\n",
      "Epoch 154/1000, Loss: 0.1552\n",
      "Epoch 155/1000, Loss: 0.1540\n",
      "Epoch 156/1000, Loss: 0.1528\n",
      "Epoch 157/1000, Loss: 0.1515\n",
      "Epoch 158/1000, Loss: 0.1503\n",
      "Epoch 159/1000, Loss: 0.1491\n",
      "Epoch 160/1000, Loss: 0.1479\n",
      "Epoch 161/1000, Loss: 0.1467\n",
      "Epoch 162/1000, Loss: 0.1455\n",
      "Epoch 163/1000, Loss: 0.1443\n",
      "Epoch 164/1000, Loss: 0.1432\n",
      "Epoch 165/1000, Loss: 0.1420\n",
      "Epoch 166/1000, Loss: 0.1408\n",
      "Epoch 167/1000, Loss: 0.1397\n",
      "Epoch 168/1000, Loss: 0.1386\n",
      "Epoch 169/1000, Loss: 0.1374\n",
      "Epoch 170/1000, Loss: 0.1363\n",
      "Epoch 171/1000, Loss: 0.1352\n",
      "Epoch 172/1000, Loss: 0.1341\n",
      "Epoch 173/1000, Loss: 0.1330\n",
      "Epoch 174/1000, Loss: 0.1319\n",
      "Epoch 175/1000, Loss: 0.1308\n",
      "Epoch 176/1000, Loss: 0.1297\n",
      "Epoch 177/1000, Loss: 0.1287\n",
      "Epoch 178/1000, Loss: 0.1276\n",
      "Epoch 179/1000, Loss: 0.1265\n",
      "Epoch 180/1000, Loss: 0.1255\n",
      "Epoch 181/1000, Loss: 0.1245\n",
      "Epoch 182/1000, Loss: 0.1234\n",
      "Epoch 183/1000, Loss: 0.1224\n",
      "Epoch 184/1000, Loss: 0.1214\n",
      "Epoch 185/1000, Loss: 0.1204\n",
      "Epoch 186/1000, Loss: 0.1194\n",
      "Epoch 187/1000, Loss: 0.1184\n",
      "Epoch 188/1000, Loss: 0.1174\n",
      "Epoch 189/1000, Loss: 0.1165\n",
      "Epoch 190/1000, Loss: 0.1155\n",
      "Epoch 191/1000, Loss: 0.1145\n",
      "Epoch 192/1000, Loss: 0.1136\n",
      "Epoch 193/1000, Loss: 0.1126\n",
      "Epoch 194/1000, Loss: 0.1117\n",
      "Epoch 195/1000, Loss: 0.1108\n",
      "Epoch 196/1000, Loss: 0.1098\n",
      "Epoch 197/1000, Loss: 0.1089\n",
      "Epoch 198/1000, Loss: 0.1080\n",
      "Epoch 199/1000, Loss: 0.1071\n",
      "Epoch 200/1000, Loss: 0.1062\n",
      "Epoch 201/1000, Loss: 0.1054\n",
      "Epoch 202/1000, Loss: 0.1045\n",
      "Epoch 203/1000, Loss: 0.1036\n",
      "Epoch 204/1000, Loss: 0.1028\n",
      "Epoch 205/1000, Loss: 0.1019\n",
      "Epoch 206/1000, Loss: 0.1011\n",
      "Epoch 207/1000, Loss: 0.1002\n",
      "Epoch 208/1000, Loss: 0.0994\n",
      "Epoch 209/1000, Loss: 0.0986\n",
      "Epoch 210/1000, Loss: 0.0977\n",
      "Epoch 211/1000, Loss: 0.0969\n",
      "Epoch 212/1000, Loss: 0.0961\n",
      "Epoch 213/1000, Loss: 0.0953\n",
      "Epoch 214/1000, Loss: 0.0946\n",
      "Epoch 215/1000, Loss: 0.0938\n",
      "Epoch 216/1000, Loss: 0.0930\n",
      "Epoch 217/1000, Loss: 0.0922\n",
      "Epoch 218/1000, Loss: 0.0915\n",
      "Epoch 219/1000, Loss: 0.0907\n",
      "Epoch 220/1000, Loss: 0.0900\n",
      "Epoch 221/1000, Loss: 0.0892\n",
      "Epoch 222/1000, Loss: 0.0885\n",
      "Epoch 223/1000, Loss: 0.0878\n",
      "Epoch 224/1000, Loss: 0.0870\n",
      "Epoch 225/1000, Loss: 0.0863\n",
      "Epoch 226/1000, Loss: 0.0856\n",
      "Epoch 227/1000, Loss: 0.0849\n",
      "Epoch 228/1000, Loss: 0.0842\n",
      "Converged\n"
     ]
    }
   ],
   "source": [
    "# Create a fresh instance of the neural network with the corrected train method\n",
    "nn = NeuralNetwork([1024, 100, 5])\n",
    "# Convert labels to one-hot encoding again\n",
    "y_train_one_hot = np.eye(5)[y_train].T\n",
    "\n",
    "# Train the neural network again with the corrected training method\n",
    "nn.train(x_train.T, y_train_one_hot, epochs=1000, mini_batch_size=32, learning_rate=0.01)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T20:57:15.278615Z",
     "start_time": "2023-11-03T20:56:13.657948Z"
    }
   },
   "id": "a087d04d64d29d66"
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 100.0%\n",
      "Test accuracy: 86.2%\n"
     ]
    }
   ],
   "source": [
    "# Compute the accuracy on the training and test sets\n",
    "y_train_pred = np.argmax(nn.feedforward(x_train.T), axis=0)\n",
    "y_test_pred = np.argmax(nn.feedforward(x_test.T), axis=0)\n",
    "print(f\"Training accuracy: {np.mean(y_train_pred == y_train) * 100}%\")\n",
    "print(f\"Test accuracy: {np.mean(y_test_pred == y_test) * 100}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T20:54:16.366737Z",
     "start_time": "2023-11-03T20:54:16.319527Z"
    }
   },
   "id": "ab2897275199bacf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# part b"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "219c7b2a8efafd53"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [
    "hidden_layer_sizes = [1, 5, 10, 50, 100]\n",
    "model_with_hidden_layer_size = {}\n",
    "\n",
    "for hidden_layer_size in hidden_layer_sizes:\n",
    "    \n",
    "    print(f'hidden_layer_size: {hidden_layer_size}')\n",
    "    nn = NeuralNetwork([1024, hidden_layer_size, 5])\n",
    "    nn.train(x_train.T, y_train_one_hot, epochs=1000, mini_batch_size=32, learning_rate=0.01)\n",
    "    model_with_hidden_layer_size[hidden_layer_size] = nn"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "552bcd8d031e5c4a"
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 hidden layer size\n",
      "Training\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.76      0.86      2589\n",
      "           1       0.63      0.89      0.74      1410\n",
      "           2       0.78      0.85      0.81      1782\n",
      "           3       0.40      0.73      0.52      1114\n",
      "           4       0.98      0.66      0.79      3105\n",
      "\n",
      "    accuracy                           0.76     10000\n",
      "   macro avg       0.76      0.78      0.74     10000\n",
      "weighted avg       0.84      0.76      0.78     10000\n",
      "\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.89      0.94       258\n",
      "           1       0.73      0.91      0.81       158\n",
      "           2       0.67      0.78      0.72       170\n",
      "           3       0.32      0.55      0.40       107\n",
      "           4       0.98      0.60      0.74       307\n",
      "\n",
      "    accuracy                           0.75      1000\n",
      "   macro avg       0.74      0.75      0.72      1000\n",
      "weighted avg       0.82      0.75      0.76      1000\n",
      "\n",
      "5 hidden layer size\n",
      "Training\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00      1980\n",
      "           1       0.97      0.97      0.97      1967\n",
      "           2       0.90      0.92      0.91      1902\n",
      "           3       0.85      0.88      0.87      1940\n",
      "           4       0.96      0.91      0.94      2211\n",
      "\n",
      "    accuracy                           0.94     10000\n",
      "   macro avg       0.94      0.94      0.94     10000\n",
      "weighted avg       0.94      0.94      0.94     10000\n",
      "\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       233\n",
      "           1       0.84      0.93      0.89       179\n",
      "           2       0.73      0.78      0.76       185\n",
      "           3       0.58      0.65      0.61       167\n",
      "           4       0.90      0.72      0.80       236\n",
      "\n",
      "    accuracy                           0.82      1000\n",
      "   macro avg       0.81      0.81      0.81      1000\n",
      "weighted avg       0.83      0.82      0.82      1000\n",
      "\n",
      "10 hidden layer size\n",
      "Training\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1977\n",
      "           1       0.98      0.98      0.98      1977\n",
      "           2       0.91      0.94      0.93      1882\n",
      "           3       0.90      0.88      0.89      2053\n",
      "           4       0.95      0.94      0.94      2111\n",
      "\n",
      "    accuracy                           0.95     10000\n",
      "   macro avg       0.95      0.95      0.95     10000\n",
      "weighted avg       0.95      0.95      0.95     10000\n",
      "\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       228\n",
      "           1       0.87      0.93      0.90       185\n",
      "           2       0.73      0.80      0.76       181\n",
      "           3       0.67      0.66      0.66       190\n",
      "           4       0.89      0.77      0.82       216\n",
      "\n",
      "    accuracy                           0.83      1000\n",
      "   macro avg       0.83      0.83      0.83      1000\n",
      "weighted avg       0.84      0.83      0.83      1000\n",
      "\n",
      "50 hidden layer size\n",
      "Training\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1977\n",
      "           1       0.98      0.99      0.98      1976\n",
      "           2       0.97      0.98      0.97      1940\n",
      "           3       0.98      0.98      0.98      2013\n",
      "           4       1.00      1.00      1.00      2094\n",
      "\n",
      "    accuracy                           0.99     10000\n",
      "   macro avg       0.99      0.99      0.99     10000\n",
      "weighted avg       0.99      0.99      0.99     10000\n",
      "\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       228\n",
      "           1       0.89      0.94      0.92       188\n",
      "           2       0.76      0.84      0.80       182\n",
      "           3       0.74      0.71      0.73       195\n",
      "           4       0.90      0.82      0.86       207\n",
      "\n",
      "    accuracy                           0.86      1000\n",
      "   macro avg       0.86      0.86      0.86      1000\n",
      "weighted avg       0.87      0.86      0.86      1000\n",
      "\n",
      "100 hidden layer size\n",
      "Training\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1977\n",
      "           1       0.98      0.98      0.98      1979\n",
      "           2       0.97      0.98      0.98      1940\n",
      "           3       0.99      0.98      0.99      2017\n",
      "           4       1.00      1.00      1.00      2087\n",
      "\n",
      "    accuracy                           0.99     10000\n",
      "   macro avg       0.99      0.99      0.99     10000\n",
      "weighted avg       0.99      0.99      0.99     10000\n",
      "\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       225\n",
      "           1       0.88      0.93      0.90       188\n",
      "           2       0.78      0.81      0.79       192\n",
      "           3       0.72      0.72      0.72       186\n",
      "           4       0.91      0.81      0.86       209\n",
      "\n",
      "    accuracy                           0.86      1000\n",
      "   macro avg       0.85      0.85      0.85      1000\n",
      "weighted avg       0.86      0.86      0.86      1000\n"
     ]
    }
   ],
   "source": [
    "for hidden_layer_size in hidden_layer_sizes:\n",
    "    nn = model_with_hidden_layer_size[hidden_layer_size]\n",
    "    y_train_pred = np.argmax(nn.feedforward(x_train.T), axis=0)\n",
    "    y_test_pred = np.argmax(nn.feedforward(x_test.T), axis=0)\n",
    "    results = classification_report(y_train_pred, y_train)\n",
    "    print(f\"{hidden_layer_size} hidden layer size\")\n",
    "    print('Training')\n",
    "    print(results)\n",
    "    \n",
    "    results = classification_report(y_test_pred, y_test)\n",
    "    print('Test')\n",
    "    print(results)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T21:26:45.277607Z",
     "start_time": "2023-11-03T21:26:45.035597Z"
    }
   },
   "id": "a2f58bd2451d8ceb"
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "outputs": [],
   "source": [
    "avg_f1_scores_training = []\n",
    "avg_f1_scores_test = []\n",
    "for hidden_layer_size in hidden_layer_sizes:\n",
    "    nn = model_with_hidden_layer_size[hidden_layer_size]\n",
    "    y_train_pred = np.argmax(nn.feedforward(x_train.T), axis=0)\n",
    "    y_test_pred = np.argmax(nn.feedforward(x_test.T), axis=0)\n",
    "    results = classification_report(y_train_pred, y_train, output_dict=True)\n",
    "    avg_f1_scores_training.append(results['weighted avg']['f1-score'])\n",
    "    \n",
    "    results = classification_report(y_test_pred, y_test, output_dict=True)\n",
    "    avg_f1_scores_test.append(results['weighted avg']['f1-score'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T21:31:37.721109Z",
     "start_time": "2023-11-03T21:31:37.461228Z"
    }
   },
   "id": "7edfcd12c7f47bf"
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hidden_layer_sizes, avg_f1_scores_training, label = 'Training')\n",
    "plt.plot(hidden_layer_sizes, avg_f1_scores_test, label = 'Test')\n",
    "plt.xlabel('Hidden Layer Size')\n",
    "plt.ylabel('Average F1 Score')\n",
    "plt.legend()\n",
    "plt.savefig('(b) f1 vs hidden_size.png')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T21:33:11.029360Z",
     "start_time": "2023-11-03T21:33:10.400233Z"
    }
   },
   "id": "1bd73810a81fe2df"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# part c"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6830333ad9ef09b8"
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_layer_size: [1024, 512, 5]\n",
      "Epoch 1/1000, Loss: 1.1081\n",
      "Epoch 2/1000, Loss: 0.8564\n",
      "Epoch 3/1000, Loss: 0.7482\n",
      "Epoch 4/1000, Loss: 0.6855\n",
      "Epoch 5/1000, Loss: 0.6427\n",
      "Epoch 6/1000, Loss: 0.6103\n",
      "Epoch 7/1000, Loss: 0.5843\n",
      "Epoch 8/1000, Loss: 0.5624\n",
      "Epoch 9/1000, Loss: 0.5435\n",
      "Epoch 10/1000, Loss: 0.5269\n",
      "Epoch 11/1000, Loss: 0.5121\n",
      "Epoch 12/1000, Loss: 0.4989\n",
      "Epoch 13/1000, Loss: 0.4869\n",
      "Epoch 14/1000, Loss: 0.4761\n",
      "Epoch 15/1000, Loss: 0.4662\n",
      "Epoch 16/1000, Loss: 0.4572\n",
      "Epoch 17/1000, Loss: 0.4490\n",
      "Epoch 18/1000, Loss: 0.4414\n",
      "Epoch 19/1000, Loss: 0.4343\n",
      "Epoch 20/1000, Loss: 0.4278\n",
      "Epoch 21/1000, Loss: 0.4218\n",
      "Epoch 22/1000, Loss: 0.4161\n",
      "Epoch 23/1000, Loss: 0.4108\n",
      "Epoch 24/1000, Loss: 0.4059\n",
      "Epoch 25/1000, Loss: 0.4012\n",
      "Epoch 26/1000, Loss: 0.3967\n",
      "Epoch 27/1000, Loss: 0.3925\n",
      "Epoch 28/1000, Loss: 0.3885\n",
      "Epoch 29/1000, Loss: 0.3847\n",
      "Epoch 30/1000, Loss: 0.3810\n",
      "Epoch 31/1000, Loss: 0.3775\n",
      "Epoch 32/1000, Loss: 0.3741\n",
      "Epoch 33/1000, Loss: 0.3709\n",
      "Epoch 34/1000, Loss: 0.3678\n",
      "Epoch 35/1000, Loss: 0.3647\n",
      "Epoch 36/1000, Loss: 0.3618\n",
      "Epoch 37/1000, Loss: 0.3590\n",
      "Epoch 38/1000, Loss: 0.3563\n",
      "Epoch 39/1000, Loss: 0.3536\n",
      "Epoch 40/1000, Loss: 0.3510\n",
      "Epoch 41/1000, Loss: 0.3485\n",
      "Epoch 42/1000, Loss: 0.3461\n",
      "Epoch 43/1000, Loss: 0.3437\n",
      "Epoch 44/1000, Loss: 0.3413\n",
      "Epoch 45/1000, Loss: 0.3391\n",
      "Epoch 46/1000, Loss: 0.3368\n",
      "Epoch 47/1000, Loss: 0.3347\n",
      "Epoch 48/1000, Loss: 0.3326\n",
      "Epoch 49/1000, Loss: 0.3305\n",
      "Epoch 50/1000, Loss: 0.3285\n",
      "Epoch 51/1000, Loss: 0.3265\n",
      "Epoch 52/1000, Loss: 0.3245\n",
      "Epoch 53/1000, Loss: 0.3226\n",
      "Epoch 54/1000, Loss: 0.3207\n",
      "Epoch 55/1000, Loss: 0.3189\n",
      "Epoch 56/1000, Loss: 0.3171\n",
      "Epoch 57/1000, Loss: 0.3153\n",
      "Epoch 58/1000, Loss: 0.3135\n",
      "Epoch 59/1000, Loss: 0.3118\n",
      "Epoch 60/1000, Loss: 0.3101\n",
      "Epoch 61/1000, Loss: 0.3085\n",
      "Epoch 62/1000, Loss: 0.3068\n",
      "Epoch 63/1000, Loss: 0.3052\n",
      "Epoch 64/1000, Loss: 0.3036\n",
      "Epoch 65/1000, Loss: 0.3020\n",
      "Epoch 66/1000, Loss: 0.3005\n",
      "Epoch 67/1000, Loss: 0.2989\n",
      "Epoch 68/1000, Loss: 0.2974\n",
      "Epoch 69/1000, Loss: 0.2959\n",
      "Epoch 70/1000, Loss: 0.2944\n",
      "Epoch 71/1000, Loss: 0.2930\n",
      "Epoch 72/1000, Loss: 0.2915\n",
      "Epoch 73/1000, Loss: 0.2901\n",
      "Epoch 74/1000, Loss: 0.2886\n",
      "Epoch 75/1000, Loss: 0.2872\n",
      "Epoch 76/1000, Loss: 0.2858\n",
      "Epoch 77/1000, Loss: 0.2845\n",
      "Epoch 78/1000, Loss: 0.2831\n",
      "Epoch 79/1000, Loss: 0.2817\n",
      "Epoch 80/1000, Loss: 0.2804\n",
      "Epoch 81/1000, Loss: 0.2790\n",
      "Epoch 82/1000, Loss: 0.2777\n",
      "Epoch 83/1000, Loss: 0.2763\n",
      "Epoch 84/1000, Loss: 0.2750\n",
      "Epoch 85/1000, Loss: 0.2737\n",
      "Epoch 86/1000, Loss: 0.2724\n",
      "Epoch 87/1000, Loss: 0.2711\n",
      "Epoch 88/1000, Loss: 0.2698\n",
      "Epoch 89/1000, Loss: 0.2685\n",
      "Epoch 90/1000, Loss: 0.2673\n",
      "Epoch 91/1000, Loss: 0.2660\n",
      "Epoch 92/1000, Loss: 0.2647\n",
      "Epoch 93/1000, Loss: 0.2635\n",
      "Epoch 94/1000, Loss: 0.2622\n",
      "Epoch 95/1000, Loss: 0.2609\n",
      "Epoch 96/1000, Loss: 0.2597\n",
      "Epoch 97/1000, Loss: 0.2584\n",
      "Epoch 98/1000, Loss: 0.2572\n",
      "Epoch 99/1000, Loss: 0.2560\n",
      "Epoch 100/1000, Loss: 0.2547\n",
      "Epoch 101/1000, Loss: 0.2535\n",
      "Epoch 102/1000, Loss: 0.2523\n",
      "Epoch 103/1000, Loss: 0.2510\n",
      "Epoch 104/1000, Loss: 0.2498\n",
      "Epoch 105/1000, Loss: 0.2486\n",
      "Epoch 106/1000, Loss: 0.2474\n",
      "Epoch 107/1000, Loss: 0.2462\n",
      "Epoch 108/1000, Loss: 0.2449\n",
      "Epoch 109/1000, Loss: 0.2437\n",
      "Epoch 110/1000, Loss: 0.2425\n",
      "Epoch 111/1000, Loss: 0.2413\n",
      "Epoch 112/1000, Loss: 0.2401\n",
      "Epoch 113/1000, Loss: 0.2389\n",
      "Epoch 114/1000, Loss: 0.2377\n",
      "Epoch 115/1000, Loss: 0.2365\n",
      "Epoch 116/1000, Loss: 0.2353\n",
      "Epoch 117/1000, Loss: 0.2341\n",
      "Epoch 118/1000, Loss: 0.2329\n",
      "Epoch 119/1000, Loss: 0.2317\n",
      "Epoch 120/1000, Loss: 0.2305\n",
      "Epoch 121/1000, Loss: 0.2293\n",
      "Epoch 122/1000, Loss: 0.2281\n",
      "Epoch 123/1000, Loss: 0.2269\n",
      "Epoch 124/1000, Loss: 0.2257\n",
      "Epoch 125/1000, Loss: 0.2245\n",
      "Epoch 126/1000, Loss: 0.2233\n",
      "Epoch 127/1000, Loss: 0.2221\n",
      "Epoch 128/1000, Loss: 0.2209\n",
      "Epoch 129/1000, Loss: 0.2197\n",
      "Epoch 130/1000, Loss: 0.2185\n",
      "Epoch 131/1000, Loss: 0.2173\n",
      "Epoch 132/1000, Loss: 0.2162\n",
      "Epoch 133/1000, Loss: 0.2150\n",
      "Epoch 134/1000, Loss: 0.2138\n",
      "Epoch 135/1000, Loss: 0.2126\n",
      "Epoch 136/1000, Loss: 0.2114\n",
      "Epoch 137/1000, Loss: 0.2102\n",
      "Epoch 138/1000, Loss: 0.2090\n",
      "Epoch 139/1000, Loss: 0.2078\n",
      "Epoch 140/1000, Loss: 0.2067\n",
      "Epoch 141/1000, Loss: 0.2055\n",
      "Epoch 142/1000, Loss: 0.2043\n",
      "Epoch 143/1000, Loss: 0.2031\n",
      "Epoch 144/1000, Loss: 0.2019\n",
      "Epoch 145/1000, Loss: 0.2007\n",
      "Epoch 146/1000, Loss: 0.1995\n",
      "Epoch 147/1000, Loss: 0.1984\n",
      "Epoch 148/1000, Loss: 0.1972\n",
      "Epoch 149/1000, Loss: 0.1960\n",
      "Epoch 150/1000, Loss: 0.1948\n",
      "Epoch 151/1000, Loss: 0.1936\n",
      "Epoch 152/1000, Loss: 0.1924\n",
      "Epoch 153/1000, Loss: 0.1913\n",
      "Epoch 154/1000, Loss: 0.1901\n",
      "Epoch 155/1000, Loss: 0.1889\n",
      "Epoch 156/1000, Loss: 0.1877\n",
      "Epoch 157/1000, Loss: 0.1866\n",
      "Epoch 158/1000, Loss: 0.1854\n",
      "Epoch 159/1000, Loss: 0.1842\n",
      "Epoch 160/1000, Loss: 0.1830\n",
      "Epoch 161/1000, Loss: 0.1819\n",
      "Epoch 162/1000, Loss: 0.1807\n",
      "Epoch 163/1000, Loss: 0.1795\n",
      "Epoch 164/1000, Loss: 0.1783\n",
      "Epoch 165/1000, Loss: 0.1772\n",
      "Epoch 166/1000, Loss: 0.1760\n",
      "Epoch 167/1000, Loss: 0.1748\n",
      "Epoch 168/1000, Loss: 0.1737\n",
      "Epoch 169/1000, Loss: 0.1725\n",
      "Epoch 170/1000, Loss: 0.1714\n",
      "Epoch 171/1000, Loss: 0.1702\n",
      "Epoch 172/1000, Loss: 0.1690\n",
      "Epoch 173/1000, Loss: 0.1679\n",
      "Epoch 174/1000, Loss: 0.1667\n",
      "Epoch 175/1000, Loss: 0.1656\n",
      "Epoch 176/1000, Loss: 0.1644\n",
      "Epoch 177/1000, Loss: 0.1633\n",
      "Epoch 178/1000, Loss: 0.1621\n",
      "Epoch 179/1000, Loss: 0.1610\n",
      "Epoch 180/1000, Loss: 0.1599\n",
      "Epoch 181/1000, Loss: 0.1587\n",
      "Epoch 182/1000, Loss: 0.1576\n",
      "Epoch 183/1000, Loss: 0.1565\n",
      "Epoch 184/1000, Loss: 0.1553\n",
      "Epoch 185/1000, Loss: 0.1542\n",
      "Epoch 186/1000, Loss: 0.1531\n",
      "Epoch 187/1000, Loss: 0.1520\n",
      "Epoch 188/1000, Loss: 0.1508\n",
      "Epoch 189/1000, Loss: 0.1497\n",
      "Epoch 190/1000, Loss: 0.1486\n",
      "Epoch 191/1000, Loss: 0.1475\n",
      "Epoch 192/1000, Loss: 0.1464\n",
      "Epoch 193/1000, Loss: 0.1453\n",
      "Epoch 194/1000, Loss: 0.1442\n",
      "Epoch 195/1000, Loss: 0.1431\n",
      "Epoch 196/1000, Loss: 0.1420\n",
      "Epoch 197/1000, Loss: 0.1409\n",
      "Epoch 198/1000, Loss: 0.1399\n",
      "Epoch 199/1000, Loss: 0.1388\n",
      "Epoch 200/1000, Loss: 0.1377\n",
      "Epoch 201/1000, Loss: 0.1366\n",
      "Epoch 202/1000, Loss: 0.1356\n",
      "Epoch 203/1000, Loss: 0.1345\n",
      "Epoch 204/1000, Loss: 0.1335\n",
      "Epoch 205/1000, Loss: 0.1324\n",
      "Epoch 206/1000, Loss: 0.1314\n",
      "Epoch 207/1000, Loss: 0.1303\n",
      "Epoch 208/1000, Loss: 0.1293\n",
      "Epoch 209/1000, Loss: 0.1283\n",
      "Epoch 210/1000, Loss: 0.1272\n",
      "Epoch 211/1000, Loss: 0.1262\n",
      "Epoch 212/1000, Loss: 0.1252\n",
      "Epoch 213/1000, Loss: 0.1242\n",
      "Epoch 214/1000, Loss: 0.1232\n",
      "Epoch 215/1000, Loss: 0.1222\n",
      "Epoch 216/1000, Loss: 0.1212\n",
      "Epoch 217/1000, Loss: 0.1202\n",
      "Epoch 218/1000, Loss: 0.1192\n",
      "Epoch 219/1000, Loss: 0.1182\n",
      "Epoch 220/1000, Loss: 0.1173\n",
      "Epoch 221/1000, Loss: 0.1163\n",
      "Epoch 222/1000, Loss: 0.1153\n",
      "Epoch 223/1000, Loss: 0.1144\n",
      "Epoch 224/1000, Loss: 0.1134\n",
      "Epoch 225/1000, Loss: 0.1125\n",
      "Epoch 226/1000, Loss: 0.1116\n",
      "Epoch 227/1000, Loss: 0.1106\n",
      "Epoch 228/1000, Loss: 0.1097\n",
      "Epoch 229/1000, Loss: 0.1088\n",
      "Epoch 230/1000, Loss: 0.1079\n",
      "Epoch 231/1000, Loss: 0.1070\n",
      "Epoch 232/1000, Loss: 0.1061\n",
      "Epoch 233/1000, Loss: 0.1052\n",
      "Epoch 234/1000, Loss: 0.1043\n",
      "Epoch 235/1000, Loss: 0.1034\n",
      "Epoch 236/1000, Loss: 0.1025\n",
      "Epoch 237/1000, Loss: 0.1017\n",
      "Epoch 238/1000, Loss: 0.1008\n",
      "Epoch 239/1000, Loss: 0.0999\n",
      "Epoch 240/1000, Loss: 0.0991\n",
      "Epoch 241/1000, Loss: 0.0983\n",
      "Epoch 242/1000, Loss: 0.0974\n",
      "Epoch 243/1000, Loss: 0.0966\n",
      "Epoch 244/1000, Loss: 0.0958\n",
      "Epoch 245/1000, Loss: 0.0949\n",
      "Epoch 246/1000, Loss: 0.0941\n",
      "Epoch 247/1000, Loss: 0.0933\n",
      "Epoch 248/1000, Loss: 0.0925\n",
      "Epoch 249/1000, Loss: 0.0917\n",
      "Epoch 250/1000, Loss: 0.0910\n",
      "Epoch 251/1000, Loss: 0.0902\n",
      "Epoch 252/1000, Loss: 0.0894\n",
      "Epoch 253/1000, Loss: 0.0886\n",
      "Epoch 254/1000, Loss: 0.0879\n",
      "Epoch 255/1000, Loss: 0.0871\n",
      "Epoch 256/1000, Loss: 0.0864\n",
      "Epoch 257/1000, Loss: 0.0856\n",
      "Epoch 258/1000, Loss: 0.0849\n",
      "Epoch 259/1000, Loss: 0.0842\n",
      "Epoch 260/1000, Loss: 0.0835\n",
      "Epoch 261/1000, Loss: 0.0827\n",
      "Epoch 262/1000, Loss: 0.0820\n",
      "Epoch 263/1000, Loss: 0.0813\n",
      "Epoch 264/1000, Loss: 0.0806\n",
      "Converged\n",
      "hidden_layer_size: [1024, 512, 256, 5]\n",
      "Epoch 1/1000, Loss: 1.6273\n",
      "Epoch 2/1000, Loss: 1.6264\n",
      "Epoch 3/1000, Loss: 1.6250\n",
      "Epoch 4/1000, Loss: 1.6227\n",
      "Epoch 5/1000, Loss: 1.6178\n",
      "Epoch 6/1000, Loss: 1.6057\n",
      "Epoch 7/1000, Loss: 1.5668\n",
      "Epoch 8/1000, Loss: 1.4254\n",
      "Epoch 9/1000, Loss: 1.1592\n",
      "Epoch 10/1000, Loss: 0.9767\n",
      "Epoch 11/1000, Loss: 0.8680\n",
      "Epoch 12/1000, Loss: 0.7962\n",
      "Epoch 13/1000, Loss: 0.7451\n",
      "Epoch 14/1000, Loss: 0.7063\n",
      "Epoch 15/1000, Loss: 0.6751\n",
      "Epoch 16/1000, Loss: 0.6488\n",
      "Epoch 17/1000, Loss: 0.6259\n",
      "Epoch 18/1000, Loss: 0.6055\n",
      "Epoch 19/1000, Loss: 0.5872\n",
      "Epoch 20/1000, Loss: 0.5705\n",
      "Epoch 21/1000, Loss: 0.5552\n",
      "Epoch 22/1000, Loss: 0.5411\n",
      "Epoch 23/1000, Loss: 0.5282\n",
      "Epoch 24/1000, Loss: 0.5163\n",
      "Epoch 25/1000, Loss: 0.5053\n",
      "Epoch 26/1000, Loss: 0.4953\n",
      "Epoch 27/1000, Loss: 0.4860\n",
      "Epoch 28/1000, Loss: 0.4775\n",
      "Epoch 29/1000, Loss: 0.4696\n",
      "Epoch 30/1000, Loss: 0.4623\n",
      "Epoch 31/1000, Loss: 0.4556\n",
      "Epoch 32/1000, Loss: 0.4493\n",
      "Epoch 33/1000, Loss: 0.4435\n",
      "Epoch 34/1000, Loss: 0.4381\n",
      "Epoch 35/1000, Loss: 0.4330\n",
      "Epoch 36/1000, Loss: 0.4282\n",
      "Epoch 37/1000, Loss: 0.4237\n",
      "Epoch 38/1000, Loss: 0.4194\n",
      "Epoch 39/1000, Loss: 0.4154\n",
      "Epoch 40/1000, Loss: 0.4116\n",
      "Epoch 41/1000, Loss: 0.4080\n",
      "Epoch 42/1000, Loss: 0.4045\n",
      "Epoch 43/1000, Loss: 0.4012\n",
      "Epoch 44/1000, Loss: 0.3981\n",
      "Epoch 45/1000, Loss: 0.3951\n",
      "Epoch 46/1000, Loss: 0.3922\n",
      "Epoch 47/1000, Loss: 0.3894\n",
      "Epoch 48/1000, Loss: 0.3868\n",
      "Epoch 49/1000, Loss: 0.3842\n",
      "Epoch 50/1000, Loss: 0.3818\n",
      "Epoch 51/1000, Loss: 0.3794\n",
      "Epoch 52/1000, Loss: 0.3771\n",
      "Epoch 53/1000, Loss: 0.3749\n",
      "Epoch 54/1000, Loss: 0.3728\n",
      "Epoch 55/1000, Loss: 0.3707\n",
      "Epoch 56/1000, Loss: 0.3687\n",
      "Epoch 57/1000, Loss: 0.3668\n",
      "Epoch 58/1000, Loss: 0.3649\n",
      "Epoch 59/1000, Loss: 0.3631\n",
      "Epoch 60/1000, Loss: 0.3613\n",
      "Epoch 61/1000, Loss: 0.3595\n",
      "Epoch 62/1000, Loss: 0.3578\n",
      "Epoch 63/1000, Loss: 0.3562\n",
      "Epoch 64/1000, Loss: 0.3546\n",
      "Epoch 65/1000, Loss: 0.3530\n",
      "Epoch 66/1000, Loss: 0.3515\n",
      "Epoch 67/1000, Loss: 0.3500\n",
      "Epoch 68/1000, Loss: 0.3485\n",
      "Epoch 69/1000, Loss: 0.3471\n",
      "Epoch 70/1000, Loss: 0.3456\n",
      "Epoch 71/1000, Loss: 0.3443\n",
      "Epoch 72/1000, Loss: 0.3429\n",
      "Epoch 73/1000, Loss: 0.3415\n",
      "Epoch 74/1000, Loss: 0.3402\n",
      "Epoch 75/1000, Loss: 0.3389\n",
      "Epoch 76/1000, Loss: 0.3376\n",
      "Epoch 77/1000, Loss: 0.3364\n",
      "Epoch 78/1000, Loss: 0.3351\n",
      "Epoch 79/1000, Loss: 0.3339\n",
      "Epoch 80/1000, Loss: 0.3327\n",
      "Epoch 81/1000, Loss: 0.3315\n",
      "Epoch 82/1000, Loss: 0.3303\n",
      "Epoch 83/1000, Loss: 0.3291\n",
      "Epoch 84/1000, Loss: 0.3280\n",
      "Epoch 85/1000, Loss: 0.3268\n",
      "Epoch 86/1000, Loss: 0.3257\n",
      "Epoch 87/1000, Loss: 0.3245\n",
      "Epoch 88/1000, Loss: 0.3234\n",
      "Epoch 89/1000, Loss: 0.3223\n",
      "Epoch 90/1000, Loss: 0.3212\n",
      "Epoch 91/1000, Loss: 0.3201\n",
      "Epoch 92/1000, Loss: 0.3190\n",
      "Epoch 93/1000, Loss: 0.3179\n",
      "Epoch 94/1000, Loss: 0.3168\n",
      "Epoch 95/1000, Loss: 0.3157\n",
      "Epoch 96/1000, Loss: 0.3147\n",
      "Epoch 97/1000, Loss: 0.3136\n",
      "Epoch 98/1000, Loss: 0.3125\n",
      "Epoch 99/1000, Loss: 0.3114\n",
      "Epoch 100/1000, Loss: 0.3104\n",
      "Epoch 101/1000, Loss: 0.3093\n",
      "Epoch 102/1000, Loss: 0.3083\n",
      "Epoch 103/1000, Loss: 0.3072\n",
      "Epoch 104/1000, Loss: 0.3061\n",
      "Epoch 105/1000, Loss: 0.3051\n",
      "Epoch 106/1000, Loss: 0.3040\n",
      "Epoch 107/1000, Loss: 0.3029\n",
      "Epoch 108/1000, Loss: 0.3019\n",
      "Epoch 109/1000, Loss: 0.3008\n",
      "Epoch 110/1000, Loss: 0.2997\n",
      "Epoch 111/1000, Loss: 0.2987\n",
      "Epoch 112/1000, Loss: 0.2976\n",
      "Epoch 113/1000, Loss: 0.2965\n",
      "Epoch 114/1000, Loss: 0.2954\n",
      "Epoch 115/1000, Loss: 0.2944\n",
      "Epoch 116/1000, Loss: 0.2933\n",
      "Epoch 117/1000, Loss: 0.2922\n",
      "Epoch 118/1000, Loss: 0.2911\n",
      "Epoch 119/1000, Loss: 0.2900\n",
      "Epoch 120/1000, Loss: 0.2889\n",
      "Epoch 121/1000, Loss: 0.2878\n",
      "Epoch 122/1000, Loss: 0.2867\n",
      "Epoch 123/1000, Loss: 0.2856\n",
      "Epoch 124/1000, Loss: 0.2845\n",
      "Epoch 125/1000, Loss: 0.2833\n",
      "Epoch 126/1000, Loss: 0.2822\n",
      "Epoch 127/1000, Loss: 0.2811\n",
      "Epoch 128/1000, Loss: 0.2799\n",
      "Epoch 129/1000, Loss: 0.2788\n",
      "Epoch 130/1000, Loss: 0.2777\n",
      "Epoch 131/1000, Loss: 0.2765\n",
      "Epoch 132/1000, Loss: 0.2754\n",
      "Epoch 133/1000, Loss: 0.2742\n",
      "Epoch 134/1000, Loss: 0.2730\n",
      "Epoch 135/1000, Loss: 0.2719\n",
      "Epoch 136/1000, Loss: 0.2707\n",
      "Epoch 137/1000, Loss: 0.2695\n",
      "Epoch 138/1000, Loss: 0.2683\n",
      "Epoch 139/1000, Loss: 0.2671\n",
      "Epoch 140/1000, Loss: 0.2659\n",
      "Epoch 141/1000, Loss: 0.2647\n",
      "Epoch 142/1000, Loss: 0.2635\n",
      "Epoch 143/1000, Loss: 0.2623\n",
      "Epoch 144/1000, Loss: 0.2611\n",
      "Epoch 145/1000, Loss: 0.2599\n",
      "Epoch 146/1000, Loss: 0.2586\n",
      "Epoch 147/1000, Loss: 0.2574\n",
      "Epoch 148/1000, Loss: 0.2562\n",
      "Epoch 149/1000, Loss: 0.2549\n",
      "Epoch 150/1000, Loss: 0.2537\n",
      "Epoch 151/1000, Loss: 0.2524\n",
      "Epoch 152/1000, Loss: 0.2512\n",
      "Epoch 153/1000, Loss: 0.2499\n",
      "Epoch 154/1000, Loss: 0.2486\n",
      "Epoch 155/1000, Loss: 0.2473\n",
      "Epoch 156/1000, Loss: 0.2461\n",
      "Epoch 157/1000, Loss: 0.2448\n",
      "Epoch 158/1000, Loss: 0.2435\n",
      "Epoch 159/1000, Loss: 0.2422\n",
      "Epoch 160/1000, Loss: 0.2409\n",
      "Epoch 161/1000, Loss: 0.2396\n",
      "Epoch 162/1000, Loss: 0.2383\n",
      "Epoch 163/1000, Loss: 0.2370\n",
      "Epoch 164/1000, Loss: 0.2357\n",
      "Epoch 165/1000, Loss: 0.2343\n",
      "Epoch 166/1000, Loss: 0.2330\n",
      "Epoch 167/1000, Loss: 0.2317\n",
      "Epoch 168/1000, Loss: 0.2304\n",
      "Epoch 169/1000, Loss: 0.2290\n",
      "Epoch 170/1000, Loss: 0.2277\n",
      "Epoch 171/1000, Loss: 0.2263\n",
      "Epoch 172/1000, Loss: 0.2250\n",
      "Epoch 173/1000, Loss: 0.2236\n",
      "Epoch 174/1000, Loss: 0.2222\n",
      "Epoch 175/1000, Loss: 0.2209\n",
      "Epoch 176/1000, Loss: 0.2195\n",
      "Epoch 177/1000, Loss: 0.2181\n",
      "Epoch 178/1000, Loss: 0.2168\n",
      "Epoch 179/1000, Loss: 0.2154\n",
      "Epoch 180/1000, Loss: 0.2140\n",
      "Epoch 181/1000, Loss: 0.2126\n",
      "Epoch 182/1000, Loss: 0.2112\n",
      "Epoch 183/1000, Loss: 0.2098\n",
      "Epoch 184/1000, Loss: 0.2084\n",
      "Epoch 185/1000, Loss: 0.2070\n",
      "Epoch 186/1000, Loss: 0.2056\n",
      "Epoch 187/1000, Loss: 0.2042\n",
      "Epoch 188/1000, Loss: 0.2027\n",
      "Epoch 189/1000, Loss: 0.2013\n",
      "Epoch 190/1000, Loss: 0.1999\n",
      "Epoch 191/1000, Loss: 0.1985\n",
      "Epoch 192/1000, Loss: 0.1970\n",
      "Epoch 193/1000, Loss: 0.1956\n",
      "Epoch 194/1000, Loss: 0.1942\n",
      "Epoch 195/1000, Loss: 0.1927\n",
      "Epoch 196/1000, Loss: 0.1913\n",
      "Epoch 197/1000, Loss: 0.1898\n",
      "Epoch 198/1000, Loss: 0.1884\n",
      "Epoch 199/1000, Loss: 0.1869\n",
      "Epoch 200/1000, Loss: 0.1855\n",
      "Epoch 201/1000, Loss: 0.1840\n",
      "Epoch 202/1000, Loss: 0.1826\n",
      "Epoch 203/1000, Loss: 0.1811\n",
      "Epoch 204/1000, Loss: 0.1797\n",
      "Epoch 205/1000, Loss: 0.1782\n",
      "Epoch 206/1000, Loss: 0.1767\n",
      "Epoch 207/1000, Loss: 0.1753\n",
      "Epoch 208/1000, Loss: 0.1738\n",
      "Epoch 209/1000, Loss: 0.1723\n",
      "Epoch 210/1000, Loss: 0.1709\n",
      "Epoch 211/1000, Loss: 0.1694\n",
      "Epoch 212/1000, Loss: 0.1679\n",
      "Epoch 213/1000, Loss: 0.1665\n",
      "Epoch 214/1000, Loss: 0.1650\n",
      "Epoch 215/1000, Loss: 0.1635\n",
      "Epoch 216/1000, Loss: 0.1621\n",
      "Epoch 217/1000, Loss: 0.1606\n",
      "Epoch 218/1000, Loss: 0.1591\n",
      "Epoch 219/1000, Loss: 0.1577\n",
      "Epoch 220/1000, Loss: 0.1562\n",
      "Epoch 221/1000, Loss: 0.1548\n",
      "Epoch 222/1000, Loss: 0.1533\n",
      "Epoch 223/1000, Loss: 0.1518\n",
      "Epoch 224/1000, Loss: 0.1504\n",
      "Epoch 225/1000, Loss: 0.1489\n",
      "Epoch 226/1000, Loss: 0.1475\n",
      "Epoch 227/1000, Loss: 0.1460\n",
      "Epoch 228/1000, Loss: 0.1446\n",
      "Epoch 229/1000, Loss: 0.1431\n",
      "Epoch 230/1000, Loss: 0.1417\n",
      "Epoch 231/1000, Loss: 0.1403\n",
      "Epoch 232/1000, Loss: 0.1388\n",
      "Epoch 233/1000, Loss: 0.1374\n",
      "Epoch 234/1000, Loss: 0.1360\n",
      "Epoch 235/1000, Loss: 0.1346\n",
      "Epoch 236/1000, Loss: 0.1332\n",
      "Epoch 237/1000, Loss: 0.1318\n",
      "Epoch 238/1000, Loss: 0.1304\n",
      "Epoch 239/1000, Loss: 0.1290\n",
      "Epoch 240/1000, Loss: 0.1276\n",
      "Epoch 241/1000, Loss: 0.1262\n",
      "Epoch 242/1000, Loss: 0.1248\n",
      "Epoch 243/1000, Loss: 0.1235\n",
      "Epoch 244/1000, Loss: 0.1221\n",
      "Epoch 245/1000, Loss: 0.1207\n",
      "Epoch 246/1000, Loss: 0.1194\n",
      "Epoch 247/1000, Loss: 0.1180\n",
      "Epoch 248/1000, Loss: 0.1167\n",
      "Epoch 249/1000, Loss: 0.1154\n",
      "Epoch 250/1000, Loss: 0.1141\n",
      "Epoch 251/1000, Loss: 0.1128\n",
      "Epoch 252/1000, Loss: 0.1115\n",
      "Epoch 253/1000, Loss: 0.1102\n",
      "Epoch 254/1000, Loss: 0.1089\n",
      "Epoch 255/1000, Loss: 0.1076\n",
      "Epoch 256/1000, Loss: 0.1063\n",
      "Epoch 257/1000, Loss: 0.1051\n",
      "Epoch 258/1000, Loss: 0.1038\n",
      "Epoch 259/1000, Loss: 0.1026\n",
      "Epoch 260/1000, Loss: 0.1014\n",
      "Epoch 261/1000, Loss: 0.1002\n",
      "Epoch 262/1000, Loss: 0.0990\n",
      "Epoch 263/1000, Loss: 0.0978\n",
      "Epoch 264/1000, Loss: 0.0966\n",
      "Epoch 265/1000, Loss: 0.0954\n",
      "Epoch 266/1000, Loss: 0.0942\n",
      "Epoch 267/1000, Loss: 0.0931\n",
      "Epoch 268/1000, Loss: 0.0920\n",
      "Epoch 269/1000, Loss: 0.0908\n",
      "Epoch 270/1000, Loss: 0.0897\n",
      "Epoch 271/1000, Loss: 0.0886\n",
      "Epoch 272/1000, Loss: 0.0875\n",
      "Epoch 273/1000, Loss: 0.0864\n",
      "Epoch 274/1000, Loss: 0.0853\n",
      "Epoch 275/1000, Loss: 0.0843\n",
      "Epoch 276/1000, Loss: 0.0832\n",
      "Epoch 277/1000, Loss: 0.0822\n",
      "Epoch 278/1000, Loss: 0.0811\n",
      "Epoch 279/1000, Loss: 0.0801\n",
      "Epoch 280/1000, Loss: 0.0791\n",
      "Epoch 281/1000, Loss: 0.0781\n",
      "Epoch 282/1000, Loss: 0.0771\n",
      "Epoch 283/1000, Loss: 0.0762\n",
      "Epoch 284/1000, Loss: 0.0752\n",
      "Epoch 285/1000, Loss: 0.0743\n",
      "Epoch 286/1000, Loss: 0.0733\n",
      "Epoch 287/1000, Loss: 0.0724\n",
      "Epoch 288/1000, Loss: 0.0715\n",
      "Epoch 289/1000, Loss: 0.0706\n",
      "Epoch 290/1000, Loss: 0.0697\n",
      "Epoch 291/1000, Loss: 0.0688\n",
      "Epoch 292/1000, Loss: 0.0679\n",
      "Epoch 293/1000, Loss: 0.0671\n",
      "Epoch 294/1000, Loss: 0.0662\n",
      "Epoch 295/1000, Loss: 0.0654\n",
      "Epoch 296/1000, Loss: 0.0646\n",
      "Epoch 297/1000, Loss: 0.0638\n",
      "Epoch 298/1000, Loss: 0.0630\n",
      "Epoch 299/1000, Loss: 0.0622\n",
      "Epoch 300/1000, Loss: 0.0614\n",
      "Epoch 301/1000, Loss: 0.0607\n",
      "Epoch 302/1000, Loss: 0.0599\n",
      "Epoch 303/1000, Loss: 0.0592\n",
      "Epoch 304/1000, Loss: 0.0584\n",
      "Epoch 305/1000, Loss: 0.0577\n",
      "Epoch 306/1000, Loss: 0.0570\n",
      "Epoch 307/1000, Loss: 0.0563\n",
      "Epoch 308/1000, Loss: 0.0556\n",
      "Epoch 309/1000, Loss: 0.0549\n",
      "Converged\n",
      "hidden_layer_size: [1024, 512, 256, 128, 5]\n",
      "Epoch 1/1000, Loss: 1.6155\n",
      "Epoch 2/1000, Loss: 1.6154\n",
      "Epoch 3/1000, Loss: 1.6154\n",
      "Epoch 4/1000, Loss: 1.6154\n",
      "Epoch 5/1000, Loss: 1.6154\n",
      "Epoch 6/1000, Loss: 1.6153\n",
      "Converged\n",
      "hidden_layer_size: [1024, 512, 256, 128, 64, 5]\n",
      "Epoch 1/1000, Loss: 1.6104\n",
      "Epoch 2/1000, Loss: 1.6104\n",
      "Epoch 3/1000, Loss: 1.6104\n",
      "Epoch 4/1000, Loss: 1.6104\n",
      "Epoch 5/1000, Loss: 1.6104\n",
      "Epoch 6/1000, Loss: 1.6104\n",
      "Converged\n"
     ]
    }
   ],
   "source": [
    "hidden_layers = [[512], [512, 256], [512, 256, 128], [512, 256, 128, 64]]\n",
    "layers = [[1024] + hidden_layer + [5] for hidden_layer in hidden_layers]\n",
    "model_with_hidden_layers = {}\n",
    "\n",
    "for layer in layers:\n",
    "    print(f'hidden_layer_size: {layer}')\n",
    "    nn = NeuralNetwork(layer)\n",
    "    nn.train(x_train.T, y_train_one_hot, epochs=1000, mini_batch_size=32, learning_rate=0.01)\n",
    "    model_with_hidden_layers[str(layer)] = nn"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T21:53:38.047237Z",
     "start_time": "2023-11-03T21:36:01.026027Z"
    }
   },
   "id": "95d87e1a46724f6d"
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_layer_size: [1024, 512, 256, 128, 5]\n",
      "Epoch 1/200, Loss: 1.6101\n",
      "Epoch 2/200, Loss: 1.6101\n",
      "Epoch 3/200, Loss: 1.6101\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[203], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhidden_layer_size: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlayer\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      3\u001B[0m nn \u001B[38;5;241m=\u001B[39m NeuralNetwork(layer)\n\u001B[0;32m----> 4\u001B[0m \u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_train\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mT\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train_one_hot\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m200\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmini_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m32\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconv_threshold\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1e-5\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m model_with_hidden_layers[\u001B[38;5;28mstr\u001B[39m(layer)] \u001B[38;5;241m=\u001B[39m nn\n",
      "Cell \u001B[0;32mIn[173], line 73\u001B[0m, in \u001B[0;36mNeuralNetwork.train\u001B[0;34m(self, x_train, y_train, epochs, mini_batch_size, learning_rate, conv_threshold, conv_epochs)\u001B[0m\n\u001B[1;32m     71\u001B[0m     d_weights, d_biases \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbackpropagation(mini_batch_y)\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;66;03m# Update parameters\u001B[39;00m\n\u001B[0;32m---> 73\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate_parameters\u001B[49m\u001B[43m(\u001B[49m\u001B[43md_weights\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43md_biases\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     75\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcross_entropy_loss(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeedforward(x_train), y_train)\n\u001B[1;32m     76\u001B[0m \u001B[38;5;66;03m# Optional: Print the loss after each epoch (can be commented out for speed)\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[173], line 51\u001B[0m, in \u001B[0;36mNeuralNetwork.update_parameters\u001B[0;34m(self, d_weights, d_biases, learning_rate)\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweights)):\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweights[i] \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m learning_rate \u001B[38;5;241m*\u001B[39m d_weights[i]\n\u001B[0;32m---> 51\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbiases[i] \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m learning_rate \u001B[38;5;241m*\u001B[39m d_biases[i]\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for layer in layers[-2:]:\n",
    "    print(f'hidden_layer_size: {layer}')\n",
    "    nn = NeuralNetwork(layer)\n",
    "    nn.train(x_train.T, y_train_one_hot, epochs=200, mini_batch_size=32, learning_rate=0.01, conv_threshold=1e-5)\n",
    "    model_with_hidden_layers[str(layer)] = nn"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T08:00:24.896583Z",
     "start_time": "2023-11-04T08:00:12.396017Z"
    }
   },
   "id": "606e00a16c9bab05"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for layer in layers[-1:]:\n",
    "    print(f'hidden_layer_size: {layer}')\n",
    "    nn = NeuralNetwork(layer)\n",
    "    nn.train(x_train.T, y_train_one_hot, epochs=1000, mini_batch_size=32, learning_rate=0.1, conv_threshold=1e-5)\n",
    "    model_with_hidden_layers[str(layer)] = nn"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T08:00:00.570082Z",
     "start_time": "2023-11-04T08:00:00.569037Z"
    }
   },
   "id": "bc2dbb8f764efd33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('models_c.pickle', 'wb') as handle:\n",
    "    pickle.dump(model_with_hidden_layers, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T08:00:00.596245Z",
     "start_time": "2023-11-04T08:00:00.570716Z"
    }
   },
   "id": "83264148dff72d23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load models\n",
    "\n",
    "with open('models_c.pickle', 'rb') as handle:\n",
    "    model_with_hidden_layers = pickle.load(handle)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cccf976506c781d9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for layer in layers:\n",
    "    nn = model_with_hidden_layers[str(layer)]\n",
    "    y_train_pred = np.argmax(nn.feedforward(x_train.T), axis=0)\n",
    "    y_test_pred = np.argmax(nn.feedforward(x_test.T), axis=0)\n",
    "    results = classification_report(y_train_pred, y_train)\n",
    "    print(f\"{layer} hidden layer size\")\n",
    "    print('Training')\n",
    "    print(results)\n",
    "    \n",
    "    results = classification_report(y_test_pred, y_test)\n",
    "    print('Test')\n",
    "    print(results)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-04T08:00:00.577180Z"
    }
   },
   "id": "8da1ea2b144d2d50"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "avg_f1_scores_training = []\n",
    "avg_f1_scores_test = []\n",
    "for layer in layers:\n",
    "    nn = model_with_hidden_layers[str(layer)]\n",
    "    y_train_pred = np.argmax(nn.feedforward(x_train.T), axis=0)\n",
    "    y_test_pred = np.argmax(nn.feedforward(x_test.T), axis=0)\n",
    "    results = classification_report(y_train_pred, y_train, output_dict=True)\n",
    "    avg_f1_scores_training.append(results['weighted avg']['f1-score'])\n",
    "    \n",
    "    results = classification_report(y_test_pred, y_test, output_dict=True)\n",
    "    avg_f1_scores_test.append(results['weighted avg']['f1-score'])\n",
    "    \n",
    "plt.plot([len(layer) for layer in layers], avg_f1_scores_training, label = 'Training')\n",
    "plt.plot([len(layer) for layer in layers], avg_f1_scores_test, label = 'Test')\n",
    "plt.xlabel('Hidden Layers Depth')\n",
    "plt.ylabel('Average F1 Score')\n",
    "plt.legend()\n",
    "plt.savefig('(c) f1 vs hidden_depth.png')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-04T08:00:00.634916Z"
    }
   },
   "id": "fa857128b08c06dd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# part d"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f29e0ba30b3cd5f2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class NeuralNetwork(NeuralNetwork):  # Extending the previously defined NeuralNetwork class\n",
    "    def train_c(self, x_train, y_train, epochs, mini_batch_size, learning_rate, conv_threshold = 0.001, conv_epochs = 5):\n",
    "        n = x_train.shape[1]  # Total number of training examples\n",
    "\n",
    "        # Training loop\n",
    "        loss_history = []\n",
    "        permutation = np.random.permutation(n)\n",
    "        learning_rate_c = learning_rate\n",
    "        for epoch in range(epochs):\n",
    "            learning_rate = learning_rate_c / pow(epoch+1, 0.5)\n",
    "            # Shuffle the training data for each epoch\n",
    "            x_train_shuffled = x_train[:, permutation]\n",
    "            y_train_shuffled = y_train[:, permutation]\n",
    "\n",
    "            # Mini-batch loop\n",
    "            for k in range(0, n, mini_batch_size):\n",
    "                mini_batch_x = x_train_shuffled[:, k:k + mini_batch_size]\n",
    "                mini_batch_y = y_train_shuffled[:, k:k + mini_batch_size]\n",
    "                # Forward pass\n",
    "                self.feedforward(mini_batch_x)\n",
    "                # Backward pass\n",
    "                d_weights, d_biases = self.backpropagation(mini_batch_y)\n",
    "                # Update parameters\n",
    "                self.update_parameters(d_weights, d_biases, learning_rate)\n",
    "            \n",
    "            loss = self.cross_entropy_loss(self.feedforward(x_train), y_train)\n",
    "            # Optional: Print the loss after each epoch (can be commented out for speed)\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}\")\n",
    "            \n",
    "            loss_history.append(loss)\n",
    "            \n",
    "            if len(loss_history) > conv_epochs:\n",
    "                temp = loss_history[-conv_epochs:]\n",
    "                if np.std(temp) < conv_threshold:\n",
    "                    print('Converged')\n",
    "                    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-04T08:00:00.595295Z"
    }
   },
   "id": "7d922afb91699240"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "models_d = {}\n",
    "for layer in layers:\n",
    "    print(f'hidden_layer_size: {layer}')\n",
    "    nn = NeuralNetwork(layer)\n",
    "    nn.train_c(x_train.T, y_train_one_hot, epochs=200, mini_batch_size=32, learning_rate=0.01)\n",
    "    models_d[str(layer)] = nn"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T08:00:00.669366Z",
     "start_time": "2023-11-04T08:00:00.598834Z"
    }
   },
   "id": "9dff9f4bcaee0f2c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save models\n",
    "import pickle\n",
    "\n",
    "with open('pickles/models_d.pickle', 'wb') as handle:\n",
    "    pickle.dump(models_d, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-04T08:00:00.601835Z"
    }
   },
   "id": "317dbd6c94b623e6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# report results\n",
    "for layer in layers:\n",
    "    nn = models_d[str(layer)]\n",
    "    y_train_pred = np.argmax(nn.feedforward(x_train.T), axis=0)\n",
    "    y_test_pred = np.argmax(nn.feedforward(x_test.T), axis=0)\n",
    "    results = classification_report(y_train_pred, y_train)\n",
    "    print(f\"{layer} hidden layer size\")\n",
    "    print('Training')\n",
    "    print(results)\n",
    "    \n",
    "    results = classification_report(y_test_pred, y_test)\n",
    "    print('Test')\n",
    "    print(results)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-04T08:00:00.626025Z"
    }
   },
   "id": "ef91a1922c095e26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "avg_f1_scores_training = []\n",
    "avg_f1_scores_test = []\n",
    "for layer in layers:\n",
    "    nn = models_d[str(layer)]\n",
    "    y_train_pred = np.argmax(nn.feedforward(x_train.T), axis=0)\n",
    "    y_test_pred = np.argmax(nn.feedforward(x_test.T), axis=0)\n",
    "    results = classification_report(y_train_pred, y_train, output_dict=True)\n",
    "    avg_f1_scores_training.append(results['weighted avg']['f1-score'])\n",
    "    \n",
    "    results = classification_report(y_test_pred, y_test, output_dict=True)\n",
    "    avg_f1_scores_test.append(results['weighted avg']['f1-score'])\n",
    "    \n",
    "plt.plot([len(layer) for layer in layers], avg_f1_scores_training, label = 'Training')\n",
    "plt.plot([len(layer) for layer in layers], avg_f1_scores_test, label = 'Test')\n",
    "plt.xlabel('Hidden Layers Depth')\n",
    "plt.ylabel('Average F1 Score')\n",
    "plt.legend()\n",
    "plt.savefig('(d) adaptive training f1 vs hidden_depth.png')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-04T08:00:00.637617Z"
    }
   },
   "id": "f34429031cfe3997"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# part f"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b5dc631a4fc3a77"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_layer_size: [512]\n",
      "Iteration 1, loss = 0.84011246\n",
      "Iteration 2, loss = 0.62304192\n",
      "Iteration 3, loss = 0.58933066\n",
      "Iteration 4, loss = 0.58147115\n",
      "Iteration 5, loss = 0.57804912\n",
      "Iteration 6, loss = 0.57594228\n",
      "Iteration 7, loss = 0.57437926\n",
      "Iteration 8, loss = 0.57312266\n",
      "Iteration 9, loss = 0.57203400\n",
      "Iteration 10, loss = 0.57107017\n",
      "Iteration 11, loss = 0.57019680\n",
      "Iteration 12, loss = 0.56939025\n",
      "Iteration 13, loss = 0.56865138\n",
      "Iteration 14, loss = 0.56796419\n",
      "Iteration 15, loss = 0.56731128\n",
      "Iteration 16, loss = 0.56670029\n",
      "Iteration 17, loss = 0.56611758\n",
      "Iteration 18, loss = 0.56556786\n",
      "Iteration 19, loss = 0.56504157\n",
      "Iteration 20, loss = 0.56453699\n",
      "Iteration 21, loss = 0.56405597\n",
      "Iteration 22, loss = 0.56358632\n",
      "Iteration 23, loss = 0.56314046\n",
      "Iteration 24, loss = 0.56270575\n",
      "Iteration 25, loss = 0.56228673\n",
      "Iteration 26, loss = 0.56188335\n",
      "Iteration 27, loss = 0.56148425\n",
      "Iteration 28, loss = 0.56110012\n",
      "Iteration 29, loss = 0.56072995\n",
      "Iteration 30, loss = 0.56036620\n",
      "Iteration 31, loss = 0.56001135\n",
      "Iteration 32, loss = 0.55966942\n",
      "Iteration 33, loss = 0.55933027\n",
      "Iteration 34, loss = 0.55900018\n",
      "Iteration 35, loss = 0.55867852\n",
      "Iteration 36, loss = 0.55836503\n",
      "Iteration 37, loss = 0.55805101\n",
      "Iteration 38, loss = 0.55775138\n",
      "Iteration 39, loss = 0.55745300\n",
      "Iteration 40, loss = 0.55716056\n",
      "Iteration 41, loss = 0.55687600\n",
      "Iteration 42, loss = 0.55659338\n",
      "Iteration 43, loss = 0.55631777\n",
      "Iteration 44, loss = 0.55604524\n",
      "Iteration 45, loss = 0.55577983\n",
      "Iteration 46, loss = 0.55551450\n",
      "Iteration 47, loss = 0.55525615\n",
      "Iteration 48, loss = 0.55500131\n",
      "Iteration 49, loss = 0.55475071\n",
      "Iteration 50, loss = 0.55450505\n",
      "Iteration 51, loss = 0.55426103\n",
      "Iteration 52, loss = 0.55401739\n",
      "Iteration 53, loss = 0.55377797\n",
      "Iteration 54, loss = 0.55354672\n",
      "Iteration 55, loss = 0.55331319\n",
      "Iteration 56, loss = 0.55308386\n",
      "Iteration 57, loss = 0.55286098\n",
      "Iteration 58, loss = 0.55263743\n",
      "Iteration 59, loss = 0.55241793\n",
      "Iteration 60, loss = 0.55219909\n",
      "Iteration 61, loss = 0.55198288\n",
      "Iteration 62, loss = 0.55177357\n",
      "Iteration 63, loss = 0.55156570\n",
      "Iteration 64, loss = 0.55135312\n",
      "Iteration 65, loss = 0.55115011\n",
      "Iteration 66, loss = 0.55094412\n",
      "Iteration 67, loss = 0.55074390\n",
      "Iteration 68, loss = 0.55054384\n",
      "Iteration 69, loss = 0.55034721\n",
      "Iteration 70, loss = 0.55015591\n",
      "Iteration 71, loss = 0.54995928\n",
      "Iteration 72, loss = 0.54976715\n",
      "Iteration 73, loss = 0.54957523\n",
      "Iteration 74, loss = 0.54938827\n",
      "Iteration 75, loss = 0.54920018\n",
      "Iteration 76, loss = 0.54901714\n",
      "Iteration 77, loss = 0.54883587\n",
      "Iteration 78, loss = 0.54865233\n",
      "Iteration 79, loss = 0.54847337\n",
      "Iteration 80, loss = 0.54829317\n",
      "Iteration 81, loss = 0.54811548\n",
      "Iteration 82, loss = 0.54794143\n",
      "Iteration 83, loss = 0.54776716\n",
      "Iteration 84, loss = 0.54759658\n",
      "Iteration 85, loss = 0.54742216\n",
      "Iteration 86, loss = 0.54725430\n",
      "Iteration 87, loss = 0.54708591\n",
      "Iteration 88, loss = 0.54691998\n",
      "Iteration 89, loss = 0.54675500\n",
      "Iteration 90, loss = 0.54658799\n",
      "Iteration 91, loss = 0.54642366\n",
      "Iteration 92, loss = 0.54626433\n",
      "Iteration 93, loss = 0.54610009\n",
      "Iteration 94, loss = 0.54593990\n",
      "Iteration 95, loss = 0.54578329\n",
      "Iteration 96, loss = 0.54562383\n",
      "Iteration 97, loss = 0.54546907\n",
      "Iteration 98, loss = 0.54531125\n",
      "Iteration 99, loss = 0.54515725\n",
      "Iteration 100, loss = 0.54500386\n",
      "Iteration 101, loss = 0.54484933\n",
      "Iteration 102, loss = 0.54469917\n",
      "Iteration 103, loss = 0.54454934\n",
      "Iteration 104, loss = 0.54439805\n",
      "Iteration 105, loss = 0.54425081\n",
      "Iteration 106, loss = 0.54410270\n",
      "Iteration 107, loss = 0.54395506\n",
      "Iteration 108, loss = 0.54380998\n",
      "Iteration 109, loss = 0.54366574\n",
      "Iteration 110, loss = 0.54351877\n",
      "Iteration 111, loss = 0.54337781\n",
      "Iteration 112, loss = 0.54323374\n",
      "Iteration 113, loss = 0.54309210\n",
      "Iteration 114, loss = 0.54295100\n",
      "Iteration 115, loss = 0.54281142\n",
      "Iteration 116, loss = 0.54267272\n",
      "Iteration 117, loss = 0.54253262\n",
      "Iteration 118, loss = 0.54239744\n",
      "Iteration 119, loss = 0.54225985\n",
      "Iteration 120, loss = 0.54212310\n",
      "Iteration 121, loss = 0.54198775\n",
      "Iteration 122, loss = 0.54185347\n",
      "Iteration 123, loss = 0.54171872\n",
      "Iteration 124, loss = 0.54158439\n",
      "Iteration 125, loss = 0.54145289\n",
      "Iteration 126, loss = 0.54132108\n",
      "Iteration 127, loss = 0.54119055\n",
      "Iteration 128, loss = 0.54106044\n",
      "Iteration 129, loss = 0.54093012\n",
      "Iteration 130, loss = 0.54080167\n",
      "Iteration 131, loss = 0.54067332\n",
      "Iteration 132, loss = 0.54054529\n",
      "Iteration 133, loss = 0.54041567\n",
      "Iteration 134, loss = 0.54029069\n",
      "Iteration 135, loss = 0.54016447\n",
      "Iteration 136, loss = 0.54003849\n",
      "Iteration 137, loss = 0.53991440\n",
      "Iteration 138, loss = 0.53978912\n",
      "Iteration 139, loss = 0.53966633\n",
      "Iteration 140, loss = 0.53954172\n",
      "Iteration 141, loss = 0.53942178\n",
      "Iteration 142, loss = 0.53929745\n",
      "Iteration 143, loss = 0.53917708\n",
      "Iteration 144, loss = 0.53905519\n",
      "Iteration 145, loss = 0.53893753\n",
      "Iteration 146, loss = 0.53881625\n",
      "Iteration 147, loss = 0.53869652\n",
      "Iteration 148, loss = 0.53857968\n",
      "Iteration 149, loss = 0.53846033\n",
      "Iteration 150, loss = 0.53834430\n",
      "Iteration 151, loss = 0.53822510\n",
      "Iteration 152, loss = 0.53810879\n",
      "Iteration 153, loss = 0.53799290\n",
      "Iteration 154, loss = 0.53787659\n",
      "Iteration 155, loss = 0.53776221\n",
      "Iteration 156, loss = 0.53764568\n",
      "Iteration 157, loss = 0.53753221\n",
      "Iteration 158, loss = 0.53741763\n",
      "Iteration 159, loss = 0.53730622\n",
      "Iteration 160, loss = 0.53719164\n",
      "Iteration 161, loss = 0.53708055\n",
      "Iteration 162, loss = 0.53697016\n",
      "Iteration 163, loss = 0.53685794\n",
      "Iteration 164, loss = 0.53674644\n",
      "Iteration 165, loss = 0.53663660\n",
      "Iteration 166, loss = 0.53652554\n",
      "Iteration 167, loss = 0.53641555\n",
      "Iteration 168, loss = 0.53630615\n",
      "Iteration 169, loss = 0.53619701\n",
      "Iteration 170, loss = 0.53608893\n",
      "Iteration 171, loss = 0.53598197\n",
      "Iteration 172, loss = 0.53587391\n",
      "Iteration 173, loss = 0.53576620\n",
      "Iteration 174, loss = 0.53565951\n",
      "Iteration 175, loss = 0.53555330\n",
      "Iteration 176, loss = 0.53544646\n",
      "Iteration 177, loss = 0.53534132\n",
      "Iteration 178, loss = 0.53523491\n",
      "Iteration 179, loss = 0.53513110\n",
      "Iteration 180, loss = 0.53502638\n",
      "Iteration 181, loss = 0.53492068\n",
      "Iteration 182, loss = 0.53481751\n",
      "Iteration 183, loss = 0.53471487\n",
      "Iteration 184, loss = 0.53461282\n",
      "Iteration 185, loss = 0.53450946\n",
      "Iteration 186, loss = 0.53440638\n",
      "Iteration 187, loss = 0.53430379\n",
      "Iteration 188, loss = 0.53420208\n",
      "Iteration 189, loss = 0.53410012\n",
      "Iteration 190, loss = 0.53399955\n",
      "Iteration 191, loss = 0.53389833\n",
      "Iteration 192, loss = 0.53379866\n",
      "Iteration 193, loss = 0.53369864\n",
      "Iteration 194, loss = 0.53359903\n",
      "Iteration 195, loss = 0.53350011\n",
      "Iteration 196, loss = 0.53340171\n",
      "Iteration 197, loss = 0.53330129\n",
      "Iteration 198, loss = 0.53320401\n",
      "Iteration 199, loss = 0.53310751\n",
      "Iteration 200, loss = 0.53300756\n",
      "hidden_layer_size: [512, 256]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.85672505\n",
      "Iteration 2, loss = 0.57621906\n",
      "Iteration 3, loss = 0.56522245\n",
      "Iteration 4, loss = 0.56076681\n",
      "Iteration 5, loss = 0.55821352\n",
      "Iteration 6, loss = 0.55644576\n",
      "Iteration 7, loss = 0.55510073\n",
      "Iteration 8, loss = 0.55399327\n",
      "Iteration 9, loss = 0.55303944\n",
      "Iteration 10, loss = 0.55218222\n",
      "Iteration 11, loss = 0.55140449\n",
      "Iteration 12, loss = 0.55069223\n",
      "Iteration 13, loss = 0.55001727\n",
      "Iteration 14, loss = 0.54938848\n",
      "Iteration 15, loss = 0.54878234\n",
      "Iteration 16, loss = 0.54821338\n",
      "Iteration 17, loss = 0.54766797\n",
      "Iteration 18, loss = 0.54714223\n",
      "Iteration 19, loss = 0.54663095\n",
      "Iteration 20, loss = 0.54614263\n",
      "Iteration 21, loss = 0.54567043\n",
      "Iteration 22, loss = 0.54521485\n",
      "Iteration 23, loss = 0.54477262\n",
      "Iteration 24, loss = 0.54433575\n",
      "Iteration 25, loss = 0.54391401\n",
      "Iteration 26, loss = 0.54350091\n",
      "Iteration 27, loss = 0.54310375\n",
      "Iteration 28, loss = 0.54271051\n",
      "Iteration 29, loss = 0.54232951\n",
      "Iteration 30, loss = 0.54195140\n",
      "Iteration 31, loss = 0.54158502\n",
      "Iteration 32, loss = 0.54122706\n",
      "Iteration 33, loss = 0.54087691\n",
      "Iteration 34, loss = 0.54052497\n",
      "Iteration 35, loss = 0.54018714\n",
      "Iteration 36, loss = 0.53985988\n",
      "Iteration 37, loss = 0.53952673\n",
      "Iteration 38, loss = 0.53920534\n",
      "Iteration 39, loss = 0.53889006\n",
      "Iteration 40, loss = 0.53857361\n",
      "Iteration 41, loss = 0.53826414\n",
      "Iteration 42, loss = 0.53796407\n",
      "Iteration 43, loss = 0.53766479\n",
      "Iteration 44, loss = 0.53737152\n",
      "Iteration 45, loss = 0.53708421\n",
      "Iteration 46, loss = 0.53679915\n",
      "Iteration 47, loss = 0.53651347\n",
      "Iteration 48, loss = 0.53623464\n",
      "Iteration 49, loss = 0.53595737\n",
      "Iteration 50, loss = 0.53568938\n",
      "Iteration 51, loss = 0.53541941\n",
      "Iteration 52, loss = 0.53515289\n",
      "Iteration 53, loss = 0.53489282\n",
      "Iteration 54, loss = 0.53463112\n",
      "Iteration 55, loss = 0.53437595\n",
      "Iteration 56, loss = 0.53412266\n",
      "Iteration 57, loss = 0.53387296\n",
      "Iteration 58, loss = 0.53362662\n",
      "Iteration 59, loss = 0.53338031\n",
      "Iteration 60, loss = 0.53313677\n",
      "Iteration 61, loss = 0.53289661\n",
      "Iteration 62, loss = 0.53266036\n",
      "Iteration 63, loss = 0.53242453\n",
      "Iteration 64, loss = 0.53219154\n",
      "Iteration 65, loss = 0.53196189\n",
      "Iteration 66, loss = 0.53173072\n",
      "Iteration 67, loss = 0.53150685\n",
      "Iteration 68, loss = 0.53127946\n",
      "Iteration 69, loss = 0.53105863\n",
      "Iteration 70, loss = 0.53084247\n",
      "Iteration 71, loss = 0.53061926\n",
      "Iteration 72, loss = 0.53040369\n",
      "Iteration 73, loss = 0.53018918\n",
      "Iteration 74, loss = 0.52997584\n",
      "Iteration 75, loss = 0.52976456\n",
      "Iteration 76, loss = 0.52955699\n",
      "Iteration 77, loss = 0.52934649\n",
      "Iteration 78, loss = 0.52914177\n",
      "Iteration 79, loss = 0.52893726\n",
      "Iteration 80, loss = 0.52873329\n",
      "Iteration 81, loss = 0.52853214\n",
      "Iteration 82, loss = 0.52833249\n",
      "Iteration 83, loss = 0.52813387\n",
      "Iteration 84, loss = 0.52793659\n",
      "Iteration 85, loss = 0.52774076\n",
      "Iteration 86, loss = 0.52754709\n",
      "Iteration 87, loss = 0.52735393\n",
      "Iteration 88, loss = 0.52716270\n",
      "Iteration 89, loss = 0.52697320\n",
      "Iteration 90, loss = 0.52678342\n",
      "Iteration 91, loss = 0.52659735\n",
      "Iteration 92, loss = 0.52640951\n",
      "Iteration 93, loss = 0.52622422\n",
      "Iteration 94, loss = 0.52604319\n",
      "Iteration 95, loss = 0.52585652\n",
      "Iteration 96, loss = 0.52567687\n",
      "Iteration 97, loss = 0.52549657\n",
      "Iteration 98, loss = 0.52531611\n",
      "Iteration 99, loss = 0.52513884\n",
      "Iteration 100, loss = 0.52496206\n",
      "Iteration 101, loss = 0.52478718\n",
      "Iteration 102, loss = 0.52461095\n",
      "Iteration 103, loss = 0.52443597\n",
      "Iteration 104, loss = 0.52426646\n",
      "Iteration 105, loss = 0.52409316\n",
      "Iteration 106, loss = 0.52392125\n",
      "Iteration 107, loss = 0.52375352\n",
      "Iteration 108, loss = 0.52358315\n",
      "Iteration 109, loss = 0.52341650\n",
      "Iteration 110, loss = 0.52324864\n",
      "Iteration 111, loss = 0.52308214\n",
      "Iteration 112, loss = 0.52291812\n",
      "Iteration 113, loss = 0.52275470\n",
      "Iteration 114, loss = 0.52259202\n",
      "Iteration 115, loss = 0.52242815\n",
      "Iteration 116, loss = 0.52226872\n",
      "Iteration 117, loss = 0.52210931\n",
      "Iteration 118, loss = 0.52194731\n",
      "Iteration 119, loss = 0.52178747\n",
      "Iteration 120, loss = 0.52163049\n",
      "Iteration 121, loss = 0.52147338\n",
      "Iteration 122, loss = 0.52131609\n",
      "Iteration 123, loss = 0.52116201\n",
      "Iteration 124, loss = 0.52100664\n",
      "Iteration 125, loss = 0.52085371\n",
      "Iteration 126, loss = 0.52070012\n",
      "Iteration 127, loss = 0.52054748\n",
      "Iteration 128, loss = 0.52039537\n",
      "Iteration 129, loss = 0.52024278\n",
      "Iteration 130, loss = 0.52009521\n",
      "Iteration 131, loss = 0.51994457\n",
      "Iteration 132, loss = 0.51979367\n",
      "Iteration 133, loss = 0.51964778\n",
      "Iteration 134, loss = 0.51949959\n",
      "Iteration 135, loss = 0.51935476\n",
      "Iteration 136, loss = 0.51920809\n",
      "Iteration 137, loss = 0.51906397\n",
      "Iteration 138, loss = 0.51891796\n",
      "Iteration 139, loss = 0.51877389\n",
      "Iteration 140, loss = 0.51862898\n",
      "Iteration 141, loss = 0.51848697\n",
      "Iteration 142, loss = 0.51834480\n",
      "Iteration 143, loss = 0.51820162\n",
      "Iteration 144, loss = 0.51806092\n",
      "Iteration 145, loss = 0.51792035\n",
      "Iteration 146, loss = 0.51778287\n",
      "Iteration 147, loss = 0.51764184\n",
      "Iteration 148, loss = 0.51750384\n",
      "Iteration 149, loss = 0.51736549\n",
      "Iteration 150, loss = 0.51722948\n",
      "Iteration 151, loss = 0.51709190\n",
      "Iteration 152, loss = 0.51695570\n",
      "Iteration 153, loss = 0.51682013\n",
      "Iteration 154, loss = 0.51668773\n",
      "Iteration 155, loss = 0.51655143\n",
      "Iteration 156, loss = 0.51641615\n",
      "Iteration 157, loss = 0.51628275\n",
      "Iteration 158, loss = 0.51615295\n",
      "Iteration 159, loss = 0.51601843\n",
      "Iteration 160, loss = 0.51588692\n",
      "Iteration 161, loss = 0.51575591\n",
      "Iteration 162, loss = 0.51562580\n",
      "Iteration 163, loss = 0.51549383\n",
      "Iteration 164, loss = 0.51536499\n",
      "Iteration 165, loss = 0.51523545\n",
      "Iteration 166, loss = 0.51510596\n",
      "Iteration 167, loss = 0.51497856\n",
      "Iteration 168, loss = 0.51485089\n",
      "Iteration 169, loss = 0.51472266\n",
      "Iteration 170, loss = 0.51459580\n",
      "Iteration 171, loss = 0.51446640\n",
      "Iteration 172, loss = 0.51434163\n",
      "Iteration 173, loss = 0.51421576\n",
      "Iteration 174, loss = 0.51408987\n",
      "Iteration 175, loss = 0.51396659\n",
      "Iteration 176, loss = 0.51384194\n",
      "Iteration 177, loss = 0.51371723\n",
      "Iteration 178, loss = 0.51359486\n",
      "Iteration 179, loss = 0.51347178\n",
      "Iteration 180, loss = 0.51334935\n",
      "Iteration 181, loss = 0.51322769\n",
      "Iteration 182, loss = 0.51310455\n",
      "Iteration 183, loss = 0.51298427\n",
      "Iteration 184, loss = 0.51286268\n",
      "Iteration 185, loss = 0.51274274\n",
      "Iteration 186, loss = 0.51262239\n",
      "Iteration 187, loss = 0.51250291\n",
      "Iteration 188, loss = 0.51238402\n",
      "Iteration 189, loss = 0.51226476\n",
      "Iteration 190, loss = 0.51214537\n",
      "Iteration 191, loss = 0.51202717\n",
      "Iteration 192, loss = 0.51191099\n",
      "Iteration 193, loss = 0.51179386\n",
      "Iteration 194, loss = 0.51167613\n",
      "Iteration 195, loss = 0.51155832\n",
      "Iteration 196, loss = 0.51144378\n",
      "Iteration 197, loss = 0.51132775\n",
      "Iteration 198, loss = 0.51121139\n",
      "Iteration 199, loss = 0.51109684\n",
      "Iteration 200, loss = 0.51098553\n",
      "hidden_layer_size: [512, 256, 128]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.83786005\n",
      "Iteration 2, loss = 0.53631445\n",
      "Iteration 3, loss = 0.53262650\n",
      "Iteration 4, loss = 0.53070630\n",
      "Iteration 5, loss = 0.52921184\n",
      "Iteration 6, loss = 0.52792814\n",
      "Iteration 7, loss = 0.52680088\n",
      "Iteration 8, loss = 0.52576838\n",
      "Iteration 9, loss = 0.52482025\n",
      "Iteration 10, loss = 0.52394015\n",
      "Iteration 11, loss = 0.52310287\n",
      "Iteration 12, loss = 0.52233403\n",
      "Iteration 13, loss = 0.52159424\n",
      "Iteration 14, loss = 0.52089123\n",
      "Iteration 15, loss = 0.52021255\n",
      "Iteration 16, loss = 0.51957148\n",
      "Iteration 17, loss = 0.51894691\n",
      "Iteration 18, loss = 0.51835595\n",
      "Iteration 19, loss = 0.51777435\n",
      "Iteration 20, loss = 0.51721581\n",
      "Iteration 21, loss = 0.51667558\n",
      "Iteration 22, loss = 0.51613723\n",
      "Iteration 23, loss = 0.51563321\n",
      "Iteration 24, loss = 0.51512665\n",
      "Iteration 25, loss = 0.51463847\n",
      "Iteration 26, loss = 0.51416003\n",
      "Iteration 27, loss = 0.51369737\n",
      "Iteration 28, loss = 0.51324175\n",
      "Iteration 29, loss = 0.51279439\n",
      "Iteration 30, loss = 0.51236081\n",
      "Iteration 31, loss = 0.51192900\n",
      "Iteration 32, loss = 0.51150881\n",
      "Iteration 33, loss = 0.51109535\n",
      "Iteration 34, loss = 0.51069767\n",
      "Iteration 35, loss = 0.51029657\n",
      "Iteration 36, loss = 0.50991168\n",
      "Iteration 37, loss = 0.50952734\n",
      "Iteration 38, loss = 0.50914244\n",
      "Iteration 39, loss = 0.50877175\n",
      "Iteration 40, loss = 0.50840747\n",
      "Iteration 41, loss = 0.50804684\n",
      "Iteration 42, loss = 0.50768919\n",
      "Iteration 43, loss = 0.50733765\n",
      "Iteration 44, loss = 0.50699221\n",
      "Iteration 45, loss = 0.50664861\n",
      "Iteration 46, loss = 0.50631435\n",
      "Iteration 47, loss = 0.50598513\n",
      "Iteration 48, loss = 0.50565859\n",
      "Iteration 49, loss = 0.50533295\n",
      "Iteration 50, loss = 0.50501450\n",
      "Iteration 51, loss = 0.50469672\n",
      "Iteration 52, loss = 0.50438522\n",
      "Iteration 53, loss = 0.50407352\n",
      "Iteration 54, loss = 0.50376687\n",
      "Iteration 55, loss = 0.50346788\n",
      "Iteration 56, loss = 0.50316619\n",
      "Iteration 57, loss = 0.50287188\n",
      "Iteration 58, loss = 0.50257724\n",
      "Iteration 59, loss = 0.50228974\n",
      "Iteration 60, loss = 0.50200338\n",
      "Iteration 61, loss = 0.50172066\n",
      "Iteration 62, loss = 0.50143731\n",
      "Iteration 63, loss = 0.50115956\n",
      "Iteration 64, loss = 0.50088313\n",
      "Iteration 65, loss = 0.50061241\n",
      "Iteration 66, loss = 0.50033988\n",
      "Iteration 67, loss = 0.50007233\n",
      "Iteration 68, loss = 0.49980584\n",
      "Iteration 69, loss = 0.49954259\n",
      "Iteration 70, loss = 0.49928556\n",
      "Iteration 71, loss = 0.49902342\n",
      "Iteration 72, loss = 0.49876764\n",
      "Iteration 73, loss = 0.49851331\n",
      "Iteration 74, loss = 0.49826029\n",
      "Iteration 75, loss = 0.49801005\n",
      "Iteration 76, loss = 0.49776373\n",
      "Iteration 77, loss = 0.49751784\n",
      "Iteration 78, loss = 0.49727195\n",
      "Iteration 79, loss = 0.49702908\n",
      "Iteration 80, loss = 0.49679182\n",
      "Iteration 81, loss = 0.49655137\n",
      "Iteration 82, loss = 0.49631548\n",
      "Iteration 83, loss = 0.49607953\n",
      "Iteration 84, loss = 0.49584374\n",
      "Iteration 85, loss = 0.49561298\n",
      "Iteration 86, loss = 0.49538150\n",
      "Iteration 87, loss = 0.49515278\n",
      "Iteration 88, loss = 0.49492743\n",
      "Iteration 89, loss = 0.49470041\n",
      "Iteration 90, loss = 0.49447693\n",
      "Iteration 91, loss = 0.49425494\n",
      "Iteration 92, loss = 0.49403110\n",
      "Iteration 93, loss = 0.49381246\n",
      "Iteration 94, loss = 0.49359585\n",
      "Iteration 95, loss = 0.49338172\n",
      "Iteration 96, loss = 0.49316267\n",
      "Iteration 97, loss = 0.49294964\n",
      "Iteration 98, loss = 0.49273648\n",
      "Iteration 99, loss = 0.49252535\n",
      "Iteration 100, loss = 0.49231397\n",
      "Iteration 101, loss = 0.49210368\n",
      "Iteration 102, loss = 0.49189614\n",
      "Iteration 103, loss = 0.49168961\n",
      "Iteration 104, loss = 0.49148766\n",
      "Iteration 105, loss = 0.49128305\n",
      "Iteration 106, loss = 0.49108060\n",
      "Iteration 107, loss = 0.49087998\n",
      "Iteration 108, loss = 0.49068090\n",
      "Iteration 109, loss = 0.49047950\n",
      "Iteration 110, loss = 0.49028357\n",
      "Iteration 111, loss = 0.49008465\n",
      "Iteration 112, loss = 0.48989177\n",
      "Iteration 113, loss = 0.48969512\n",
      "Iteration 114, loss = 0.48950040\n",
      "Iteration 115, loss = 0.48930943\n",
      "Iteration 116, loss = 0.48911444\n",
      "Iteration 117, loss = 0.48892485\n",
      "Iteration 118, loss = 0.48873599\n",
      "Iteration 119, loss = 0.48855039\n",
      "Iteration 120, loss = 0.48836032\n",
      "Iteration 121, loss = 0.48817349\n",
      "Iteration 122, loss = 0.48798823\n",
      "Iteration 123, loss = 0.48780063\n",
      "Iteration 124, loss = 0.48761828\n",
      "Iteration 125, loss = 0.48743568\n",
      "Iteration 126, loss = 0.48725560\n",
      "Iteration 127, loss = 0.48707226\n",
      "Iteration 128, loss = 0.48688943\n",
      "Iteration 129, loss = 0.48671156\n",
      "Iteration 130, loss = 0.48653559\n",
      "Iteration 131, loss = 0.48635664\n",
      "Iteration 132, loss = 0.48618235\n",
      "Iteration 133, loss = 0.48600168\n",
      "Iteration 134, loss = 0.48582609\n",
      "Iteration 135, loss = 0.48565161\n",
      "Iteration 136, loss = 0.48548016\n",
      "Iteration 137, loss = 0.48530655\n",
      "Iteration 138, loss = 0.48513373\n",
      "Iteration 139, loss = 0.48496402\n",
      "Iteration 140, loss = 0.48479185\n",
      "Iteration 141, loss = 0.48462303\n",
      "Iteration 142, loss = 0.48445604\n",
      "Iteration 143, loss = 0.48428487\n",
      "Iteration 144, loss = 0.48411638\n",
      "Iteration 145, loss = 0.48395163\n",
      "Iteration 146, loss = 0.48378499\n",
      "Iteration 147, loss = 0.48361707\n",
      "Iteration 148, loss = 0.48345332\n",
      "Iteration 149, loss = 0.48328614\n",
      "Iteration 150, loss = 0.48312656\n",
      "Iteration 151, loss = 0.48296398\n",
      "Iteration 152, loss = 0.48279854\n",
      "Iteration 153, loss = 0.48263642\n",
      "Iteration 154, loss = 0.48247722\n",
      "Iteration 155, loss = 0.48231722\n",
      "Iteration 156, loss = 0.48215631\n",
      "Iteration 157, loss = 0.48199771\n",
      "Iteration 158, loss = 0.48183890\n",
      "Iteration 159, loss = 0.48168017\n",
      "Iteration 160, loss = 0.48152334\n",
      "Iteration 161, loss = 0.48136693\n",
      "Iteration 162, loss = 0.48121282\n",
      "Iteration 163, loss = 0.48105738\n",
      "Iteration 164, loss = 0.48090045\n",
      "Iteration 165, loss = 0.48074580\n",
      "Iteration 166, loss = 0.48059135\n",
      "Iteration 167, loss = 0.48043914\n",
      "Iteration 168, loss = 0.48028725\n",
      "Iteration 169, loss = 0.48013417\n",
      "Iteration 170, loss = 0.47998340\n",
      "Iteration 171, loss = 0.47983229\n",
      "Iteration 172, loss = 0.47968175\n",
      "Iteration 173, loss = 0.47953530\n",
      "Iteration 174, loss = 0.47938312\n",
      "Iteration 175, loss = 0.47923543\n",
      "Iteration 176, loss = 0.47908556\n",
      "Iteration 177, loss = 0.47893858\n",
      "Iteration 178, loss = 0.47879299\n",
      "Iteration 179, loss = 0.47864448\n",
      "Iteration 180, loss = 0.47849861\n",
      "Iteration 181, loss = 0.47835216\n",
      "Iteration 182, loss = 0.47820820\n",
      "Iteration 183, loss = 0.47806204\n",
      "Iteration 184, loss = 0.47791920\n",
      "Iteration 185, loss = 0.47777530\n",
      "Iteration 186, loss = 0.47763323\n",
      "Iteration 187, loss = 0.47748795\n",
      "Iteration 188, loss = 0.47734597\n",
      "Iteration 189, loss = 0.47720512\n",
      "Iteration 190, loss = 0.47706291\n",
      "Iteration 191, loss = 0.47692312\n",
      "Iteration 192, loss = 0.47678375\n",
      "Iteration 193, loss = 0.47664076\n",
      "Iteration 194, loss = 0.47650322\n",
      "Iteration 195, loss = 0.47636322\n",
      "Iteration 196, loss = 0.47622425\n",
      "Iteration 197, loss = 0.47608688\n",
      "Iteration 198, loss = 0.47594867\n",
      "Iteration 199, loss = 0.47581179\n",
      "Iteration 200, loss = 0.47567545\n",
      "hidden_layer_size: [512, 256, 128, 64]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.90930399\n",
      "Iteration 2, loss = 0.60190621\n",
      "Iteration 3, loss = 0.53674433\n",
      "Iteration 4, loss = 0.52880154\n",
      "Iteration 5, loss = 0.52567206\n",
      "Iteration 6, loss = 0.52374652\n",
      "Iteration 7, loss = 0.52226000\n",
      "Iteration 8, loss = 0.52100844\n",
      "Iteration 9, loss = 0.51990123\n",
      "Iteration 10, loss = 0.51890331\n",
      "Iteration 11, loss = 0.51796550\n",
      "Iteration 12, loss = 0.51707890\n",
      "Iteration 13, loss = 0.51624444\n",
      "Iteration 14, loss = 0.51545149\n",
      "Iteration 15, loss = 0.51470449\n",
      "Iteration 16, loss = 0.51397869\n",
      "Iteration 17, loss = 0.51328775\n",
      "Iteration 18, loss = 0.51260802\n",
      "Iteration 19, loss = 0.51196275\n",
      "Iteration 20, loss = 0.51133686\n",
      "Iteration 21, loss = 0.51072778\n",
      "Iteration 22, loss = 0.51014251\n",
      "Iteration 23, loss = 0.50956066\n",
      "Iteration 24, loss = 0.50901132\n",
      "Iteration 25, loss = 0.50846365\n",
      "Iteration 26, loss = 0.50792510\n",
      "Iteration 27, loss = 0.50741484\n",
      "Iteration 28, loss = 0.50689086\n",
      "Iteration 29, loss = 0.50639249\n",
      "Iteration 30, loss = 0.50591384\n",
      "Iteration 31, loss = 0.50543321\n",
      "Iteration 32, loss = 0.50496633\n",
      "Iteration 33, loss = 0.50450748\n",
      "Iteration 34, loss = 0.50405695\n",
      "Iteration 35, loss = 0.50362059\n",
      "Iteration 36, loss = 0.50319000\n",
      "Iteration 37, loss = 0.50275620\n",
      "Iteration 38, loss = 0.50233432\n",
      "Iteration 39, loss = 0.50191761\n",
      "Iteration 40, loss = 0.50152254\n",
      "Iteration 41, loss = 0.50111753\n",
      "Iteration 42, loss = 0.50071991\n",
      "Iteration 43, loss = 0.50032985\n",
      "Iteration 44, loss = 0.49994328\n",
      "Iteration 45, loss = 0.49956543\n",
      "Iteration 46, loss = 0.49919290\n",
      "Iteration 47, loss = 0.49882447\n",
      "Iteration 48, loss = 0.49846131\n",
      "Iteration 49, loss = 0.49810541\n",
      "Iteration 50, loss = 0.49775299\n",
      "Iteration 51, loss = 0.49740208\n",
      "Iteration 52, loss = 0.49705536\n",
      "Iteration 53, loss = 0.49671481\n",
      "Iteration 54, loss = 0.49637967\n",
      "Iteration 55, loss = 0.49604371\n",
      "Iteration 56, loss = 0.49571685\n",
      "Iteration 57, loss = 0.49538655\n",
      "Iteration 58, loss = 0.49506366\n",
      "Iteration 59, loss = 0.49474051\n",
      "Iteration 60, loss = 0.49442196\n",
      "Iteration 61, loss = 0.49411127\n",
      "Iteration 62, loss = 0.49380372\n",
      "Iteration 63, loss = 0.49349413\n",
      "Iteration 64, loss = 0.49318725\n",
      "Iteration 65, loss = 0.49288717\n",
      "Iteration 66, loss = 0.49259588\n",
      "Iteration 67, loss = 0.49230036\n",
      "Iteration 68, loss = 0.49200317\n",
      "Iteration 69, loss = 0.49171584\n",
      "Iteration 70, loss = 0.49143035\n",
      "Iteration 71, loss = 0.49114051\n",
      "Iteration 72, loss = 0.49085717\n",
      "Iteration 73, loss = 0.49057978\n",
      "Iteration 74, loss = 0.49029967\n",
      "Iteration 75, loss = 0.49002534\n",
      "Iteration 76, loss = 0.48975120\n",
      "Iteration 77, loss = 0.48948203\n",
      "Iteration 78, loss = 0.48921129\n",
      "Iteration 79, loss = 0.48893979\n",
      "Iteration 80, loss = 0.48867929\n",
      "Iteration 81, loss = 0.48841119\n",
      "Iteration 82, loss = 0.48815639\n",
      "Iteration 83, loss = 0.48788856\n",
      "Iteration 84, loss = 0.48763189\n",
      "Iteration 85, loss = 0.48738084\n",
      "Iteration 86, loss = 0.48712050\n",
      "Iteration 87, loss = 0.48687274\n",
      "Iteration 88, loss = 0.48662281\n",
      "Iteration 89, loss = 0.48637977\n",
      "Iteration 90, loss = 0.48612432\n",
      "Iteration 91, loss = 0.48587843\n",
      "Iteration 92, loss = 0.48563914\n",
      "Iteration 93, loss = 0.48539463\n",
      "Iteration 94, loss = 0.48515212\n",
      "Iteration 95, loss = 0.48491679\n",
      "Iteration 96, loss = 0.48467907\n",
      "Iteration 97, loss = 0.48444281\n",
      "Iteration 98, loss = 0.48420720\n",
      "Iteration 99, loss = 0.48397310\n",
      "Iteration 100, loss = 0.48373886\n",
      "Iteration 101, loss = 0.48351040\n",
      "Iteration 102, loss = 0.48328480\n",
      "Iteration 103, loss = 0.48305566\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "models_f = {}\n",
    "for hidden_layer in hidden_layers:\n",
    "    print(f'hidden_layer_size: {hidden_layer}')\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=hidden_layer,\n",
    "                        activation='relu',\n",
    "                        solver='sgd',\n",
    "                        alpha=0,\n",
    "                        batch_size=32,\n",
    "                        learning_rate='invscaling',\n",
    "                        max_iter=200,\n",
    "                        n_iter_no_change=5,\n",
    "                        verbose=True)\n",
    "    mlp.fit(x_train, y_train)\n",
    "    \n",
    "    models_f[str(hidden_layer)] = mlp"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-04T08:44:04.148853Z"
    }
   },
   "id": "7fbb393a46ce22b"
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_layer_size: [512, 256, 128]\n",
      "Iteration 1, loss = 0.87514613\n",
      "Iteration 2, loss = 0.54215932\n",
      "Iteration 3, loss = 0.53186683\n",
      "Iteration 4, loss = 0.52898992\n",
      "Iteration 5, loss = 0.52726676\n",
      "Iteration 6, loss = 0.52591673\n",
      "Iteration 7, loss = 0.52474353\n",
      "Iteration 8, loss = 0.52366832\n",
      "Iteration 9, loss = 0.52269450\n",
      "Iteration 10, loss = 0.52178630\n",
      "Iteration 11, loss = 0.52093355\n",
      "Iteration 12, loss = 0.52013140\n",
      "Iteration 13, loss = 0.51937098\n",
      "Iteration 14, loss = 0.51864888\n",
      "Iteration 15, loss = 0.51795255\n",
      "Iteration 16, loss = 0.51728424\n",
      "Iteration 17, loss = 0.51664530\n",
      "Iteration 18, loss = 0.51602793\n",
      "Iteration 19, loss = 0.51543852\n",
      "Iteration 20, loss = 0.51486126\n",
      "Iteration 21, loss = 0.51429758\n",
      "Iteration 22, loss = 0.51375710\n",
      "Iteration 23, loss = 0.51322473\n",
      "Iteration 24, loss = 0.51270992\n",
      "Iteration 25, loss = 0.51220830\n",
      "Iteration 26, loss = 0.51171612\n",
      "Iteration 27, loss = 0.51123707\n",
      "Iteration 28, loss = 0.51077062\n",
      "Iteration 29, loss = 0.51031305\n",
      "Iteration 30, loss = 0.50986139\n",
      "Iteration 31, loss = 0.50942647\n",
      "Iteration 32, loss = 0.50899350\n",
      "Iteration 33, loss = 0.50856854\n",
      "Iteration 34, loss = 0.50815673\n",
      "Iteration 35, loss = 0.50774541\n",
      "Iteration 36, loss = 0.50734478\n",
      "Iteration 37, loss = 0.50695172\n",
      "Iteration 38, loss = 0.50655895\n",
      "Iteration 39, loss = 0.50618285\n",
      "Iteration 40, loss = 0.50580100\n",
      "Iteration 41, loss = 0.50543109\n",
      "Iteration 42, loss = 0.50506441\n",
      "Iteration 43, loss = 0.50470825\n",
      "Iteration 44, loss = 0.50435396\n",
      "Iteration 45, loss = 0.50400055\n",
      "Iteration 46, loss = 0.50365629\n",
      "Iteration 47, loss = 0.50331348\n",
      "Iteration 48, loss = 0.50297429\n",
      "Iteration 49, loss = 0.50264428\n",
      "Iteration 50, loss = 0.50231370\n",
      "Iteration 51, loss = 0.50199067\n",
      "Iteration 52, loss = 0.50166784\n",
      "Iteration 53, loss = 0.50135278\n",
      "Iteration 54, loss = 0.50103931\n",
      "Iteration 55, loss = 0.50072795\n",
      "Iteration 56, loss = 0.50042326\n",
      "Iteration 57, loss = 0.50011747\n",
      "Iteration 58, loss = 0.49981837\n",
      "Iteration 59, loss = 0.49951591\n",
      "Iteration 60, loss = 0.49922639\n",
      "Iteration 61, loss = 0.49893131\n",
      "Iteration 62, loss = 0.49864414\n",
      "Iteration 63, loss = 0.49835697\n",
      "Iteration 64, loss = 0.49807257\n",
      "Iteration 65, loss = 0.49779578\n",
      "Iteration 66, loss = 0.49752190\n",
      "Iteration 67, loss = 0.49724390\n",
      "Iteration 68, loss = 0.49697177\n",
      "Iteration 69, loss = 0.49669716\n",
      "Iteration 70, loss = 0.49643314\n",
      "Iteration 71, loss = 0.49616644\n",
      "Iteration 72, loss = 0.49590209\n",
      "Iteration 73, loss = 0.49564229\n",
      "Iteration 74, loss = 0.49537928\n",
      "Iteration 75, loss = 0.49512156\n",
      "Iteration 76, loss = 0.49486817\n",
      "Iteration 77, loss = 0.49461686\n",
      "Iteration 78, loss = 0.49436522\n",
      "Iteration 79, loss = 0.49411673\n",
      "Iteration 80, loss = 0.49386941\n",
      "Iteration 81, loss = 0.49362537\n",
      "Iteration 82, loss = 0.49337984\n",
      "Iteration 83, loss = 0.49314218\n",
      "Iteration 84, loss = 0.49289577\n",
      "Iteration 85, loss = 0.49265817\n",
      "Iteration 86, loss = 0.49242300\n",
      "Iteration 87, loss = 0.49218877\n",
      "Iteration 88, loss = 0.49195602\n",
      "Iteration 89, loss = 0.49172646\n",
      "Iteration 90, loss = 0.49149539\n",
      "Iteration 91, loss = 0.49126652\n",
      "Iteration 92, loss = 0.49103941\n",
      "Iteration 93, loss = 0.49081686\n",
      "Iteration 94, loss = 0.49059101\n",
      "Iteration 95, loss = 0.49036979\n",
      "Iteration 96, loss = 0.49014496\n",
      "Iteration 97, loss = 0.48992809\n",
      "Iteration 98, loss = 0.48970962\n",
      "Iteration 99, loss = 0.48949374\n",
      "Iteration 100, loss = 0.48927855\n",
      "Iteration 101, loss = 0.48906513\n",
      "Iteration 102, loss = 0.48884915\n",
      "Iteration 103, loss = 0.48863748\n",
      "Iteration 104, loss = 0.48842595\n",
      "Iteration 105, loss = 0.48821936\n",
      "Iteration 106, loss = 0.48800757\n",
      "Iteration 107, loss = 0.48780235\n",
      "Iteration 108, loss = 0.48759725\n",
      "Iteration 109, loss = 0.48739224\n",
      "Iteration 110, loss = 0.48718778\n",
      "Iteration 111, loss = 0.48698710\n",
      "Iteration 112, loss = 0.48678772\n",
      "Iteration 113, loss = 0.48658589\n",
      "Iteration 114, loss = 0.48638634\n",
      "Iteration 115, loss = 0.48618835\n",
      "Iteration 116, loss = 0.48599366\n",
      "Iteration 117, loss = 0.48579931\n",
      "Iteration 118, loss = 0.48560087\n",
      "Iteration 119, loss = 0.48540749\n",
      "Iteration 120, loss = 0.48521390\n",
      "Iteration 121, loss = 0.48502441\n",
      "Iteration 122, loss = 0.48482967\n",
      "Iteration 123, loss = 0.48464182\n",
      "Iteration 124, loss = 0.48445351\n",
      "Iteration 125, loss = 0.48426743\n",
      "Iteration 126, loss = 0.48407985\n",
      "Iteration 127, loss = 0.48389325\n",
      "Iteration 128, loss = 0.48370555\n",
      "Iteration 129, loss = 0.48352372\n",
      "Iteration 130, loss = 0.48334021\n",
      "Iteration 131, loss = 0.48315491\n",
      "Iteration 132, loss = 0.48297396\n",
      "Iteration 133, loss = 0.48279338\n",
      "Iteration 134, loss = 0.48261475\n",
      "Iteration 135, loss = 0.48243399\n",
      "Iteration 136, loss = 0.48225349\n",
      "Iteration 137, loss = 0.48207621\n",
      "Iteration 138, loss = 0.48190030\n",
      "Iteration 139, loss = 0.48172298\n",
      "Iteration 140, loss = 0.48154993\n",
      "Iteration 141, loss = 0.48137513\n",
      "Iteration 142, loss = 0.48119971\n",
      "Iteration 143, loss = 0.48102704\n",
      "Iteration 144, loss = 0.48085541\n",
      "Iteration 145, loss = 0.48068424\n",
      "Iteration 146, loss = 0.48051223\n",
      "Iteration 147, loss = 0.48034111\n",
      "Iteration 148, loss = 0.48017101\n",
      "Iteration 149, loss = 0.48000201\n",
      "Iteration 150, loss = 0.47983541\n",
      "Iteration 151, loss = 0.47966799\n",
      "Iteration 152, loss = 0.47949999\n",
      "Iteration 153, loss = 0.47933502\n",
      "Iteration 154, loss = 0.47917151\n",
      "Iteration 155, loss = 0.47900673\n",
      "Iteration 156, loss = 0.47884518\n",
      "Iteration 157, loss = 0.47867838\n",
      "Iteration 158, loss = 0.47851671\n",
      "Iteration 159, loss = 0.47835401\n",
      "Iteration 160, loss = 0.47819492\n",
      "Iteration 161, loss = 0.47803269\n",
      "Iteration 162, loss = 0.47787373\n",
      "Iteration 163, loss = 0.47771648\n",
      "Iteration 164, loss = 0.47755440\n",
      "Iteration 165, loss = 0.47739642\n",
      "Iteration 166, loss = 0.47723813\n",
      "Iteration 167, loss = 0.47708289\n",
      "Iteration 168, loss = 0.47692602\n",
      "Iteration 169, loss = 0.47676919\n",
      "Iteration 170, loss = 0.47661438\n",
      "Iteration 171, loss = 0.47645767\n",
      "Iteration 172, loss = 0.47630501\n",
      "Iteration 173, loss = 0.47614925\n",
      "Iteration 174, loss = 0.47599544\n",
      "Iteration 175, loss = 0.47584291\n",
      "Iteration 176, loss = 0.47569098\n",
      "Iteration 177, loss = 0.47553970\n",
      "Iteration 178, loss = 0.47538989\n",
      "Iteration 179, loss = 0.47524190\n",
      "Iteration 180, loss = 0.47508873\n",
      "Iteration 181, loss = 0.47494081\n",
      "Iteration 182, loss = 0.47479103\n",
      "Iteration 183, loss = 0.47464253\n",
      "Iteration 184, loss = 0.47449460\n",
      "Iteration 185, loss = 0.47434795\n",
      "Iteration 186, loss = 0.47420068\n",
      "Iteration 187, loss = 0.47405418\n",
      "Iteration 188, loss = 0.47391111\n",
      "Iteration 189, loss = 0.47376377\n",
      "Iteration 190, loss = 0.47361869\n",
      "Iteration 191, loss = 0.47347349\n",
      "Iteration 192, loss = 0.47332968\n",
      "Iteration 193, loss = 0.47318642\n",
      "Iteration 194, loss = 0.47304361\n",
      "Iteration 195, loss = 0.47290078\n",
      "Iteration 196, loss = 0.47275947\n",
      "Iteration 197, loss = 0.47261736\n",
      "Iteration 198, loss = 0.47247631\n",
      "Iteration 199, loss = 0.47233805\n",
      "Iteration 200, loss = 0.47219348\n",
      "hidden_layer_size: [512, 256, 128, 64]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.88384739\n",
      "Iteration 2, loss = 0.52783294\n",
      "Iteration 3, loss = 0.52314994\n",
      "Iteration 4, loss = 0.52067399\n",
      "Iteration 5, loss = 0.51888259\n",
      "Iteration 6, loss = 0.51739704\n",
      "Iteration 7, loss = 0.51610765\n",
      "Iteration 8, loss = 0.51498103\n",
      "Iteration 9, loss = 0.51392380\n",
      "Iteration 10, loss = 0.51297233\n",
      "Iteration 11, loss = 0.51208177\n",
      "Iteration 12, loss = 0.51124432\n",
      "Iteration 13, loss = 0.51043616\n",
      "Iteration 14, loss = 0.50967090\n",
      "Iteration 15, loss = 0.50894539\n",
      "Iteration 16, loss = 0.50824690\n",
      "Iteration 17, loss = 0.50757261\n",
      "Iteration 18, loss = 0.50693315\n",
      "Iteration 19, loss = 0.50630039\n",
      "Iteration 20, loss = 0.50570229\n",
      "Iteration 21, loss = 0.50511680\n",
      "Iteration 22, loss = 0.50453618\n",
      "Iteration 23, loss = 0.50398529\n",
      "Iteration 24, loss = 0.50344973\n",
      "Iteration 25, loss = 0.50291882\n",
      "Iteration 26, loss = 0.50240439\n",
      "Iteration 27, loss = 0.50190257\n",
      "Iteration 28, loss = 0.50140949\n",
      "Iteration 29, loss = 0.50092840\n",
      "Iteration 30, loss = 0.50046641\n",
      "Iteration 31, loss = 0.50000484\n",
      "Iteration 32, loss = 0.49955032\n",
      "Iteration 33, loss = 0.49910974\n",
      "Iteration 34, loss = 0.49867446\n",
      "Iteration 35, loss = 0.49824732\n",
      "Iteration 36, loss = 0.49783245\n",
      "Iteration 37, loss = 0.49741363\n",
      "Iteration 38, loss = 0.49700965\n",
      "Iteration 39, loss = 0.49660620\n",
      "Iteration 40, loss = 0.49621903\n",
      "Iteration 41, loss = 0.49583404\n",
      "Iteration 42, loss = 0.49545330\n",
      "Iteration 43, loss = 0.49507830\n",
      "Iteration 44, loss = 0.49470480\n",
      "Iteration 45, loss = 0.49434219\n",
      "Iteration 46, loss = 0.49397709\n",
      "Iteration 47, loss = 0.49362120\n",
      "Iteration 48, loss = 0.49326634\n",
      "Iteration 49, loss = 0.49292157\n",
      "Iteration 50, loss = 0.49257993\n",
      "Iteration 51, loss = 0.49224077\n",
      "Iteration 52, loss = 0.49190480\n",
      "Iteration 53, loss = 0.49157249\n",
      "Iteration 54, loss = 0.49125055\n",
      "Iteration 55, loss = 0.49091856\n",
      "Iteration 56, loss = 0.49060197\n",
      "Iteration 57, loss = 0.49028588\n",
      "Iteration 58, loss = 0.48997241\n",
      "Iteration 59, loss = 0.48965720\n",
      "Iteration 60, loss = 0.48935128\n",
      "Iteration 61, loss = 0.48905252\n",
      "Iteration 62, loss = 0.48874664\n",
      "Iteration 63, loss = 0.48845196\n",
      "Iteration 64, loss = 0.48815850\n",
      "Iteration 65, loss = 0.48786605\n",
      "Iteration 66, loss = 0.48757064\n",
      "Iteration 67, loss = 0.48728341\n",
      "Iteration 68, loss = 0.48699674\n",
      "Iteration 69, loss = 0.48671518\n",
      "Iteration 70, loss = 0.48643614\n",
      "Iteration 71, loss = 0.48616211\n",
      "Iteration 72, loss = 0.48588680\n",
      "Iteration 73, loss = 0.48561481\n",
      "Iteration 74, loss = 0.48534237\n",
      "Iteration 75, loss = 0.48507554\n",
      "Iteration 76, loss = 0.48480748\n",
      "Iteration 77, loss = 0.48454127\n",
      "Iteration 78, loss = 0.48428708\n",
      "Iteration 79, loss = 0.48402061\n",
      "Iteration 80, loss = 0.48376713\n",
      "Iteration 81, loss = 0.48350563\n",
      "Iteration 82, loss = 0.48325268\n",
      "Iteration 83, loss = 0.48300229\n",
      "Iteration 84, loss = 0.48275023\n",
      "Iteration 85, loss = 0.48249969\n",
      "Iteration 86, loss = 0.48225327\n",
      "Iteration 87, loss = 0.48200907\n",
      "Iteration 88, loss = 0.48176538\n",
      "Iteration 89, loss = 0.48152284\n",
      "Iteration 90, loss = 0.48128711\n",
      "Iteration 91, loss = 0.48104175\n",
      "Iteration 92, loss = 0.48080719\n",
      "Iteration 93, loss = 0.48056767\n",
      "Iteration 94, loss = 0.48033489\n",
      "Iteration 95, loss = 0.48010365\n",
      "Iteration 96, loss = 0.47987427\n",
      "Iteration 97, loss = 0.47964281\n",
      "Iteration 98, loss = 0.47941298\n",
      "Iteration 99, loss = 0.47918571\n",
      "Iteration 100, loss = 0.47896016\n",
      "Iteration 101, loss = 0.47873911\n",
      "Iteration 102, loss = 0.47851536\n",
      "Iteration 103, loss = 0.47829647\n",
      "Iteration 104, loss = 0.47807260\n",
      "Iteration 105, loss = 0.47785139\n",
      "Iteration 106, loss = 0.47763426\n",
      "Iteration 107, loss = 0.47741779\n",
      "Iteration 108, loss = 0.47720123\n",
      "Iteration 109, loss = 0.47698799\n",
      "Iteration 110, loss = 0.47677917\n",
      "Iteration 111, loss = 0.47656114\n",
      "Iteration 112, loss = 0.47635280\n",
      "Iteration 113, loss = 0.47614338\n",
      "Iteration 114, loss = 0.47593543\n",
      "Iteration 115, loss = 0.47573065\n",
      "Iteration 116, loss = 0.47552368\n",
      "Iteration 117, loss = 0.47531491\n",
      "Iteration 118, loss = 0.47511210\n",
      "Iteration 119, loss = 0.47490948\n",
      "Iteration 120, loss = 0.47470582\n",
      "Iteration 121, loss = 0.47450659\n",
      "Iteration 122, loss = 0.47430832\n",
      "Iteration 123, loss = 0.47410776\n",
      "Iteration 124, loss = 0.47390759\n",
      "Iteration 125, loss = 0.47371483\n",
      "Iteration 126, loss = 0.47351632\n",
      "Iteration 127, loss = 0.47332417\n",
      "Iteration 128, loss = 0.47312455\n",
      "Iteration 129, loss = 0.47293380\n",
      "Iteration 130, loss = 0.47274336\n",
      "Iteration 131, loss = 0.47254988\n",
      "Iteration 132, loss = 0.47235705\n",
      "Iteration 133, loss = 0.47216809\n",
      "Iteration 134, loss = 0.47197977\n",
      "Iteration 135, loss = 0.47179168\n",
      "Iteration 136, loss = 0.47160144\n",
      "Iteration 137, loss = 0.47141690\n",
      "Iteration 138, loss = 0.47123251\n",
      "Iteration 139, loss = 0.47104774\n",
      "Iteration 140, loss = 0.47086428\n",
      "Iteration 141, loss = 0.47068004\n",
      "Iteration 142, loss = 0.47049676\n",
      "Iteration 143, loss = 0.47031540\n",
      "Iteration 144, loss = 0.47013462\n",
      "Iteration 145, loss = 0.46995286\n",
      "Iteration 146, loss = 0.46977397\n",
      "Iteration 147, loss = 0.46959702\n",
      "Iteration 148, loss = 0.46941779\n",
      "Iteration 149, loss = 0.46923822\n",
      "Iteration 150, loss = 0.46906347\n",
      "Iteration 151, loss = 0.46888804\n",
      "Iteration 152, loss = 0.46871573\n",
      "Iteration 153, loss = 0.46853707\n",
      "Iteration 154, loss = 0.46836327\n",
      "Iteration 155, loss = 0.46819516\n",
      "Iteration 156, loss = 0.46801929\n",
      "Iteration 157, loss = 0.46784903\n",
      "Iteration 158, loss = 0.46768053\n",
      "Iteration 159, loss = 0.46750689\n",
      "Iteration 160, loss = 0.46733801\n",
      "Iteration 161, loss = 0.46717373\n",
      "Iteration 162, loss = 0.46700047\n",
      "Iteration 163, loss = 0.46683421\n",
      "Iteration 164, loss = 0.46666904\n",
      "Iteration 165, loss = 0.46649960\n",
      "Iteration 166, loss = 0.46633302\n",
      "Iteration 167, loss = 0.46617004\n",
      "Iteration 168, loss = 0.46600366\n",
      "Iteration 169, loss = 0.46583889\n",
      "Iteration 170, loss = 0.46567680\n",
      "Iteration 171, loss = 0.46551574\n",
      "Iteration 172, loss = 0.46535285\n",
      "Iteration 173, loss = 0.46518624\n",
      "Iteration 174, loss = 0.46502718\n",
      "Iteration 175, loss = 0.46486526\n",
      "Iteration 176, loss = 0.46470787\n",
      "Iteration 177, loss = 0.46454803\n",
      "Iteration 178, loss = 0.46439084\n",
      "Iteration 179, loss = 0.46423189\n",
      "Iteration 180, loss = 0.46407343\n",
      "Iteration 181, loss = 0.46391610\n",
      "Iteration 182, loss = 0.46375847\n",
      "Iteration 183, loss = 0.46360262\n",
      "Iteration 184, loss = 0.46344632\n",
      "Iteration 185, loss = 0.46329281\n",
      "Iteration 186, loss = 0.46313736\n",
      "Iteration 187, loss = 0.46298171\n",
      "Iteration 188, loss = 0.46283084\n",
      "Iteration 189, loss = 0.46267596\n",
      "Iteration 190, loss = 0.46252310\n",
      "Iteration 191, loss = 0.46237044\n",
      "Iteration 192, loss = 0.46222129\n",
      "Iteration 193, loss = 0.46206995\n",
      "Iteration 194, loss = 0.46191817\n",
      "Iteration 195, loss = 0.46177048\n",
      "Iteration 196, loss = 0.46161491\n",
      "Iteration 197, loss = 0.46146696\n",
      "Iteration 198, loss = 0.46132099\n",
      "Iteration 199, loss = 0.46117061\n",
      "Iteration 200, loss = 0.46102348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for hidden_layer in hidden_layers[-2:]:\n",
    "    print(f'hidden_layer_size: {hidden_layer}')\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=hidden_layer,\n",
    "                        activation='relu',\n",
    "                        solver='sgd',\n",
    "                        alpha=0,\n",
    "                        batch_size=32,\n",
    "                        learning_rate='invscaling',\n",
    "                        max_iter=200,\n",
    "                        n_iter_no_change=5,\n",
    "                        verbose=True)\n",
    "    mlp.fit(x_train, y_train)\n",
    "    \n",
    "    models_f[str(hidden_layer)] = mlp"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T10:22:06.557469Z",
     "start_time": "2023-11-04T10:00:40.576839Z"
    }
   },
   "id": "6df5f69e8ed08440"
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "outputs": [],
   "source": [
    "# save models\n",
    "import pickle\n",
    "\n",
    "with open('pickles/models_f.pickle', 'wb') as handle:\n",
    "    pickle.dump(models_f, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T10:42:36.772081Z",
     "start_time": "2023-11-04T10:42:36.593822Z"
    }
   },
   "id": "17f47c76b6f507f4"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# load models\n",
    "import pickle\n",
    "with open('pickles/models_f.pickle', 'rb') as handle:\n",
    "    models_f = pickle.load(handle)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T15:47:29.131463Z",
     "start_time": "2023-11-04T15:47:27.901355Z"
    }
   },
   "id": "fd039b748a78aecc"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[512]': MLPClassifier(alpha=0, batch_size=32, hidden_layer_sizes=[512],\n",
      "              learning_rate='invscaling', n_iter_no_change=5, solver='sgd',\n",
      "              verbose=True), '[512, 256]': MLPClassifier(alpha=0, batch_size=32, hidden_layer_sizes=[512, 256],\n",
      "              learning_rate='invscaling', n_iter_no_change=5, solver='sgd',\n",
      "              verbose=True), '[512, 256, 128]': MLPClassifier(alpha=0, batch_size=32, hidden_layer_sizes=[512, 256, 128],\n",
      "              learning_rate='invscaling', n_iter_no_change=5, solver='sgd',\n",
      "              verbose=True), '[512, 256, 128, 64]': MLPClassifier(alpha=0, batch_size=32, hidden_layer_sizes=[512, 256, 128, 64],\n",
      "              learning_rate='invscaling', n_iter_no_change=5, solver='sgd',\n",
      "              verbose=True)}\n"
     ]
    }
   ],
   "source": [
    "print(models_f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T15:47:30.878401Z",
     "start_time": "2023-11-04T15:47:30.874293Z"
    }
   },
   "id": "7441c060325a8eec"
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[512] hidden layer size\n",
      "Training\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.93      0.95      2042\n",
      "           1       0.82      0.82      0.82      1976\n",
      "           2       0.69      0.71      0.70      1906\n",
      "           3       0.66      0.69      0.68      1913\n",
      "           4       0.86      0.83      0.84      2163\n",
      "\n",
      "    accuracy                           0.80     10000\n",
      "   macro avg       0.80      0.80      0.80     10000\n",
      "weighted avg       0.80      0.80      0.80     10000\n",
      "\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.98      0.94       214\n",
      "           1       0.80      0.77      0.78       206\n",
      "           2       0.61      0.66      0.63       183\n",
      "           3       0.59      0.62      0.61       179\n",
      "           4       0.87      0.74      0.80       218\n",
      "\n",
      "    accuracy                           0.76      1000\n",
      "   macro avg       0.76      0.75      0.75      1000\n",
      "weighted avg       0.77      0.76      0.76      1000\n",
      "[512, 256] hidden layer size\n",
      "Training\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.94      0.95      2025\n",
      "           1       0.84      0.84      0.84      1974\n",
      "           2       0.71      0.73      0.72      1911\n",
      "           3       0.65      0.69      0.67      1889\n",
      "           4       0.86      0.82      0.84      2201\n",
      "\n",
      "    accuracy                           0.81     10000\n",
      "   macro avg       0.81      0.80      0.81     10000\n",
      "weighted avg       0.81      0.81      0.81     10000\n",
      "\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95       216\n",
      "           1       0.79      0.80      0.79       196\n",
      "           2       0.64      0.67      0.65       189\n",
      "           3       0.57      0.60      0.58       179\n",
      "           4       0.84      0.72      0.78       220\n",
      "\n",
      "    accuracy                           0.76      1000\n",
      "   macro avg       0.75      0.75      0.75      1000\n",
      "weighted avg       0.76      0.76      0.76      1000\n",
      "[512, 256, 128] hidden layer size\n",
      "Training\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96      2010\n",
      "           1       0.85      0.86      0.85      1975\n",
      "           2       0.74      0.76      0.75      1905\n",
      "           3       0.68      0.71      0.70      1929\n",
      "           4       0.86      0.82      0.84      2181\n",
      "\n",
      "    accuracy                           0.82     10000\n",
      "   macro avg       0.82      0.82      0.82     10000\n",
      "weighted avg       0.82      0.82      0.82     10000\n",
      "\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.96       222\n",
      "           1       0.76      0.83      0.80       181\n",
      "           2       0.66      0.67      0.67       196\n",
      "           3       0.57      0.59      0.58       179\n",
      "           4       0.86      0.72      0.78       222\n",
      "\n",
      "    accuracy                           0.77      1000\n",
      "   macro avg       0.76      0.76      0.76      1000\n",
      "weighted avg       0.77      0.77      0.77      1000\n",
      "\n",
      "[512, 256, 128, 64] hidden layer size\n",
      "Training\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97      2011\n",
      "           1       0.87      0.86      0.86      1999\n",
      "           2       0.72      0.75      0.73      1871\n",
      "           3       0.66      0.68      0.67      1956\n",
      "           4       0.85      0.82      0.83      2163\n",
      "\n",
      "    accuracy                           0.82     10000\n",
      "   macro avg       0.82      0.81      0.81     10000\n",
      "weighted avg       0.82      0.82      0.82     10000\n",
      "\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       224\n",
      "           1       0.82      0.87      0.84       188\n",
      "           2       0.66      0.72      0.69       183\n",
      "           3       0.61      0.64      0.62       179\n",
      "           4       0.90      0.75      0.82       226\n",
      "\n",
      "    accuracy                           0.80      1000\n",
      "   macro avg       0.79      0.79      0.79      1000\n",
      "weighted avg       0.80      0.80      0.80      1000\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "for hidden_layer in hidden_layers:\n",
    "    mlp = models_f[str(hidden_layer)]\n",
    "    y_train_pred = mlp.predict(x_train)\n",
    "    y_test_pred = mlp.predict(x_test)\n",
    "    results = classification_report(y_train_pred, y_train)\n",
    "    print(f\"{hidden_layer} hidden layer size\")\n",
    "    print('Training')\n",
    "    print(results)\n",
    "    \n",
    "    results = classification_report(y_test_pred, y_test)\n",
    "    print('Test')\n",
    "    print(results)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T10:42:45.409678Z",
     "start_time": "2023-11-04T10:42:44.076806Z"
    }
   },
   "id": "bd12962bb9593479"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot avg f1 score\n",
    "hidden_layers = [[512], [512, 256], [512, 256, 128], [512, 256, 128, 64]]\n",
    "x_train, y_train = get_data('x_train.npy', 'y_train.npy')\n",
    "x_test, y_test = get_data('x_test.npy', 'y_test.npy')\n",
    "\n",
    "avg_f1_scores_training = []\n",
    "avg_f1_scores_test = []\n",
    "\n",
    "for hidden_layer in hidden_layers:\n",
    "    mlp = models_f[str(hidden_layer)]\n",
    "    y_train_pred = mlp.predict(x_train)\n",
    "    y_test_pred = mlp.predict(x_test)\n",
    "    results = classification_report(y_train_pred, y_train, output_dict=True)\n",
    "    avg_f1_scores_training.append(results['weighted avg']['f1-score'])\n",
    "    \n",
    "    results = classification_report(y_test_pred, y_test, output_dict=True)\n",
    "    avg_f1_scores_test.append(results['weighted avg']['f1-score'])\n",
    "\n",
    "plt.plot([len(layer) for layer in hidden_layers], avg_f1_scores_training, label = 'Training')\n",
    "plt.plot([len(layer) for layer in hidden_layers], avg_f1_scores_test, label = 'Test')\n",
    "plt.xlabel('Hidden Layers Depth')\n",
    "plt.ylabel('Average F1 Score')\n",
    "plt.legend()\n",
    "plt.savefig('(f) f1 vs hidden_depth.png')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T15:49:49.165718Z",
     "start_time": "2023-11-04T15:49:47.200927Z"
    }
   },
   "id": "db6ff06312473c21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7ec0d5e7dd18e1a3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
