{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-03T20:48:14.715172Z",
     "start_time": "2023-11-03T20:48:14.703520Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import sys\n",
    "import pdb\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def get_data(x_path, y_path):\n",
    "    '''\n",
    "    Args:\n",
    "        x_path: path to x file\n",
    "        y_path: path to y file\n",
    "    Returns:\n",
    "        x: np array of [NUM_OF_SAMPLES x n]\n",
    "        y: np array of [NUM_OF_SAMPLES]\n",
    "    '''\n",
    "    x = np.load(x_path)\n",
    "    y = np.load(y_path)\n",
    "\n",
    "    x = x.astype('float')\n",
    "\n",
    "    #normalize each example in x to have 0 mean and 1 std\n",
    "    \n",
    "    # Calculate the mean and standard deviation for each feature\n",
    "    feature_means = np.mean(x, axis=0)\n",
    "    feature_stds = np.std(x, axis=0)\n",
    "    feature_stds = feature_stds + (feature_stds == 0)\n",
    "\n",
    "    # Normalize each feature to have 0 mean and 1 std\n",
    "    x = (x - feature_means) / (feature_stds)\n",
    "    \n",
    "    # Adjust labels to start from 0 if they start from 1\n",
    "    y = y - 1\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "def get_metric(y_true, y_pred):\n",
    "    '''\n",
    "    Args:\n",
    "        y_true: np array of [NUM_SAMPLES x r] (one hot) \n",
    "                or np array of [NUM_SAMPLES]\n",
    "        y_pred: np array of [NUM_SAMPLES x r] (one hot) \n",
    "                or np array of [NUM_SAMPLES]\n",
    "                \n",
    "    '''\n",
    "    results = classification_report(y_pred, y_true)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "outputs": [
    {
     "data": {
      "text/plain": "((10000, 1024), (10000,), (1000, 1024), (1000,))"
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test, y_test = get_data('x_test.npy', 'y_test.npy')\n",
    "x_train, y_train = get_data('x_train.npy', 'y_train.npy')\n",
    "\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T20:48:14.907282Z",
     "start_time": "2023-11-03T20:48:14.709194Z"
    }
   },
   "id": "55dc64fea5b7f37c"
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [
    {
     "data": {
      "text/plain": "array([-0.39813868,  0.1879447 ,  0.91463024, ..., -0.15117837,\n        0.26519742,  0.        ])"
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T20:48:14.915988Z",
     "start_time": "2023-11-03T20:48:14.907635Z"
    }
   },
   "id": "730b00b27185555f"
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [],
   "source": [
    "label_encoder = OneHotEncoder(sparse_output = False)\n",
    "label_encoder.fit(np.expand_dims(y_train, axis = -1))\n",
    "\n",
    "y_train_onehot = label_encoder.transform(np.expand_dims(y_train, axis = -1))\n",
    "y_test_onehot = label_encoder.transform(np.expand_dims(y_test, axis = -1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T20:48:14.961634Z",
     "start_time": "2023-11-03T20:48:14.917795Z"
    }
   },
   "id": "9c612b2b9691a17d"
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.21001698],\n       [0.18440967],\n       [0.20223693],\n       [0.20182329],\n       [0.20151313]])"
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layer_sizes):\n",
    "        \"\"\"\n",
    "        Initializes the neural network with the given layer sizes.\n",
    "        layer_sizes is a list of integers, where the i-th integer represents\n",
    "        the number of neurons in the i-th layer.\n",
    "        \"\"\"\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.activations = []\n",
    "\n",
    "        # Initialize weights and biases for each layer\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            # Weights are initialized with small random values\n",
    "            self.weights.append(np.random.randn(layer_sizes[i+1], layer_sizes[i]) * 0.01)\n",
    "            self.biases.append(np.zeros((layer_sizes[i+1], 1)))\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        The sigmoid activation function.\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def softmax(self, z):\n",
    "        \"\"\"\n",
    "        The softmax function.\n",
    "        \"\"\"\n",
    "        e_z = np.exp(z)  # Subtracting np.max(z) for numerical stability\n",
    "        return e_z / e_z.sum(axis=0, keepdims=True)\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        \"\"\"\n",
    "        Performs a feedforward computation.\n",
    "        \"\"\"\n",
    "        activation = x\n",
    "        self.activations = [x]  # List to store all the activations, layer by layer\n",
    "\n",
    "        # Compute activations for each layer\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(w, activation) + b\n",
    "            activation = self.sigmoid(z) if w is not self.weights[-1] else self.softmax(z)\n",
    "            self.activations.append(activation)\n",
    "\n",
    "        return self.activations[-1]  # The final activation is the output of the network\n",
    "\n",
    "# Let's test the initialization and feedforward computation with a small network\n",
    "nn = NeuralNetwork([1024, 100, 5])  # A network with 1024 input features, one hidden layer with 100 neurons, and 5 output classes\n",
    "sample_input = np.random.randn(1024, 1)  # A random sample input\n",
    "output = nn.feedforward(sample_input)  # Perform a feedforward computation\n",
    "\n",
    "output  # Display the output probabilities"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T20:48:14.971503Z",
     "start_time": "2023-11-03T20:48:14.952378Z"
    }
   },
   "id": "c7e0def69df983a4"
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [],
   "source": [
    "class NeuralNetwork(NeuralNetwork):  # Extending the previously defined NeuralNetwork class\n",
    "    def cross_entropy_loss(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Computes the cross-entropy loss.\n",
    "        \"\"\"\n",
    "        m = y_true.shape[1]  # Number of examples\n",
    "        # To avoid division by zero, we clip the predictions to a minimum value\n",
    "        # y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        # Compute the cross-entropy loss\n",
    "        loss = -np.sum(y_true * np.log(y_pred)) / m\n",
    "        return loss\n",
    "\n",
    "    def backpropagation(self, y_true):\n",
    "        \"\"\"\n",
    "        Performs backpropagation to compute the gradients of the loss function\n",
    "        with respect to the weights and biases.\n",
    "        \"\"\"\n",
    "        m = y_true.shape[1]  # Number of examples\n",
    "        y_pred = self.activations[-1]  # The output of the last layer\n",
    "        y_true = y_true.reshape(y_pred.shape)  # Ensure same shape\n",
    "\n",
    "        # Initialize gradients for each layer\n",
    "        d_weights = [np.zeros(w.shape) for w in self.weights]\n",
    "        d_biases = [np.zeros(b.shape) for b in self.biases]\n",
    "\n",
    "        # Calculate derivative of loss w.r.t. the last layer output\n",
    "        d_loss = y_pred - y_true\n",
    "\n",
    "        for i in reversed(range(len(d_weights))):\n",
    "            d_activations = d_loss * self.sigmoid_derivative(self.activations[i+1]) if i != len(d_weights) - 1 else d_loss\n",
    "            d_weights[i] = np.dot(d_activations, self.activations[i].T) / m\n",
    "            d_biases[i] = np.sum(d_activations, axis=1, keepdims=True) / m\n",
    "            if i != 0:\n",
    "                d_loss = np.dot(self.weights[i].T, d_activations)\n",
    "\n",
    "        return d_weights, d_biases\n",
    "\n",
    "    def sigmoid_derivative(self, s):\n",
    "        \"\"\"\n",
    "        Derivative of the sigmoid function.\n",
    "        \"\"\"\n",
    "        return s * (1 - s)\n",
    "\n",
    "    def update_parameters(self, d_weights, d_biases, learning_rate):\n",
    "        \"\"\"\n",
    "        Updates the parameters using the computed gradients.\n",
    "        \"\"\"\n",
    "        # Update each parameter with a simple gradient descent step\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= learning_rate * d_weights[i]\n",
    "            self.biases[i] -= learning_rate * d_biases[i]\n",
    "            \n",
    "    def train(self, x_train, y_train, epochs, mini_batch_size, learning_rate, conv_threshold = 0.001, conv_epochs = 5):\n",
    "        n = x_train.shape[1]  # Total number of training examples\n",
    "\n",
    "        # Training loop\n",
    "        loss_history = []\n",
    "        permutation = np.random.permutation(n)\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle the training data for each epoch\n",
    "            x_train_shuffled = x_train[:, permutation]\n",
    "            y_train_shuffled = y_train[:, permutation]\n",
    "\n",
    "            # Mini-batch loop\n",
    "            for k in range(0, n, mini_batch_size):\n",
    "                mini_batch_x = x_train_shuffled[:, k:k + mini_batch_size]\n",
    "                mini_batch_y = y_train_shuffled[:, k:k + mini_batch_size]\n",
    "                # Forward pass\n",
    "                self.feedforward(mini_batch_x)\n",
    "                # Backward pass\n",
    "                d_weights, d_biases = self.backpropagation(mini_batch_y)\n",
    "                # Update parameters\n",
    "                self.update_parameters(d_weights, d_biases, learning_rate)\n",
    "            \n",
    "            loss = self.cross_entropy_loss(self.feedforward(x_train), y_train)\n",
    "            # Optional: Print the loss after each epoch (can be commented out for speed)\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}\")\n",
    "            \n",
    "            loss_history.append(loss)\n",
    "            \n",
    "            if len(loss_history) > conv_epochs:\n",
    "                temp = loss_history[-conv_epochs:]\n",
    "                if np.std(temp) < conv_threshold:\n",
    "                    print('Converged')\n",
    "                    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T20:56:11.482667Z",
     "start_time": "2023-11-03T20:56:11.479660Z"
    }
   },
   "id": "9798dcda491c7393"
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 1.3209\n",
      "Epoch 2/1000, Loss: 0.9888\n",
      "Epoch 3/1000, Loss: 0.8633\n",
      "Epoch 4/1000, Loss: 0.7913\n",
      "Epoch 5/1000, Loss: 0.7416\n",
      "Epoch 6/1000, Loss: 0.7035\n",
      "Epoch 7/1000, Loss: 0.6723\n",
      "Epoch 8/1000, Loss: 0.6458\n",
      "Epoch 9/1000, Loss: 0.6226\n",
      "Epoch 10/1000, Loss: 0.6021\n",
      "Epoch 11/1000, Loss: 0.5837\n",
      "Epoch 12/1000, Loss: 0.5670\n",
      "Epoch 13/1000, Loss: 0.5519\n",
      "Epoch 14/1000, Loss: 0.5381\n",
      "Epoch 15/1000, Loss: 0.5255\n",
      "Epoch 16/1000, Loss: 0.5138\n",
      "Epoch 17/1000, Loss: 0.5030\n",
      "Epoch 18/1000, Loss: 0.4930\n",
      "Epoch 19/1000, Loss: 0.4837\n",
      "Epoch 20/1000, Loss: 0.4749\n",
      "Epoch 21/1000, Loss: 0.4667\n",
      "Epoch 22/1000, Loss: 0.4590\n",
      "Epoch 23/1000, Loss: 0.4517\n",
      "Epoch 24/1000, Loss: 0.4448\n",
      "Epoch 25/1000, Loss: 0.4383\n",
      "Epoch 26/1000, Loss: 0.4321\n",
      "Epoch 27/1000, Loss: 0.4262\n",
      "Epoch 28/1000, Loss: 0.4205\n",
      "Epoch 29/1000, Loss: 0.4151\n",
      "Epoch 30/1000, Loss: 0.4100\n",
      "Epoch 31/1000, Loss: 0.4050\n",
      "Epoch 32/1000, Loss: 0.4003\n",
      "Epoch 33/1000, Loss: 0.3957\n",
      "Epoch 34/1000, Loss: 0.3913\n",
      "Epoch 35/1000, Loss: 0.3871\n",
      "Epoch 36/1000, Loss: 0.3830\n",
      "Epoch 37/1000, Loss: 0.3790\n",
      "Epoch 38/1000, Loss: 0.3752\n",
      "Epoch 39/1000, Loss: 0.3715\n",
      "Epoch 40/1000, Loss: 0.3679\n",
      "Epoch 41/1000, Loss: 0.3644\n",
      "Epoch 42/1000, Loss: 0.3610\n",
      "Epoch 43/1000, Loss: 0.3577\n",
      "Epoch 44/1000, Loss: 0.3545\n",
      "Epoch 45/1000, Loss: 0.3513\n",
      "Epoch 46/1000, Loss: 0.3483\n",
      "Epoch 47/1000, Loss: 0.3453\n",
      "Epoch 48/1000, Loss: 0.3424\n",
      "Epoch 49/1000, Loss: 0.3395\n",
      "Epoch 50/1000, Loss: 0.3367\n",
      "Epoch 51/1000, Loss: 0.3339\n",
      "Epoch 52/1000, Loss: 0.3312\n",
      "Epoch 53/1000, Loss: 0.3286\n",
      "Epoch 54/1000, Loss: 0.3260\n",
      "Epoch 55/1000, Loss: 0.3234\n",
      "Epoch 56/1000, Loss: 0.3209\n",
      "Epoch 57/1000, Loss: 0.3185\n",
      "Epoch 58/1000, Loss: 0.3160\n",
      "Epoch 59/1000, Loss: 0.3136\n",
      "Epoch 60/1000, Loss: 0.3113\n",
      "Epoch 61/1000, Loss: 0.3090\n",
      "Epoch 62/1000, Loss: 0.3067\n",
      "Epoch 63/1000, Loss: 0.3044\n",
      "Epoch 64/1000, Loss: 0.3022\n",
      "Epoch 65/1000, Loss: 0.3000\n",
      "Epoch 66/1000, Loss: 0.2978\n",
      "Epoch 67/1000, Loss: 0.2956\n",
      "Epoch 68/1000, Loss: 0.2935\n",
      "Epoch 69/1000, Loss: 0.2914\n",
      "Epoch 70/1000, Loss: 0.2893\n",
      "Epoch 71/1000, Loss: 0.2872\n",
      "Epoch 72/1000, Loss: 0.2852\n",
      "Epoch 73/1000, Loss: 0.2832\n",
      "Epoch 74/1000, Loss: 0.2812\n",
      "Epoch 75/1000, Loss: 0.2792\n",
      "Epoch 76/1000, Loss: 0.2772\n",
      "Epoch 77/1000, Loss: 0.2753\n",
      "Epoch 78/1000, Loss: 0.2733\n",
      "Epoch 79/1000, Loss: 0.2714\n",
      "Epoch 80/1000, Loss: 0.2695\n",
      "Epoch 81/1000, Loss: 0.2676\n",
      "Epoch 82/1000, Loss: 0.2658\n",
      "Epoch 83/1000, Loss: 0.2639\n",
      "Epoch 84/1000, Loss: 0.2621\n",
      "Epoch 85/1000, Loss: 0.2602\n",
      "Epoch 86/1000, Loss: 0.2584\n",
      "Epoch 87/1000, Loss: 0.2566\n",
      "Epoch 88/1000, Loss: 0.2548\n",
      "Epoch 89/1000, Loss: 0.2530\n",
      "Epoch 90/1000, Loss: 0.2513\n",
      "Epoch 91/1000, Loss: 0.2495\n",
      "Epoch 92/1000, Loss: 0.2477\n",
      "Epoch 93/1000, Loss: 0.2460\n",
      "Epoch 94/1000, Loss: 0.2443\n",
      "Epoch 95/1000, Loss: 0.2425\n",
      "Epoch 96/1000, Loss: 0.2408\n",
      "Epoch 97/1000, Loss: 0.2391\n",
      "Epoch 98/1000, Loss: 0.2374\n",
      "Epoch 99/1000, Loss: 0.2357\n",
      "Epoch 100/1000, Loss: 0.2340\n",
      "Epoch 101/1000, Loss: 0.2324\n",
      "Epoch 102/1000, Loss: 0.2307\n",
      "Epoch 103/1000, Loss: 0.2291\n",
      "Epoch 104/1000, Loss: 0.2274\n",
      "Epoch 105/1000, Loss: 0.2258\n",
      "Epoch 106/1000, Loss: 0.2242\n",
      "Epoch 107/1000, Loss: 0.2225\n",
      "Epoch 108/1000, Loss: 0.2209\n",
      "Epoch 109/1000, Loss: 0.2193\n",
      "Epoch 110/1000, Loss: 0.2177\n",
      "Epoch 111/1000, Loss: 0.2161\n",
      "Epoch 112/1000, Loss: 0.2146\n",
      "Epoch 113/1000, Loss: 0.2130\n",
      "Epoch 114/1000, Loss: 0.2114\n",
      "Epoch 115/1000, Loss: 0.2099\n",
      "Epoch 116/1000, Loss: 0.2083\n",
      "Epoch 117/1000, Loss: 0.2068\n",
      "Epoch 118/1000, Loss: 0.2052\n",
      "Epoch 119/1000, Loss: 0.2037\n",
      "Epoch 120/1000, Loss: 0.2022\n",
      "Epoch 121/1000, Loss: 0.2007\n",
      "Epoch 122/1000, Loss: 0.1992\n",
      "Epoch 123/1000, Loss: 0.1977\n",
      "Epoch 124/1000, Loss: 0.1962\n",
      "Epoch 125/1000, Loss: 0.1947\n",
      "Epoch 126/1000, Loss: 0.1933\n",
      "Epoch 127/1000, Loss: 0.1918\n",
      "Epoch 128/1000, Loss: 0.1903\n",
      "Epoch 129/1000, Loss: 0.1889\n",
      "Epoch 130/1000, Loss: 0.1874\n",
      "Epoch 131/1000, Loss: 0.1860\n",
      "Epoch 132/1000, Loss: 0.1846\n",
      "Epoch 133/1000, Loss: 0.1832\n",
      "Epoch 134/1000, Loss: 0.1818\n",
      "Epoch 135/1000, Loss: 0.1804\n",
      "Epoch 136/1000, Loss: 0.1790\n",
      "Epoch 137/1000, Loss: 0.1776\n",
      "Epoch 138/1000, Loss: 0.1762\n",
      "Epoch 139/1000, Loss: 0.1748\n",
      "Epoch 140/1000, Loss: 0.1735\n",
      "Epoch 141/1000, Loss: 0.1721\n",
      "Epoch 142/1000, Loss: 0.1708\n",
      "Epoch 143/1000, Loss: 0.1694\n",
      "Epoch 144/1000, Loss: 0.1681\n",
      "Epoch 145/1000, Loss: 0.1668\n",
      "Epoch 146/1000, Loss: 0.1655\n",
      "Epoch 147/1000, Loss: 0.1642\n",
      "Epoch 148/1000, Loss: 0.1629\n",
      "Epoch 149/1000, Loss: 0.1616\n",
      "Epoch 150/1000, Loss: 0.1603\n",
      "Epoch 151/1000, Loss: 0.1590\n",
      "Epoch 152/1000, Loss: 0.1578\n",
      "Epoch 153/1000, Loss: 0.1565\n",
      "Epoch 154/1000, Loss: 0.1552\n",
      "Epoch 155/1000, Loss: 0.1540\n",
      "Epoch 156/1000, Loss: 0.1528\n",
      "Epoch 157/1000, Loss: 0.1515\n",
      "Epoch 158/1000, Loss: 0.1503\n",
      "Epoch 159/1000, Loss: 0.1491\n",
      "Epoch 160/1000, Loss: 0.1479\n",
      "Epoch 161/1000, Loss: 0.1467\n",
      "Epoch 162/1000, Loss: 0.1455\n",
      "Epoch 163/1000, Loss: 0.1443\n",
      "Epoch 164/1000, Loss: 0.1432\n",
      "Epoch 165/1000, Loss: 0.1420\n",
      "Epoch 166/1000, Loss: 0.1408\n",
      "Epoch 167/1000, Loss: 0.1397\n",
      "Epoch 168/1000, Loss: 0.1386\n",
      "Epoch 169/1000, Loss: 0.1374\n",
      "Epoch 170/1000, Loss: 0.1363\n",
      "Epoch 171/1000, Loss: 0.1352\n",
      "Epoch 172/1000, Loss: 0.1341\n",
      "Epoch 173/1000, Loss: 0.1330\n",
      "Epoch 174/1000, Loss: 0.1319\n",
      "Epoch 175/1000, Loss: 0.1308\n",
      "Epoch 176/1000, Loss: 0.1297\n",
      "Epoch 177/1000, Loss: 0.1287\n",
      "Epoch 178/1000, Loss: 0.1276\n",
      "Epoch 179/1000, Loss: 0.1265\n",
      "Epoch 180/1000, Loss: 0.1255\n",
      "Epoch 181/1000, Loss: 0.1245\n",
      "Epoch 182/1000, Loss: 0.1234\n",
      "Epoch 183/1000, Loss: 0.1224\n",
      "Epoch 184/1000, Loss: 0.1214\n",
      "Epoch 185/1000, Loss: 0.1204\n",
      "Epoch 186/1000, Loss: 0.1194\n",
      "Epoch 187/1000, Loss: 0.1184\n",
      "Epoch 188/1000, Loss: 0.1174\n",
      "Epoch 189/1000, Loss: 0.1165\n",
      "Epoch 190/1000, Loss: 0.1155\n",
      "Epoch 191/1000, Loss: 0.1145\n",
      "Epoch 192/1000, Loss: 0.1136\n",
      "Epoch 193/1000, Loss: 0.1126\n",
      "Epoch 194/1000, Loss: 0.1117\n",
      "Epoch 195/1000, Loss: 0.1108\n",
      "Epoch 196/1000, Loss: 0.1098\n",
      "Epoch 197/1000, Loss: 0.1089\n",
      "Epoch 198/1000, Loss: 0.1080\n",
      "Epoch 199/1000, Loss: 0.1071\n",
      "Epoch 200/1000, Loss: 0.1062\n",
      "Epoch 201/1000, Loss: 0.1054\n",
      "Epoch 202/1000, Loss: 0.1045\n",
      "Epoch 203/1000, Loss: 0.1036\n",
      "Epoch 204/1000, Loss: 0.1028\n",
      "Epoch 205/1000, Loss: 0.1019\n",
      "Epoch 206/1000, Loss: 0.1011\n",
      "Epoch 207/1000, Loss: 0.1002\n",
      "Epoch 208/1000, Loss: 0.0994\n",
      "Epoch 209/1000, Loss: 0.0986\n",
      "Epoch 210/1000, Loss: 0.0977\n",
      "Epoch 211/1000, Loss: 0.0969\n",
      "Epoch 212/1000, Loss: 0.0961\n",
      "Epoch 213/1000, Loss: 0.0953\n",
      "Epoch 214/1000, Loss: 0.0946\n",
      "Epoch 215/1000, Loss: 0.0938\n",
      "Epoch 216/1000, Loss: 0.0930\n",
      "Epoch 217/1000, Loss: 0.0922\n",
      "Epoch 218/1000, Loss: 0.0915\n",
      "Epoch 219/1000, Loss: 0.0907\n",
      "Epoch 220/1000, Loss: 0.0900\n",
      "Epoch 221/1000, Loss: 0.0892\n",
      "Epoch 222/1000, Loss: 0.0885\n",
      "Epoch 223/1000, Loss: 0.0878\n",
      "Epoch 224/1000, Loss: 0.0870\n",
      "Epoch 225/1000, Loss: 0.0863\n",
      "Epoch 226/1000, Loss: 0.0856\n",
      "Epoch 227/1000, Loss: 0.0849\n",
      "Epoch 228/1000, Loss: 0.0842\n",
      "Converged\n"
     ]
    }
   ],
   "source": [
    "# Create a fresh instance of the neural network with the corrected train method\n",
    "nn = NeuralNetwork([1024, 100, 5])\n",
    "# Convert labels to one-hot encoding again\n",
    "y_train_one_hot = np.eye(5)[y_train].T\n",
    "\n",
    "# Train the neural network again with the corrected training method\n",
    "nn.train(x_train.T, y_train_one_hot, epochs=1000, mini_batch_size=32, learning_rate=0.01)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T20:57:15.278615Z",
     "start_time": "2023-11-03T20:56:13.657948Z"
    }
   },
   "id": "a087d04d64d29d66"
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 100.0%\n",
      "Test accuracy: 86.2%\n"
     ]
    }
   ],
   "source": [
    "# Compute the accuracy on the training and test sets\n",
    "y_train_pred = np.argmax(nn.feedforward(x_train.T), axis=0)\n",
    "y_test_pred = np.argmax(nn.feedforward(x_test.T), axis=0)\n",
    "print(f\"Training accuracy: {np.mean(y_train_pred == y_train) * 100}%\")\n",
    "print(f\"Test accuracy: {np.mean(y_test_pred == y_test) * 100}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T20:54:16.366737Z",
     "start_time": "2023-11-03T20:54:16.319527Z"
    }
   },
   "id": "ab2897275199bacf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# part b"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "219c7b2a8efafd53"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [
    "hidden_layer_sizes = [1, 5, 10, 50, 100]\n",
    "model_with_hidden_layer_size = {}\n",
    "\n",
    "for hidden_layer_size in hidden_layer_sizes:\n",
    "    \n",
    "    print(f'hidden_layer_size: {hidden_layer_size}')\n",
    "    nn = NeuralNetwork([1024, hidden_layer_size, 5])\n",
    "    nn.train(x_train.T, y_train_one_hot, epochs=1000, mini_batch_size=32, learning_rate=0.01)\n",
    "    model_with_hidden_layer_size[hidden_layer_size] = nn"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "552bcd8d031e5c4a"
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 hidden layer size\n",
      "Training\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.76      0.86      2589\n",
      "           1       0.63      0.89      0.74      1410\n",
      "           2       0.78      0.85      0.81      1782\n",
      "           3       0.40      0.73      0.52      1114\n",
      "           4       0.98      0.66      0.79      3105\n",
      "\n",
      "    accuracy                           0.76     10000\n",
      "   macro avg       0.76      0.78      0.74     10000\n",
      "weighted avg       0.84      0.76      0.78     10000\n",
      "\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.89      0.94       258\n",
      "           1       0.73      0.91      0.81       158\n",
      "           2       0.67      0.78      0.72       170\n",
      "           3       0.32      0.55      0.40       107\n",
      "           4       0.98      0.60      0.74       307\n",
      "\n",
      "    accuracy                           0.75      1000\n",
      "   macro avg       0.74      0.75      0.72      1000\n",
      "weighted avg       0.82      0.75      0.76      1000\n",
      "\n",
      "5 hidden layer size\n",
      "Training\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00      1980\n",
      "           1       0.97      0.97      0.97      1967\n",
      "           2       0.90      0.92      0.91      1902\n",
      "           3       0.85      0.88      0.87      1940\n",
      "           4       0.96      0.91      0.94      2211\n",
      "\n",
      "    accuracy                           0.94     10000\n",
      "   macro avg       0.94      0.94      0.94     10000\n",
      "weighted avg       0.94      0.94      0.94     10000\n",
      "\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       233\n",
      "           1       0.84      0.93      0.89       179\n",
      "           2       0.73      0.78      0.76       185\n",
      "           3       0.58      0.65      0.61       167\n",
      "           4       0.90      0.72      0.80       236\n",
      "\n",
      "    accuracy                           0.82      1000\n",
      "   macro avg       0.81      0.81      0.81      1000\n",
      "weighted avg       0.83      0.82      0.82      1000\n",
      "\n",
      "10 hidden layer size\n",
      "Training\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1977\n",
      "           1       0.98      0.98      0.98      1977\n",
      "           2       0.91      0.94      0.93      1882\n",
      "           3       0.90      0.88      0.89      2053\n",
      "           4       0.95      0.94      0.94      2111\n",
      "\n",
      "    accuracy                           0.95     10000\n",
      "   macro avg       0.95      0.95      0.95     10000\n",
      "weighted avg       0.95      0.95      0.95     10000\n",
      "\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       228\n",
      "           1       0.87      0.93      0.90       185\n",
      "           2       0.73      0.80      0.76       181\n",
      "           3       0.67      0.66      0.66       190\n",
      "           4       0.89      0.77      0.82       216\n",
      "\n",
      "    accuracy                           0.83      1000\n",
      "   macro avg       0.83      0.83      0.83      1000\n",
      "weighted avg       0.84      0.83      0.83      1000\n",
      "\n",
      "50 hidden layer size\n",
      "Training\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1977\n",
      "           1       0.98      0.99      0.98      1976\n",
      "           2       0.97      0.98      0.97      1940\n",
      "           3       0.98      0.98      0.98      2013\n",
      "           4       1.00      1.00      1.00      2094\n",
      "\n",
      "    accuracy                           0.99     10000\n",
      "   macro avg       0.99      0.99      0.99     10000\n",
      "weighted avg       0.99      0.99      0.99     10000\n",
      "\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       228\n",
      "           1       0.89      0.94      0.92       188\n",
      "           2       0.76      0.84      0.80       182\n",
      "           3       0.74      0.71      0.73       195\n",
      "           4       0.90      0.82      0.86       207\n",
      "\n",
      "    accuracy                           0.86      1000\n",
      "   macro avg       0.86      0.86      0.86      1000\n",
      "weighted avg       0.87      0.86      0.86      1000\n",
      "\n",
      "100 hidden layer size\n",
      "Training\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1977\n",
      "           1       0.98      0.98      0.98      1979\n",
      "           2       0.97      0.98      0.98      1940\n",
      "           3       0.99      0.98      0.99      2017\n",
      "           4       1.00      1.00      1.00      2087\n",
      "\n",
      "    accuracy                           0.99     10000\n",
      "   macro avg       0.99      0.99      0.99     10000\n",
      "weighted avg       0.99      0.99      0.99     10000\n",
      "\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       225\n",
      "           1       0.88      0.93      0.90       188\n",
      "           2       0.78      0.81      0.79       192\n",
      "           3       0.72      0.72      0.72       186\n",
      "           4       0.91      0.81      0.86       209\n",
      "\n",
      "    accuracy                           0.86      1000\n",
      "   macro avg       0.85      0.85      0.85      1000\n",
      "weighted avg       0.86      0.86      0.86      1000\n"
     ]
    }
   ],
   "source": [
    "for hidden_layer_size in hidden_layer_sizes:\n",
    "    nn = model_with_hidden_layer_size[hidden_layer_size]\n",
    "    y_train_pred = np.argmax(nn.feedforward(x_train.T), axis=0)\n",
    "    y_test_pred = np.argmax(nn.feedforward(x_test.T), axis=0)\n",
    "    results = classification_report(y_train_pred, y_train)\n",
    "    print(f\"{hidden_layer_size} hidden layer size\")\n",
    "    print('Training')\n",
    "    print(results)\n",
    "    \n",
    "    results = classification_report(y_test_pred, y_test)\n",
    "    print('Test')\n",
    "    print(results)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T21:26:45.277607Z",
     "start_time": "2023-11-03T21:26:45.035597Z"
    }
   },
   "id": "a2f58bd2451d8ceb"
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "outputs": [],
   "source": [
    "avg_f1_scores_training = []\n",
    "avg_f1_scores_test = []\n",
    "for hidden_layer_size in hidden_layer_sizes:\n",
    "    nn = model_with_hidden_layer_size[hidden_layer_size]\n",
    "    y_train_pred = np.argmax(nn.feedforward(x_train.T), axis=0)\n",
    "    y_test_pred = np.argmax(nn.feedforward(x_test.T), axis=0)\n",
    "    results = classification_report(y_train_pred, y_train, output_dict=True)\n",
    "    avg_f1_scores_training.append(results['weighted avg']['f1-score'])\n",
    "    \n",
    "    results = classification_report(y_test_pred, y_test, output_dict=True)\n",
    "    avg_f1_scores_test.append(results['weighted avg']['f1-score'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T21:31:37.721109Z",
     "start_time": "2023-11-03T21:31:37.461228Z"
    }
   },
   "id": "7edfcd12c7f47bf"
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hidden_layer_sizes, avg_f1_scores_training, label = 'Training')\n",
    "plt.plot(hidden_layer_sizes, avg_f1_scores_test, label = 'Test')\n",
    "plt.xlabel('Hidden Layer Size')\n",
    "plt.ylabel('Average F1 Score')\n",
    "plt.legend()\n",
    "plt.savefig('(b) f1 vs hidden_size.png')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T21:33:11.029360Z",
     "start_time": "2023-11-03T21:33:10.400233Z"
    }
   },
   "id": "1bd73810a81fe2df"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# part c"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6830333ad9ef09b8"
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_layer_size: [1024, 512, 5]\n",
      "Epoch 1/1000, Loss: 1.1081\n",
      "Epoch 2/1000, Loss: 0.8564\n",
      "Epoch 3/1000, Loss: 0.7482\n",
      "Epoch 4/1000, Loss: 0.6855\n",
      "Epoch 5/1000, Loss: 0.6427\n",
      "Epoch 6/1000, Loss: 0.6103\n",
      "Epoch 7/1000, Loss: 0.5843\n",
      "Epoch 8/1000, Loss: 0.5624\n",
      "Epoch 9/1000, Loss: 0.5435\n",
      "Epoch 10/1000, Loss: 0.5269\n",
      "Epoch 11/1000, Loss: 0.5121\n",
      "Epoch 12/1000, Loss: 0.4989\n",
      "Epoch 13/1000, Loss: 0.4869\n",
      "Epoch 14/1000, Loss: 0.4761\n",
      "Epoch 15/1000, Loss: 0.4662\n",
      "Epoch 16/1000, Loss: 0.4572\n",
      "Epoch 17/1000, Loss: 0.4490\n",
      "Epoch 18/1000, Loss: 0.4414\n",
      "Epoch 19/1000, Loss: 0.4343\n",
      "Epoch 20/1000, Loss: 0.4278\n",
      "Epoch 21/1000, Loss: 0.4218\n",
      "Epoch 22/1000, Loss: 0.4161\n",
      "Epoch 23/1000, Loss: 0.4108\n",
      "Epoch 24/1000, Loss: 0.4059\n",
      "Epoch 25/1000, Loss: 0.4012\n",
      "Epoch 26/1000, Loss: 0.3967\n",
      "Epoch 27/1000, Loss: 0.3925\n",
      "Epoch 28/1000, Loss: 0.3885\n",
      "Epoch 29/1000, Loss: 0.3847\n",
      "Epoch 30/1000, Loss: 0.3810\n",
      "Epoch 31/1000, Loss: 0.3775\n",
      "Epoch 32/1000, Loss: 0.3741\n",
      "Epoch 33/1000, Loss: 0.3709\n",
      "Epoch 34/1000, Loss: 0.3678\n",
      "Epoch 35/1000, Loss: 0.3647\n",
      "Epoch 36/1000, Loss: 0.3618\n",
      "Epoch 37/1000, Loss: 0.3590\n",
      "Epoch 38/1000, Loss: 0.3563\n",
      "Epoch 39/1000, Loss: 0.3536\n",
      "Epoch 40/1000, Loss: 0.3510\n",
      "Epoch 41/1000, Loss: 0.3485\n",
      "Epoch 42/1000, Loss: 0.3461\n",
      "Epoch 43/1000, Loss: 0.3437\n",
      "Epoch 44/1000, Loss: 0.3413\n",
      "Epoch 45/1000, Loss: 0.3391\n",
      "Epoch 46/1000, Loss: 0.3368\n",
      "Epoch 47/1000, Loss: 0.3347\n",
      "Epoch 48/1000, Loss: 0.3326\n",
      "Epoch 49/1000, Loss: 0.3305\n",
      "Epoch 50/1000, Loss: 0.3285\n",
      "Epoch 51/1000, Loss: 0.3265\n",
      "Epoch 52/1000, Loss: 0.3245\n",
      "Epoch 53/1000, Loss: 0.3226\n",
      "Epoch 54/1000, Loss: 0.3207\n",
      "Epoch 55/1000, Loss: 0.3189\n",
      "Epoch 56/1000, Loss: 0.3171\n",
      "Epoch 57/1000, Loss: 0.3153\n",
      "Epoch 58/1000, Loss: 0.3135\n",
      "Epoch 59/1000, Loss: 0.3118\n",
      "Epoch 60/1000, Loss: 0.3101\n",
      "Epoch 61/1000, Loss: 0.3085\n",
      "Epoch 62/1000, Loss: 0.3068\n",
      "Epoch 63/1000, Loss: 0.3052\n",
      "Epoch 64/1000, Loss: 0.3036\n",
      "Epoch 65/1000, Loss: 0.3020\n",
      "Epoch 66/1000, Loss: 0.3005\n",
      "Epoch 67/1000, Loss: 0.2989\n",
      "Epoch 68/1000, Loss: 0.2974\n",
      "Epoch 69/1000, Loss: 0.2959\n",
      "Epoch 70/1000, Loss: 0.2944\n",
      "Epoch 71/1000, Loss: 0.2930\n",
      "Epoch 72/1000, Loss: 0.2915\n",
      "Epoch 73/1000, Loss: 0.2901\n",
      "Epoch 74/1000, Loss: 0.2886\n",
      "Epoch 75/1000, Loss: 0.2872\n",
      "Epoch 76/1000, Loss: 0.2858\n",
      "Epoch 77/1000, Loss: 0.2845\n",
      "Epoch 78/1000, Loss: 0.2831\n",
      "Epoch 79/1000, Loss: 0.2817\n",
      "Epoch 80/1000, Loss: 0.2804\n",
      "Epoch 81/1000, Loss: 0.2790\n",
      "Epoch 82/1000, Loss: 0.2777\n",
      "Epoch 83/1000, Loss: 0.2763\n",
      "Epoch 84/1000, Loss: 0.2750\n",
      "Epoch 85/1000, Loss: 0.2737\n",
      "Epoch 86/1000, Loss: 0.2724\n",
      "Epoch 87/1000, Loss: 0.2711\n",
      "Epoch 88/1000, Loss: 0.2698\n",
      "Epoch 89/1000, Loss: 0.2685\n",
      "Epoch 90/1000, Loss: 0.2673\n",
      "Epoch 91/1000, Loss: 0.2660\n",
      "Epoch 92/1000, Loss: 0.2647\n",
      "Epoch 93/1000, Loss: 0.2635\n",
      "Epoch 94/1000, Loss: 0.2622\n",
      "Epoch 95/1000, Loss: 0.2609\n",
      "Epoch 96/1000, Loss: 0.2597\n",
      "Epoch 97/1000, Loss: 0.2584\n",
      "Epoch 98/1000, Loss: 0.2572\n",
      "Epoch 99/1000, Loss: 0.2560\n",
      "Epoch 100/1000, Loss: 0.2547\n",
      "Epoch 101/1000, Loss: 0.2535\n",
      "Epoch 102/1000, Loss: 0.2523\n",
      "Epoch 103/1000, Loss: 0.2510\n",
      "Epoch 104/1000, Loss: 0.2498\n",
      "Epoch 105/1000, Loss: 0.2486\n",
      "Epoch 106/1000, Loss: 0.2474\n",
      "Epoch 107/1000, Loss: 0.2462\n",
      "Epoch 108/1000, Loss: 0.2449\n",
      "Epoch 109/1000, Loss: 0.2437\n",
      "Epoch 110/1000, Loss: 0.2425\n",
      "Epoch 111/1000, Loss: 0.2413\n",
      "Epoch 112/1000, Loss: 0.2401\n",
      "Epoch 113/1000, Loss: 0.2389\n",
      "Epoch 114/1000, Loss: 0.2377\n",
      "Epoch 115/1000, Loss: 0.2365\n",
      "Epoch 116/1000, Loss: 0.2353\n",
      "Epoch 117/1000, Loss: 0.2341\n",
      "Epoch 118/1000, Loss: 0.2329\n",
      "Epoch 119/1000, Loss: 0.2317\n",
      "Epoch 120/1000, Loss: 0.2305\n",
      "Epoch 121/1000, Loss: 0.2293\n",
      "Epoch 122/1000, Loss: 0.2281\n",
      "Epoch 123/1000, Loss: 0.2269\n",
      "Epoch 124/1000, Loss: 0.2257\n",
      "Epoch 125/1000, Loss: 0.2245\n",
      "Epoch 126/1000, Loss: 0.2233\n",
      "Epoch 127/1000, Loss: 0.2221\n",
      "Epoch 128/1000, Loss: 0.2209\n",
      "Epoch 129/1000, Loss: 0.2197\n",
      "Epoch 130/1000, Loss: 0.2185\n",
      "Epoch 131/1000, Loss: 0.2173\n",
      "Epoch 132/1000, Loss: 0.2162\n",
      "Epoch 133/1000, Loss: 0.2150\n",
      "Epoch 134/1000, Loss: 0.2138\n",
      "Epoch 135/1000, Loss: 0.2126\n",
      "Epoch 136/1000, Loss: 0.2114\n",
      "Epoch 137/1000, Loss: 0.2102\n",
      "Epoch 138/1000, Loss: 0.2090\n",
      "Epoch 139/1000, Loss: 0.2078\n",
      "Epoch 140/1000, Loss: 0.2067\n",
      "Epoch 141/1000, Loss: 0.2055\n",
      "Epoch 142/1000, Loss: 0.2043\n",
      "Epoch 143/1000, Loss: 0.2031\n",
      "Epoch 144/1000, Loss: 0.2019\n",
      "Epoch 145/1000, Loss: 0.2007\n",
      "Epoch 146/1000, Loss: 0.1995\n",
      "Epoch 147/1000, Loss: 0.1984\n",
      "Epoch 148/1000, Loss: 0.1972\n",
      "Epoch 149/1000, Loss: 0.1960\n",
      "Epoch 150/1000, Loss: 0.1948\n",
      "Epoch 151/1000, Loss: 0.1936\n",
      "Epoch 152/1000, Loss: 0.1924\n",
      "Epoch 153/1000, Loss: 0.1913\n",
      "Epoch 154/1000, Loss: 0.1901\n",
      "Epoch 155/1000, Loss: 0.1889\n",
      "Epoch 156/1000, Loss: 0.1877\n",
      "Epoch 157/1000, Loss: 0.1866\n",
      "Epoch 158/1000, Loss: 0.1854\n",
      "Epoch 159/1000, Loss: 0.1842\n",
      "Epoch 160/1000, Loss: 0.1830\n",
      "Epoch 161/1000, Loss: 0.1819\n",
      "Epoch 162/1000, Loss: 0.1807\n",
      "Epoch 163/1000, Loss: 0.1795\n",
      "Epoch 164/1000, Loss: 0.1783\n",
      "Epoch 165/1000, Loss: 0.1772\n",
      "Epoch 166/1000, Loss: 0.1760\n",
      "Epoch 167/1000, Loss: 0.1748\n",
      "Epoch 168/1000, Loss: 0.1737\n",
      "Epoch 169/1000, Loss: 0.1725\n",
      "Epoch 170/1000, Loss: 0.1714\n",
      "Epoch 171/1000, Loss: 0.1702\n",
      "Epoch 172/1000, Loss: 0.1690\n",
      "Epoch 173/1000, Loss: 0.1679\n",
      "Epoch 174/1000, Loss: 0.1667\n",
      "Epoch 175/1000, Loss: 0.1656\n",
      "Epoch 176/1000, Loss: 0.1644\n",
      "Epoch 177/1000, Loss: 0.1633\n",
      "Epoch 178/1000, Loss: 0.1621\n",
      "Epoch 179/1000, Loss: 0.1610\n",
      "Epoch 180/1000, Loss: 0.1599\n",
      "Epoch 181/1000, Loss: 0.1587\n",
      "Epoch 182/1000, Loss: 0.1576\n",
      "Epoch 183/1000, Loss: 0.1565\n",
      "Epoch 184/1000, Loss: 0.1553\n",
      "Epoch 185/1000, Loss: 0.1542\n",
      "Epoch 186/1000, Loss: 0.1531\n",
      "Epoch 187/1000, Loss: 0.1520\n",
      "Epoch 188/1000, Loss: 0.1508\n",
      "Epoch 189/1000, Loss: 0.1497\n",
      "Epoch 190/1000, Loss: 0.1486\n",
      "Epoch 191/1000, Loss: 0.1475\n",
      "Epoch 192/1000, Loss: 0.1464\n",
      "Epoch 193/1000, Loss: 0.1453\n",
      "Epoch 194/1000, Loss: 0.1442\n",
      "Epoch 195/1000, Loss: 0.1431\n",
      "Epoch 196/1000, Loss: 0.1420\n",
      "Epoch 197/1000, Loss: 0.1409\n",
      "Epoch 198/1000, Loss: 0.1399\n",
      "Epoch 199/1000, Loss: 0.1388\n",
      "Epoch 200/1000, Loss: 0.1377\n",
      "Epoch 201/1000, Loss: 0.1366\n",
      "Epoch 202/1000, Loss: 0.1356\n",
      "Epoch 203/1000, Loss: 0.1345\n",
      "Epoch 204/1000, Loss: 0.1335\n",
      "Epoch 205/1000, Loss: 0.1324\n",
      "Epoch 206/1000, Loss: 0.1314\n",
      "Epoch 207/1000, Loss: 0.1303\n",
      "Epoch 208/1000, Loss: 0.1293\n",
      "Epoch 209/1000, Loss: 0.1283\n",
      "Epoch 210/1000, Loss: 0.1272\n",
      "Epoch 211/1000, Loss: 0.1262\n",
      "Epoch 212/1000, Loss: 0.1252\n",
      "Epoch 213/1000, Loss: 0.1242\n",
      "Epoch 214/1000, Loss: 0.1232\n",
      "Epoch 215/1000, Loss: 0.1222\n",
      "Epoch 216/1000, Loss: 0.1212\n",
      "Epoch 217/1000, Loss: 0.1202\n",
      "Epoch 218/1000, Loss: 0.1192\n",
      "Epoch 219/1000, Loss: 0.1182\n",
      "Epoch 220/1000, Loss: 0.1173\n",
      "Epoch 221/1000, Loss: 0.1163\n",
      "Epoch 222/1000, Loss: 0.1153\n",
      "Epoch 223/1000, Loss: 0.1144\n",
      "Epoch 224/1000, Loss: 0.1134\n",
      "Epoch 225/1000, Loss: 0.1125\n",
      "Epoch 226/1000, Loss: 0.1116\n",
      "Epoch 227/1000, Loss: 0.1106\n",
      "Epoch 228/1000, Loss: 0.1097\n",
      "Epoch 229/1000, Loss: 0.1088\n",
      "Epoch 230/1000, Loss: 0.1079\n",
      "Epoch 231/1000, Loss: 0.1070\n",
      "Epoch 232/1000, Loss: 0.1061\n",
      "Epoch 233/1000, Loss: 0.1052\n",
      "Epoch 234/1000, Loss: 0.1043\n",
      "Epoch 235/1000, Loss: 0.1034\n",
      "Epoch 236/1000, Loss: 0.1025\n",
      "Epoch 237/1000, Loss: 0.1017\n",
      "Epoch 238/1000, Loss: 0.1008\n",
      "Epoch 239/1000, Loss: 0.0999\n",
      "Epoch 240/1000, Loss: 0.0991\n",
      "Epoch 241/1000, Loss: 0.0983\n",
      "Epoch 242/1000, Loss: 0.0974\n",
      "Epoch 243/1000, Loss: 0.0966\n",
      "Epoch 244/1000, Loss: 0.0958\n",
      "Epoch 245/1000, Loss: 0.0949\n",
      "Epoch 246/1000, Loss: 0.0941\n",
      "Epoch 247/1000, Loss: 0.0933\n",
      "Epoch 248/1000, Loss: 0.0925\n",
      "Epoch 249/1000, Loss: 0.0917\n",
      "Epoch 250/1000, Loss: 0.0910\n",
      "Epoch 251/1000, Loss: 0.0902\n",
      "Epoch 252/1000, Loss: 0.0894\n",
      "Epoch 253/1000, Loss: 0.0886\n",
      "Epoch 254/1000, Loss: 0.0879\n",
      "Epoch 255/1000, Loss: 0.0871\n",
      "Epoch 256/1000, Loss: 0.0864\n",
      "Epoch 257/1000, Loss: 0.0856\n",
      "Epoch 258/1000, Loss: 0.0849\n",
      "Epoch 259/1000, Loss: 0.0842\n",
      "Epoch 260/1000, Loss: 0.0835\n",
      "Epoch 261/1000, Loss: 0.0827\n",
      "Epoch 262/1000, Loss: 0.0820\n",
      "Epoch 263/1000, Loss: 0.0813\n",
      "Epoch 264/1000, Loss: 0.0806\n",
      "Converged\n",
      "hidden_layer_size: [1024, 512, 256, 5]\n",
      "Epoch 1/1000, Loss: 1.6273\n",
      "Epoch 2/1000, Loss: 1.6264\n",
      "Epoch 3/1000, Loss: 1.6250\n",
      "Epoch 4/1000, Loss: 1.6227\n",
      "Epoch 5/1000, Loss: 1.6178\n",
      "Epoch 6/1000, Loss: 1.6057\n",
      "Epoch 7/1000, Loss: 1.5668\n",
      "Epoch 8/1000, Loss: 1.4254\n",
      "Epoch 9/1000, Loss: 1.1592\n",
      "Epoch 10/1000, Loss: 0.9767\n",
      "Epoch 11/1000, Loss: 0.8680\n",
      "Epoch 12/1000, Loss: 0.7962\n",
      "Epoch 13/1000, Loss: 0.7451\n",
      "Epoch 14/1000, Loss: 0.7063\n",
      "Epoch 15/1000, Loss: 0.6751\n",
      "Epoch 16/1000, Loss: 0.6488\n",
      "Epoch 17/1000, Loss: 0.6259\n",
      "Epoch 18/1000, Loss: 0.6055\n",
      "Epoch 19/1000, Loss: 0.5872\n",
      "Epoch 20/1000, Loss: 0.5705\n",
      "Epoch 21/1000, Loss: 0.5552\n",
      "Epoch 22/1000, Loss: 0.5411\n",
      "Epoch 23/1000, Loss: 0.5282\n",
      "Epoch 24/1000, Loss: 0.5163\n",
      "Epoch 25/1000, Loss: 0.5053\n",
      "Epoch 26/1000, Loss: 0.4953\n",
      "Epoch 27/1000, Loss: 0.4860\n",
      "Epoch 28/1000, Loss: 0.4775\n",
      "Epoch 29/1000, Loss: 0.4696\n",
      "Epoch 30/1000, Loss: 0.4623\n",
      "Epoch 31/1000, Loss: 0.4556\n",
      "Epoch 32/1000, Loss: 0.4493\n",
      "Epoch 33/1000, Loss: 0.4435\n",
      "Epoch 34/1000, Loss: 0.4381\n",
      "Epoch 35/1000, Loss: 0.4330\n",
      "Epoch 36/1000, Loss: 0.4282\n",
      "Epoch 37/1000, Loss: 0.4237\n",
      "Epoch 38/1000, Loss: 0.4194\n",
      "Epoch 39/1000, Loss: 0.4154\n",
      "Epoch 40/1000, Loss: 0.4116\n",
      "Epoch 41/1000, Loss: 0.4080\n",
      "Epoch 42/1000, Loss: 0.4045\n",
      "Epoch 43/1000, Loss: 0.4012\n",
      "Epoch 44/1000, Loss: 0.3981\n",
      "Epoch 45/1000, Loss: 0.3951\n",
      "Epoch 46/1000, Loss: 0.3922\n",
      "Epoch 47/1000, Loss: 0.3894\n",
      "Epoch 48/1000, Loss: 0.3868\n",
      "Epoch 49/1000, Loss: 0.3842\n",
      "Epoch 50/1000, Loss: 0.3818\n",
      "Epoch 51/1000, Loss: 0.3794\n",
      "Epoch 52/1000, Loss: 0.3771\n",
      "Epoch 53/1000, Loss: 0.3749\n",
      "Epoch 54/1000, Loss: 0.3728\n",
      "Epoch 55/1000, Loss: 0.3707\n",
      "Epoch 56/1000, Loss: 0.3687\n",
      "Epoch 57/1000, Loss: 0.3668\n",
      "Epoch 58/1000, Loss: 0.3649\n",
      "Epoch 59/1000, Loss: 0.3631\n",
      "Epoch 60/1000, Loss: 0.3613\n",
      "Epoch 61/1000, Loss: 0.3595\n",
      "Epoch 62/1000, Loss: 0.3578\n",
      "Epoch 63/1000, Loss: 0.3562\n",
      "Epoch 64/1000, Loss: 0.3546\n",
      "Epoch 65/1000, Loss: 0.3530\n",
      "Epoch 66/1000, Loss: 0.3515\n",
      "Epoch 67/1000, Loss: 0.3500\n",
      "Epoch 68/1000, Loss: 0.3485\n",
      "Epoch 69/1000, Loss: 0.3471\n",
      "Epoch 70/1000, Loss: 0.3456\n",
      "Epoch 71/1000, Loss: 0.3443\n",
      "Epoch 72/1000, Loss: 0.3429\n",
      "Epoch 73/1000, Loss: 0.3415\n",
      "Epoch 74/1000, Loss: 0.3402\n",
      "Epoch 75/1000, Loss: 0.3389\n",
      "Epoch 76/1000, Loss: 0.3376\n",
      "Epoch 77/1000, Loss: 0.3364\n",
      "Epoch 78/1000, Loss: 0.3351\n",
      "Epoch 79/1000, Loss: 0.3339\n",
      "Epoch 80/1000, Loss: 0.3327\n",
      "Epoch 81/1000, Loss: 0.3315\n",
      "Epoch 82/1000, Loss: 0.3303\n",
      "Epoch 83/1000, Loss: 0.3291\n",
      "Epoch 84/1000, Loss: 0.3280\n",
      "Epoch 85/1000, Loss: 0.3268\n",
      "Epoch 86/1000, Loss: 0.3257\n",
      "Epoch 87/1000, Loss: 0.3245\n",
      "Epoch 88/1000, Loss: 0.3234\n",
      "Epoch 89/1000, Loss: 0.3223\n",
      "Epoch 90/1000, Loss: 0.3212\n",
      "Epoch 91/1000, Loss: 0.3201\n",
      "Epoch 92/1000, Loss: 0.3190\n",
      "Epoch 93/1000, Loss: 0.3179\n",
      "Epoch 94/1000, Loss: 0.3168\n",
      "Epoch 95/1000, Loss: 0.3157\n",
      "Epoch 96/1000, Loss: 0.3147\n",
      "Epoch 97/1000, Loss: 0.3136\n",
      "Epoch 98/1000, Loss: 0.3125\n",
      "Epoch 99/1000, Loss: 0.3114\n",
      "Epoch 100/1000, Loss: 0.3104\n",
      "Epoch 101/1000, Loss: 0.3093\n",
      "Epoch 102/1000, Loss: 0.3083\n",
      "Epoch 103/1000, Loss: 0.3072\n",
      "Epoch 104/1000, Loss: 0.3061\n",
      "Epoch 105/1000, Loss: 0.3051\n",
      "Epoch 106/1000, Loss: 0.3040\n",
      "Epoch 107/1000, Loss: 0.3029\n",
      "Epoch 108/1000, Loss: 0.3019\n",
      "Epoch 109/1000, Loss: 0.3008\n",
      "Epoch 110/1000, Loss: 0.2997\n",
      "Epoch 111/1000, Loss: 0.2987\n",
      "Epoch 112/1000, Loss: 0.2976\n",
      "Epoch 113/1000, Loss: 0.2965\n",
      "Epoch 114/1000, Loss: 0.2954\n",
      "Epoch 115/1000, Loss: 0.2944\n",
      "Epoch 116/1000, Loss: 0.2933\n",
      "Epoch 117/1000, Loss: 0.2922\n",
      "Epoch 118/1000, Loss: 0.2911\n",
      "Epoch 119/1000, Loss: 0.2900\n",
      "Epoch 120/1000, Loss: 0.2889\n",
      "Epoch 121/1000, Loss: 0.2878\n",
      "Epoch 122/1000, Loss: 0.2867\n",
      "Epoch 123/1000, Loss: 0.2856\n",
      "Epoch 124/1000, Loss: 0.2845\n",
      "Epoch 125/1000, Loss: 0.2833\n",
      "Epoch 126/1000, Loss: 0.2822\n",
      "Epoch 127/1000, Loss: 0.2811\n",
      "Epoch 128/1000, Loss: 0.2799\n",
      "Epoch 129/1000, Loss: 0.2788\n",
      "Epoch 130/1000, Loss: 0.2777\n",
      "Epoch 131/1000, Loss: 0.2765\n",
      "Epoch 132/1000, Loss: 0.2754\n",
      "Epoch 133/1000, Loss: 0.2742\n",
      "Epoch 134/1000, Loss: 0.2730\n",
      "Epoch 135/1000, Loss: 0.2719\n",
      "Epoch 136/1000, Loss: 0.2707\n",
      "Epoch 137/1000, Loss: 0.2695\n",
      "Epoch 138/1000, Loss: 0.2683\n",
      "Epoch 139/1000, Loss: 0.2671\n",
      "Epoch 140/1000, Loss: 0.2659\n",
      "Epoch 141/1000, Loss: 0.2647\n",
      "Epoch 142/1000, Loss: 0.2635\n",
      "Epoch 143/1000, Loss: 0.2623\n",
      "Epoch 144/1000, Loss: 0.2611\n",
      "Epoch 145/1000, Loss: 0.2599\n",
      "Epoch 146/1000, Loss: 0.2586\n",
      "Epoch 147/1000, Loss: 0.2574\n",
      "Epoch 148/1000, Loss: 0.2562\n",
      "Epoch 149/1000, Loss: 0.2549\n",
      "Epoch 150/1000, Loss: 0.2537\n",
      "Epoch 151/1000, Loss: 0.2524\n",
      "Epoch 152/1000, Loss: 0.2512\n",
      "Epoch 153/1000, Loss: 0.2499\n",
      "Epoch 154/1000, Loss: 0.2486\n",
      "Epoch 155/1000, Loss: 0.2473\n",
      "Epoch 156/1000, Loss: 0.2461\n",
      "Epoch 157/1000, Loss: 0.2448\n",
      "Epoch 158/1000, Loss: 0.2435\n",
      "Epoch 159/1000, Loss: 0.2422\n",
      "Epoch 160/1000, Loss: 0.2409\n",
      "Epoch 161/1000, Loss: 0.2396\n",
      "Epoch 162/1000, Loss: 0.2383\n",
      "Epoch 163/1000, Loss: 0.2370\n",
      "Epoch 164/1000, Loss: 0.2357\n",
      "Epoch 165/1000, Loss: 0.2343\n",
      "Epoch 166/1000, Loss: 0.2330\n",
      "Epoch 167/1000, Loss: 0.2317\n",
      "Epoch 168/1000, Loss: 0.2304\n",
      "Epoch 169/1000, Loss: 0.2290\n",
      "Epoch 170/1000, Loss: 0.2277\n",
      "Epoch 171/1000, Loss: 0.2263\n",
      "Epoch 172/1000, Loss: 0.2250\n",
      "Epoch 173/1000, Loss: 0.2236\n",
      "Epoch 174/1000, Loss: 0.2222\n",
      "Epoch 175/1000, Loss: 0.2209\n",
      "Epoch 176/1000, Loss: 0.2195\n",
      "Epoch 177/1000, Loss: 0.2181\n",
      "Epoch 178/1000, Loss: 0.2168\n",
      "Epoch 179/1000, Loss: 0.2154\n",
      "Epoch 180/1000, Loss: 0.2140\n",
      "Epoch 181/1000, Loss: 0.2126\n",
      "Epoch 182/1000, Loss: 0.2112\n",
      "Epoch 183/1000, Loss: 0.2098\n",
      "Epoch 184/1000, Loss: 0.2084\n",
      "Epoch 185/1000, Loss: 0.2070\n",
      "Epoch 186/1000, Loss: 0.2056\n",
      "Epoch 187/1000, Loss: 0.2042\n",
      "Epoch 188/1000, Loss: 0.2027\n",
      "Epoch 189/1000, Loss: 0.2013\n",
      "Epoch 190/1000, Loss: 0.1999\n",
      "Epoch 191/1000, Loss: 0.1985\n",
      "Epoch 192/1000, Loss: 0.1970\n",
      "Epoch 193/1000, Loss: 0.1956\n",
      "Epoch 194/1000, Loss: 0.1942\n",
      "Epoch 195/1000, Loss: 0.1927\n",
      "Epoch 196/1000, Loss: 0.1913\n",
      "Epoch 197/1000, Loss: 0.1898\n",
      "Epoch 198/1000, Loss: 0.1884\n",
      "Epoch 199/1000, Loss: 0.1869\n",
      "Epoch 200/1000, Loss: 0.1855\n",
      "Epoch 201/1000, Loss: 0.1840\n",
      "Epoch 202/1000, Loss: 0.1826\n",
      "Epoch 203/1000, Loss: 0.1811\n",
      "Epoch 204/1000, Loss: 0.1797\n",
      "Epoch 205/1000, Loss: 0.1782\n",
      "Epoch 206/1000, Loss: 0.1767\n",
      "Epoch 207/1000, Loss: 0.1753\n",
      "Epoch 208/1000, Loss: 0.1738\n",
      "Epoch 209/1000, Loss: 0.1723\n",
      "Epoch 210/1000, Loss: 0.1709\n",
      "Epoch 211/1000, Loss: 0.1694\n",
      "Epoch 212/1000, Loss: 0.1679\n",
      "Epoch 213/1000, Loss: 0.1665\n",
      "Epoch 214/1000, Loss: 0.1650\n",
      "Epoch 215/1000, Loss: 0.1635\n",
      "Epoch 216/1000, Loss: 0.1621\n",
      "Epoch 217/1000, Loss: 0.1606\n",
      "Epoch 218/1000, Loss: 0.1591\n",
      "Epoch 219/1000, Loss: 0.1577\n",
      "Epoch 220/1000, Loss: 0.1562\n",
      "Epoch 221/1000, Loss: 0.1548\n",
      "Epoch 222/1000, Loss: 0.1533\n",
      "Epoch 223/1000, Loss: 0.1518\n",
      "Epoch 224/1000, Loss: 0.1504\n",
      "Epoch 225/1000, Loss: 0.1489\n",
      "Epoch 226/1000, Loss: 0.1475\n",
      "Epoch 227/1000, Loss: 0.1460\n",
      "Epoch 228/1000, Loss: 0.1446\n",
      "Epoch 229/1000, Loss: 0.1431\n",
      "Epoch 230/1000, Loss: 0.1417\n",
      "Epoch 231/1000, Loss: 0.1403\n",
      "Epoch 232/1000, Loss: 0.1388\n",
      "Epoch 233/1000, Loss: 0.1374\n",
      "Epoch 234/1000, Loss: 0.1360\n",
      "Epoch 235/1000, Loss: 0.1346\n",
      "Epoch 236/1000, Loss: 0.1332\n",
      "Epoch 237/1000, Loss: 0.1318\n",
      "Epoch 238/1000, Loss: 0.1304\n",
      "Epoch 239/1000, Loss: 0.1290\n",
      "Epoch 240/1000, Loss: 0.1276\n",
      "Epoch 241/1000, Loss: 0.1262\n",
      "Epoch 242/1000, Loss: 0.1248\n",
      "Epoch 243/1000, Loss: 0.1235\n",
      "Epoch 244/1000, Loss: 0.1221\n",
      "Epoch 245/1000, Loss: 0.1207\n",
      "Epoch 246/1000, Loss: 0.1194\n",
      "Epoch 247/1000, Loss: 0.1180\n",
      "Epoch 248/1000, Loss: 0.1167\n",
      "Epoch 249/1000, Loss: 0.1154\n",
      "Epoch 250/1000, Loss: 0.1141\n",
      "Epoch 251/1000, Loss: 0.1128\n",
      "Epoch 252/1000, Loss: 0.1115\n",
      "Epoch 253/1000, Loss: 0.1102\n",
      "Epoch 254/1000, Loss: 0.1089\n",
      "Epoch 255/1000, Loss: 0.1076\n",
      "Epoch 256/1000, Loss: 0.1063\n",
      "Epoch 257/1000, Loss: 0.1051\n",
      "Epoch 258/1000, Loss: 0.1038\n",
      "Epoch 259/1000, Loss: 0.1026\n",
      "Epoch 260/1000, Loss: 0.1014\n",
      "Epoch 261/1000, Loss: 0.1002\n",
      "Epoch 262/1000, Loss: 0.0990\n",
      "Epoch 263/1000, Loss: 0.0978\n",
      "Epoch 264/1000, Loss: 0.0966\n",
      "Epoch 265/1000, Loss: 0.0954\n",
      "Epoch 266/1000, Loss: 0.0942\n",
      "Epoch 267/1000, Loss: 0.0931\n",
      "Epoch 268/1000, Loss: 0.0920\n",
      "Epoch 269/1000, Loss: 0.0908\n",
      "Epoch 270/1000, Loss: 0.0897\n",
      "Epoch 271/1000, Loss: 0.0886\n",
      "Epoch 272/1000, Loss: 0.0875\n",
      "Epoch 273/1000, Loss: 0.0864\n",
      "Epoch 274/1000, Loss: 0.0853\n",
      "Epoch 275/1000, Loss: 0.0843\n",
      "Epoch 276/1000, Loss: 0.0832\n",
      "Epoch 277/1000, Loss: 0.0822\n",
      "Epoch 278/1000, Loss: 0.0811\n",
      "Epoch 279/1000, Loss: 0.0801\n",
      "Epoch 280/1000, Loss: 0.0791\n",
      "Epoch 281/1000, Loss: 0.0781\n",
      "Epoch 282/1000, Loss: 0.0771\n",
      "Epoch 283/1000, Loss: 0.0762\n",
      "Epoch 284/1000, Loss: 0.0752\n",
      "Epoch 285/1000, Loss: 0.0743\n",
      "Epoch 286/1000, Loss: 0.0733\n",
      "Epoch 287/1000, Loss: 0.0724\n",
      "Epoch 288/1000, Loss: 0.0715\n",
      "Epoch 289/1000, Loss: 0.0706\n",
      "Epoch 290/1000, Loss: 0.0697\n",
      "Epoch 291/1000, Loss: 0.0688\n",
      "Epoch 292/1000, Loss: 0.0679\n",
      "Epoch 293/1000, Loss: 0.0671\n",
      "Epoch 294/1000, Loss: 0.0662\n",
      "Epoch 295/1000, Loss: 0.0654\n",
      "Epoch 296/1000, Loss: 0.0646\n",
      "Epoch 297/1000, Loss: 0.0638\n",
      "Epoch 298/1000, Loss: 0.0630\n",
      "Epoch 299/1000, Loss: 0.0622\n",
      "Epoch 300/1000, Loss: 0.0614\n",
      "Epoch 301/1000, Loss: 0.0607\n",
      "Epoch 302/1000, Loss: 0.0599\n",
      "Epoch 303/1000, Loss: 0.0592\n",
      "Epoch 304/1000, Loss: 0.0584\n",
      "Epoch 305/1000, Loss: 0.0577\n",
      "Epoch 306/1000, Loss: 0.0570\n",
      "Epoch 307/1000, Loss: 0.0563\n",
      "Epoch 308/1000, Loss: 0.0556\n",
      "Epoch 309/1000, Loss: 0.0549\n",
      "Converged\n",
      "hidden_layer_size: [1024, 512, 256, 128, 5]\n",
      "Epoch 1/1000, Loss: 1.6155\n",
      "Epoch 2/1000, Loss: 1.6154\n",
      "Epoch 3/1000, Loss: 1.6154\n",
      "Epoch 4/1000, Loss: 1.6154\n",
      "Epoch 5/1000, Loss: 1.6154\n",
      "Epoch 6/1000, Loss: 1.6153\n",
      "Converged\n",
      "hidden_layer_size: [1024, 512, 256, 128, 64, 5]\n",
      "Epoch 1/1000, Loss: 1.6104\n",
      "Epoch 2/1000, Loss: 1.6104\n",
      "Epoch 3/1000, Loss: 1.6104\n",
      "Epoch 4/1000, Loss: 1.6104\n",
      "Epoch 5/1000, Loss: 1.6104\n",
      "Epoch 6/1000, Loss: 1.6104\n",
      "Converged\n"
     ]
    }
   ],
   "source": [
    "hidden_layers = [[512], [512, 256], [512, 256, 128], [512, 256, 128, 64]]\n",
    "layers = [[1024] + hidden_layer + [5] for hidden_layer in hidden_layers]\n",
    "model_with_hidden_layers = {}\n",
    "\n",
    "for layer in layers:\n",
    "    print(f'hidden_layer_size: {layer}')\n",
    "    nn = NeuralNetwork(layer)\n",
    "    nn.train(x_train.T, y_train_one_hot, epochs=1000, mini_batch_size=32, learning_rate=0.01)\n",
    "    model_with_hidden_layers[str(layer)] = nn"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T21:53:38.047237Z",
     "start_time": "2023-11-03T21:36:01.026027Z"
    }
   },
   "id": "95d87e1a46724f6d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for layer in layers[-2:]:\n",
    "    print(f'hidden_layer_size: {layer}')\n",
    "    nn = NeuralNetwork(layer)\n",
    "    nn.train(x_train.T, y_train_one_hot, epochs=1000, mini_batch_size=32, learning_rate=0.01, conv_threshold=1e-5)\n",
    "    model_with_hidden_layers[str(layer)] = nn"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "606e00a16c9bab05"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for layer in layers[-1:]:\n",
    "    print(f'hidden_layer_size: {layer}')\n",
    "    nn = NeuralNetwork(layer)\n",
    "    nn.train(x_train.T, y_train_one_hot, epochs=1000, mini_batch_size=32, learning_rate=0.1, conv_threshold=1e-5)\n",
    "    model_with_hidden_layers[str(layer)] = nn"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "bc2dbb8f764efd33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save models\n",
    "import pickle\n",
    "\n",
    "with open('model_with_hidden_layers.pickle', 'wb') as handle:\n",
    "    pickle.dump(model_with_hidden_layers, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "83264148dff72d23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for layer in layers:\n",
    "    nn = model_with_hidden_layers[str(layer)]\n",
    "    y_train_pred = np.argmax(nn.feedforward(x_train.T), axis=0)\n",
    "    y_test_pred = np.argmax(nn.feedforward(x_test.T), axis=0)\n",
    "    results = classification_report(y_train_pred, y_train)\n",
    "    print(f\"{layer} hidden layer size\")\n",
    "    print('Training')\n",
    "    print(results)\n",
    "    \n",
    "    results = classification_report(y_test_pred, y_test)\n",
    "    print('Test')\n",
    "    print(results)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "8da1ea2b144d2d50"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# part c"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f29e0ba30b3cd5f2"
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [],
   "source": [
    "class NeuralNetwork(NeuralNetwork):  # Extending the previously defined NeuralNetwork class\n",
    "    def train_c(self, x_train, y_train, epochs, mini_batch_size, learning_rate, conv_threshold = 0.001, conv_epochs = 5):\n",
    "        n = x_train.shape[1]  # Total number of training examples\n",
    "\n",
    "        # Training loop\n",
    "        loss_history = []\n",
    "        permutation = np.random.permutation(n)\n",
    "        learning_rate_c = learning_rate\n",
    "        for epoch in range(epochs):\n",
    "            learning_rate = learning_rate_c / pow(epoch, 0.5)\n",
    "            # Shuffle the training data for each epoch\n",
    "            x_train_shuffled = x_train[:, permutation]\n",
    "            y_train_shuffled = y_train[:, permutation]\n",
    "\n",
    "            # Mini-batch loop\n",
    "            for k in range(0, n, mini_batch_size):\n",
    "                mini_batch_x = x_train_shuffled[:, k:k + mini_batch_size]\n",
    "                mini_batch_y = y_train_shuffled[:, k:k + mini_batch_size]\n",
    "                # Forward pass\n",
    "                self.feedforward(mini_batch_x)\n",
    "                # Backward pass\n",
    "                d_weights, d_biases = self.backpropagation(mini_batch_y)\n",
    "                # Update parameters\n",
    "                self.update_parameters(d_weights, d_biases, learning_rate)\n",
    "            \n",
    "            loss = self.cross_entropy_loss(self.feedforward(x_train), y_train)\n",
    "            # Optional: Print the loss after each epoch (can be commented out for speed)\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}\")\n",
    "            \n",
    "            loss_history.append(loss)\n",
    "            \n",
    "            if len(loss_history) > conv_epochs:\n",
    "                temp = loss_history[-conv_epochs:]\n",
    "                if np.std(temp) < conv_threshold:\n",
    "                    print('Converged')\n",
    "                    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T23:20:21.294311Z",
     "start_time": "2023-11-03T23:20:21.291275Z"
    }
   },
   "id": "7d922afb91699240"
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "outputs": [],
   "source": [
    "models_d = {}\n",
    "for layer in layers:\n",
    "    print(f'hidden_layer_size: {layer}')\n",
    "    nn = NeuralNetwork(layer)\n",
    "    nn.train_c(x_train.T, y_train_one_hot, epochs=1000, mini_batch_size=32, learning_rate=0.01)\n",
    "    models_d[str(layer)] = nn"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9dff9f4bcaee0f2c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# part f"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b5dc631a4fc3a77"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_layer_size: [512]\n",
      "Iteration 1, loss = 0.85098997\n",
      "Iteration 2, loss = 0.59706419\n",
      "Iteration 3, loss = 0.59042063\n",
      "Iteration 4, loss = 0.58687388\n",
      "Iteration 5, loss = 0.58450515\n",
      "Iteration 6, loss = 0.58268417\n",
      "Iteration 7, loss = 0.58117875\n",
      "Iteration 8, loss = 0.57990014\n",
      "Iteration 9, loss = 0.57877349\n",
      "Iteration 10, loss = 0.57777492\n",
      "Iteration 11, loss = 0.57685872\n",
      "Iteration 12, loss = 0.57602095\n",
      "Iteration 13, loss = 0.57523868\n",
      "Iteration 14, loss = 0.57451889\n",
      "Iteration 15, loss = 0.57384004\n",
      "Iteration 16, loss = 0.57319678\n",
      "Iteration 17, loss = 0.57259446\n",
      "Iteration 18, loss = 0.57201568\n",
      "Iteration 19, loss = 0.57147014\n",
      "Iteration 20, loss = 0.57094349\n",
      "Iteration 21, loss = 0.57044027\n",
      "Iteration 22, loss = 0.56995713\n",
      "Iteration 23, loss = 0.56948431\n",
      "Iteration 24, loss = 0.56903466\n",
      "Iteration 25, loss = 0.56860076\n",
      "Iteration 26, loss = 0.56817817\n",
      "Iteration 27, loss = 0.56777220\n",
      "Iteration 28, loss = 0.56736976\n",
      "Iteration 29, loss = 0.56698630\n",
      "Iteration 30, loss = 0.56660851\n",
      "Iteration 31, loss = 0.56624280\n",
      "Iteration 32, loss = 0.56588569\n",
      "Iteration 33, loss = 0.56553935\n",
      "Iteration 34, loss = 0.56519796\n",
      "Iteration 35, loss = 0.56486258\n",
      "Iteration 36, loss = 0.56453730\n",
      "Iteration 37, loss = 0.56421544\n",
      "Iteration 38, loss = 0.56390594\n",
      "Iteration 39, loss = 0.56359521\n",
      "Iteration 40, loss = 0.56330010\n",
      "Iteration 41, loss = 0.56300031\n",
      "Iteration 42, loss = 0.56271281\n",
      "Iteration 43, loss = 0.56242684\n",
      "Iteration 44, loss = 0.56214842\n",
      "Iteration 45, loss = 0.56187055\n",
      "Iteration 46, loss = 0.56159553\n",
      "Iteration 47, loss = 0.56133065\n",
      "Iteration 48, loss = 0.56106853\n",
      "Iteration 49, loss = 0.56081254\n",
      "Iteration 50, loss = 0.56055598\n",
      "Iteration 51, loss = 0.56030317\n",
      "Iteration 52, loss = 0.56005339\n",
      "Iteration 53, loss = 0.55981190\n",
      "Iteration 54, loss = 0.55956927\n",
      "Iteration 55, loss = 0.55933380\n",
      "Iteration 56, loss = 0.55909780\n",
      "Iteration 57, loss = 0.55886128\n",
      "Iteration 58, loss = 0.55863296\n",
      "Iteration 59, loss = 0.55840462\n",
      "Iteration 60, loss = 0.55818269\n",
      "Iteration 61, loss = 0.55796103\n",
      "Iteration 62, loss = 0.55774145\n",
      "Iteration 63, loss = 0.55752459\n",
      "Iteration 64, loss = 0.55730943\n",
      "Iteration 65, loss = 0.55709795\n",
      "Iteration 66, loss = 0.55688799\n",
      "Iteration 67, loss = 0.55668263\n",
      "Iteration 68, loss = 0.55647659\n",
      "Iteration 69, loss = 0.55627264\n",
      "Iteration 70, loss = 0.55607241\n",
      "Iteration 71, loss = 0.55587054\n",
      "Iteration 72, loss = 0.55567343\n",
      "Iteration 73, loss = 0.55547721\n",
      "Iteration 74, loss = 0.55528521\n",
      "Iteration 75, loss = 0.55509388\n",
      "Iteration 76, loss = 0.55490299\n",
      "Iteration 77, loss = 0.55471550\n",
      "Iteration 78, loss = 0.55452801\n",
      "Iteration 79, loss = 0.55434248\n",
      "Iteration 80, loss = 0.55416150\n",
      "Iteration 81, loss = 0.55397744\n",
      "Iteration 82, loss = 0.55379600\n",
      "Iteration 83, loss = 0.55361691\n",
      "Iteration 84, loss = 0.55343797\n",
      "Iteration 85, loss = 0.55326255\n",
      "Iteration 86, loss = 0.55308975\n",
      "Iteration 87, loss = 0.55291523\n",
      "Iteration 88, loss = 0.55274139\n",
      "Iteration 89, loss = 0.55257125\n",
      "Iteration 90, loss = 0.55240220\n",
      "Iteration 91, loss = 0.55223408\n",
      "Iteration 92, loss = 0.55206662\n",
      "Iteration 93, loss = 0.55189976\n",
      "Iteration 94, loss = 0.55173523\n",
      "Iteration 95, loss = 0.55157209\n",
      "Iteration 96, loss = 0.55140950\n",
      "Iteration 97, loss = 0.55124769\n",
      "Iteration 98, loss = 0.55108753\n",
      "Iteration 99, loss = 0.55092922\n",
      "Iteration 100, loss = 0.55077120\n",
      "Iteration 101, loss = 0.55061465\n",
      "Iteration 102, loss = 0.55045831\n",
      "Iteration 103, loss = 0.55030167\n",
      "Iteration 104, loss = 0.55014904\n",
      "Iteration 105, loss = 0.54999752\n",
      "Iteration 106, loss = 0.54984444\n",
      "Iteration 107, loss = 0.54969257\n",
      "Iteration 108, loss = 0.54954113\n",
      "Iteration 109, loss = 0.54939259\n",
      "Iteration 110, loss = 0.54924619\n",
      "Iteration 111, loss = 0.54909804\n",
      "Iteration 112, loss = 0.54895119\n",
      "Iteration 113, loss = 0.54880483\n",
      "Iteration 114, loss = 0.54865991\n",
      "Iteration 115, loss = 0.54851962\n",
      "Iteration 116, loss = 0.54837521\n",
      "Iteration 117, loss = 0.54823271\n",
      "Iteration 118, loss = 0.54808871\n",
      "Iteration 119, loss = 0.54794862\n",
      "Iteration 120, loss = 0.54780973\n",
      "Iteration 121, loss = 0.54767128\n",
      "Iteration 122, loss = 0.54753330\n",
      "Iteration 123, loss = 0.54739445\n",
      "Iteration 124, loss = 0.54725540\n",
      "Iteration 125, loss = 0.54712098\n",
      "Iteration 126, loss = 0.54698415\n",
      "Iteration 127, loss = 0.54685051\n",
      "Iteration 128, loss = 0.54671666\n",
      "Iteration 129, loss = 0.54658062\n",
      "Iteration 130, loss = 0.54644941\n",
      "Iteration 131, loss = 0.54631622\n",
      "Iteration 132, loss = 0.54618495\n",
      "Iteration 133, loss = 0.54605346\n",
      "Iteration 134, loss = 0.54592289\n",
      "Iteration 135, loss = 0.54579500\n",
      "Iteration 136, loss = 0.54566504\n",
      "Iteration 137, loss = 0.54553673\n",
      "Iteration 138, loss = 0.54540812\n",
      "Iteration 139, loss = 0.54528286\n",
      "Iteration 140, loss = 0.54515548\n",
      "Iteration 141, loss = 0.54503122\n",
      "Iteration 142, loss = 0.54490399\n",
      "Iteration 143, loss = 0.54477975\n",
      "Iteration 144, loss = 0.54465545\n",
      "Iteration 145, loss = 0.54453241\n",
      "Iteration 146, loss = 0.54440745\n",
      "Iteration 147, loss = 0.54428440\n",
      "Iteration 148, loss = 0.54416352\n",
      "Iteration 149, loss = 0.54404378\n",
      "Iteration 150, loss = 0.54392202\n",
      "Iteration 151, loss = 0.54380064\n",
      "Iteration 152, loss = 0.54368060\n",
      "Iteration 153, loss = 0.54356224\n",
      "Iteration 154, loss = 0.54344261\n",
      "Iteration 155, loss = 0.54332561\n",
      "Iteration 156, loss = 0.54320623\n",
      "Iteration 157, loss = 0.54308846\n",
      "Iteration 158, loss = 0.54297156\n",
      "Iteration 159, loss = 0.54285707\n",
      "Iteration 160, loss = 0.54274055\n",
      "Iteration 161, loss = 0.54262509\n",
      "Iteration 162, loss = 0.54250990\n",
      "Iteration 163, loss = 0.54239492\n",
      "Iteration 164, loss = 0.54228102\n",
      "Iteration 165, loss = 0.54216709\n",
      "Iteration 166, loss = 0.54205422\n",
      "Iteration 167, loss = 0.54194182\n",
      "Iteration 168, loss = 0.54183104\n",
      "Iteration 169, loss = 0.54171873\n",
      "Iteration 170, loss = 0.54160585\n",
      "Iteration 171, loss = 0.54149649\n",
      "Iteration 172, loss = 0.54138535\n",
      "Iteration 173, loss = 0.54127411\n",
      "Iteration 174, loss = 0.54116393\n",
      "Iteration 175, loss = 0.54105696\n",
      "Iteration 176, loss = 0.54094601\n",
      "Iteration 177, loss = 0.54083754\n",
      "Iteration 178, loss = 0.54073061\n",
      "Iteration 179, loss = 0.54062301\n",
      "Iteration 180, loss = 0.54051498\n",
      "Iteration 181, loss = 0.54040888\n",
      "Iteration 182, loss = 0.54030235\n",
      "Iteration 183, loss = 0.54019485\n",
      "Iteration 184, loss = 0.54009053\n",
      "Iteration 185, loss = 0.53998393\n",
      "Iteration 186, loss = 0.53988060\n",
      "Iteration 187, loss = 0.53977465\n",
      "Iteration 188, loss = 0.53967192\n",
      "Iteration 189, loss = 0.53956710\n",
      "Iteration 190, loss = 0.53946379\n",
      "Iteration 191, loss = 0.53935952\n",
      "Iteration 192, loss = 0.53925798\n",
      "Iteration 193, loss = 0.53915495\n",
      "Iteration 194, loss = 0.53905204\n",
      "Iteration 195, loss = 0.53894962\n",
      "Iteration 196, loss = 0.53884991\n",
      "Iteration 197, loss = 0.53874862\n",
      "Iteration 198, loss = 0.53864835\n",
      "Iteration 199, loss = 0.53854618\n",
      "Iteration 200, loss = 0.53844618\n",
      "hidden_layer_size: [512, 256]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.84732109\n",
      "Iteration 2, loss = 0.55792190\n",
      "Iteration 3, loss = 0.55426019\n",
      "Iteration 4, loss = 0.55205214\n",
      "Iteration 5, loss = 0.55037141\n",
      "Iteration 6, loss = 0.54900566\n",
      "Iteration 7, loss = 0.54784589\n",
      "Iteration 8, loss = 0.54681499\n",
      "Iteration 9, loss = 0.54586764\n",
      "Iteration 10, loss = 0.54501781\n",
      "Iteration 11, loss = 0.54423312\n",
      "Iteration 12, loss = 0.54349883\n",
      "Iteration 13, loss = 0.54280202\n",
      "Iteration 14, loss = 0.54214736\n",
      "Iteration 15, loss = 0.54152301\n",
      "Iteration 16, loss = 0.54093273\n",
      "Iteration 17, loss = 0.54035979\n",
      "Iteration 18, loss = 0.53981771\n",
      "Iteration 19, loss = 0.53929209\n",
      "Iteration 20, loss = 0.53877916\n",
      "Iteration 21, loss = 0.53829075\n",
      "Iteration 22, loss = 0.53781337\n",
      "Iteration 23, loss = 0.53734835\n",
      "Iteration 24, loss = 0.53690571\n",
      "Iteration 25, loss = 0.53646692\n",
      "Iteration 26, loss = 0.53604249\n",
      "Iteration 27, loss = 0.53562578\n",
      "Iteration 28, loss = 0.53522441\n",
      "Iteration 29, loss = 0.53482673\n",
      "Iteration 30, loss = 0.53444002\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "models_f = {}\n",
    "for hidden_layer in hidden_layers:\n",
    "    print(f'hidden_layer_size: {hidden_layer}')\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=hidden_layer,\n",
    "                        activation='relu',\n",
    "                        solver='sgd',\n",
    "                        alpha=0,\n",
    "                        batch_size=32,\n",
    "                        learning_rate='invscaling',\n",
    "                        max_iter=200,\n",
    "                        n_iter_no_change=5,\n",
    "                        verbose=True)\n",
    "    mlp.fit(x_train, y_train)\n",
    "    \n",
    "    models_f[str(hidden_layer_size)] = mlp"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-04T06:46:57.715192Z"
    }
   },
   "id": "7fbb393a46ce22b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save models\n",
    "import pickle\n",
    "\n",
    "with open('pickles/models_f.pickle', 'wb') as handle:\n",
    "    pickle.dump(models_f, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "17f47c76b6f507f4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load models\n",
    "with open('pickles/models_f.pickle', 'rb') as handle:\n",
    "    models_f = pickle.load(handle)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "fd039b748a78aecc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "bd12962bb9593479"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
